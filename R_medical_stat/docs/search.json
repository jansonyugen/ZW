[
  {
    "objectID": "1020-discriminant.html#fisher判别分析",
    "href": "1020-discriminant.html#fisher判别分析",
    "title": "20  R语言判别分析",
    "section": "20.1 Fisher判别分析",
    "text": "20.1 Fisher判别分析\nFisher判别又称为典型判别（canonical discriminant）分析，适用于两类和多分类判别。\nFisher判别使用贝叶斯定理确定每个观测属于某个类别的概率。如果你有两个类别，比如良性和恶性，判别分析会分别计算属于两个类别的概率，然后选择概率大的类别作为正确的类别。\n线性判别分析假设每个类中的观测服从多元正态分布，并且不同类别之间的协方差相等。二次判别假设观测服从正态分布，每种类别都有自己的协方差。\n使用孙振球版《医学统计学》第4版例20-1的数据。电子版及配套数据已上传到QQ群，需要的加群下载即可。\n收集了22例肝硬化患者的3个指标，其中早期患者（用1表示）12例，晚期患者（用2表示），试做判别分析。\n\ndf &lt;- read.csv(\"datasets/例20-1.csv\")\n\npsych::headTail(df)\n##      id  x1  x2  x3   y\n## 1     1  23   8   0   1\n## 2     2  -1   9  -2   1\n## 3     3 -10   5   0   1\n## 4     4  -7  -2   1   1\n## ... ... ... ... ... ...\n## 19   19  -9 -20   3   2\n## 20   20  -7  -2   3   2\n## 21   21  -9   6   0   2\n## 22   22  12   0   0   2\n\n这个数据集中id是编号，x1,x2,x3是自变量，y是因变量。\n线性判别分析可以通过MASS包中的lda函数实现：\n\nlibrary(MASS)\n\nfit &lt;- lda(y ~ x1+x2+x3, data = df)\nfit\n## Call:\n## lda(y ~ x1 + x2 + x3, data = df)\n## \n## Prior probabilities of groups:\n##         1         2 \n## 0.5454545 0.4545455 \n## \n## Group means:\n##   x1 x2 x3\n## 1 -3  4 -1\n## 2  4 -5  1\n## \n## Coefficients of linear discriminants:\n##           LD1\n## x1  0.0395150\n## x2 -0.1265698\n## x3  0.1792631\n\nPrior probabilities of groups是先验概率，类别1的概率是0.5454545，类别2是0.4545455。\n然后给出了每个组在不同类别中的均值。\n最下面给出了线性判别系数，如果你的结果变量是3个类别，会给出两组判别系数，这里我的结果变量只有2分类，所以结果只有1组。\n结果可以画出来：\n\nplot(fit,type=\"both\")\n\n\n\n\n上图是判别分析结果的直方图和密度图，可以看出组间有重合，说明有些分组分错了。\n下面用predict提取判别分析的分类结果。\npredict用于判别分析可以得到3种类型的结果，class是类别，posterior是概率，x是线性判别评分。\n\npred &lt;- predict(fit)$class\ntable(df$y, pred)\n##    pred\n##      1  2\n##   1 11  1\n##   2  2  8\n\n可以看到有3个分类分错了，结果还是可以的。\n可以查看每个患者的后验概率：\n\n# 查看概率\npredict(fit)$posterior\n##             1           2\n## 1  0.62566758 0.374332416\n## 2  0.95508370 0.044916302\n## 3  0.89600449 0.103995511\n## 4  0.51330556 0.486694443\n## 5  0.95464457 0.045355435\n## 6  0.88314148 0.116858515\n## 7  0.77454260 0.225457398\n## 8  0.99508599 0.004914013\n## 9  0.89391137 0.106088634\n## 10 0.84899794 0.151002059\n## 11 0.31960372 0.680396284\n## 12 0.64144092 0.358559076\n## 13 0.14903037 0.850969632\n## 14 0.57026493 0.429735074\n## 15 0.13106732 0.868932682\n## 16 0.26925350 0.730746503\n## 17 0.03911397 0.960886034\n## 18 0.04332382 0.956676176\n## 19 0.01115243 0.988847571\n## 20 0.35826933 0.641730670\n## 21 0.90954200 0.090457999\n## 22 0.37480490 0.625195100\n\n上面的图我们也可以用ggplot2画出来。\n\ndf.plot &lt;- data.frame(LD1 = predict(fit)$x[,1],\n                      y = factor(df$y,labels = c(\"早期患者\",\"晚期患者\"))\n                      )\n\nlibrary(ggplot2)\n\nggplot(df.plot, aes(x=LD1, fill=y))+\n  geom_histogram()+\n  facet_wrap(~ y, ncol = 1)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n如果你想用这个模型预测新的数据，只需要predict(fit, newdata = xxx)即可。比如我们新建一个数据：\n\ntmp &lt;- data.frame(x1 = c(-9,-7,-9),\n                  x2 = c(-18,-2,6),\n                  x3 = c(3,3,1)\n                  )\n\npredict(fit, newdata = tmp)\n## $class\n## [1] 2 2 1\n## Levels: 1 2\n## \n## $posterior\n##            1         2\n## 1 0.01736557 0.9826344\n## 2 0.35826933 0.6417307\n## 3 0.87974275 0.1202573\n## \n## $x\n##          LD1\n## 1  2.4580167\n## 2  0.5119296\n## 3 -0.9381851\n\n这样就得到新的结果。\n我们再用一个iris鸢尾花数据集演示下线性判别分析的结果可视化，这个结果变量是3分类的。\n\nstr(iris)\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n拟合模型：\n\nlibrary(MASS)\n\nfit &lt;- lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris)\nfit\n## Call:\n## lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, \n##     data = iris)\n## \n## Prior probabilities of groups:\n##     setosa versicolor  virginica \n##  0.3333333  0.3333333  0.3333333 \n## \n## Group means:\n##            Sepal.Length Sepal.Width Petal.Length Petal.Width\n## setosa            5.006       3.428        1.462       0.246\n## versicolor        5.936       2.770        4.260       1.326\n## virginica         6.588       2.974        5.552       2.026\n## \n## Coefficients of linear discriminants:\n##                     LD1         LD2\n## Sepal.Length  0.8293776 -0.02410215\n## Sepal.Width   1.5344731 -2.16452123\n## Petal.Length -2.2012117  0.93192121\n## Petal.Width  -2.8104603 -2.83918785\n## \n## Proportion of trace:\n##    LD1    LD2 \n## 0.9912 0.0088\n\n可视化结果：\n\niris$LD1 &lt;- predict(fit)$x[,1]\niris$LD2 &lt;- predict(fit)$x[,2]\n\nlibrary(ggplot2)\n\nggplot(iris, aes(LD1,LD2))+\n  geom_point(aes(color=Species),size=3)\n\n\n\n\n\nggplot(iris, aes(x=LD1, fill=Species))+\n  geom_histogram()+\n  facet_wrap(~ Species, ncol = 1)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n二次判别分析和线性判别分析用法一样。\n\nfit &lt;- qda(y ~ x1+x2+x3, data = df)\nfit\n## Call:\n## qda(y ~ x1 + x2 + x3, data = df)\n## \n## Prior probabilities of groups:\n##         1         2 \n## 0.5454545 0.4545455 \n## \n## Group means:\n##   x1 x2 x3\n## 1 -3  4 -1\n## 2  4 -5  1\n\n结果不含判别系数，查看分类结果：\n\npred &lt;- predict(fit)$class\ntable(df$y, pred)\n##    pred\n##      1  2\n##   1 10  2\n##   2  1  9\n\n也是3个分错了。"
  },
  {
    "objectID": "1020-discriminant.html#bayes判别分析",
    "href": "1020-discriminant.html#bayes判别分析",
    "title": "20  R语言判别分析",
    "section": "20.2 Bayes判别分析",
    "text": "20.2 Bayes判别分析\n贝叶斯判别也是根据概率大小进行判别，要求各类近似服从多元正态分布。当各类的协方差相等时，可得到线性贝叶斯判别函数，当各类的协方差不相等时，可得到二次贝叶斯判别函数。\n欲用4个标化后的影像学指标鉴别脑囊肿（1）、胶质瘤（2）、转移瘤（3），收集了17个病例，试建立判别贝叶斯函数。\n\ndf &lt;- read.csv(\"datasets/例20-4.csv\")\n\ndf$y &lt;- factor(df$y)\n\npsych::headTail(df)\n##       x1    x2  x3  x4    y\n## 1      6 -11.5  19  90    1\n## 2    -11 -18.5  25 -36    3\n## 3   90.2   -17  17   3    2\n## 4     -4   -15  13  54    1\n## ...  ...   ... ... ... &lt;NA&gt;\n## 14    10   -18  14  50    1\n## 15    -8   -14  16  56    1\n## 16   0.6   -13  26  21    3\n## 17   -40   -20  22 -50    3\n\n使用klaR包实现贝叶斯判别分析：\n\nlibrary(klaR)\n\nfit &lt;- NaiveBayes(y ~ ., data = df)\nfit\n## $apriori\n## grouping\n##         1         2         3 \n## 0.4117647 0.2352941 0.3529412 \n## \n## $tables\n## $tables$x1\n##        [,1]     [,2]\n## 1 -14.42857 38.26163\n## 2   0.80000 78.10779\n## 3  -6.65000 19.78017\n## \n## $tables$x2\n##        [,1]     [,2]\n## 1 -17.34286 4.103599\n## 2 -17.42500 3.085855\n## 3 -17.33333 4.143268\n## \n## $tables$x3\n##       [,1]     [,2]\n## 1 12.71429 4.990467\n## 2 17.50000 2.081666\n## 3 20.16667 6.493587\n## \n## $tables$x4\n##        [,1]     [,2]\n## 1  31.14286 44.03948\n## 2   0.00000 30.75711\n## 3 -15.00000 35.83295\n## \n## \n## $levels\n## [1] \"1\" \"2\" \"3\"\n## \n## $call\n## NaiveBayes.default(x = X, grouping = Y)\n## \n## $x\n##        x1    x2 x3  x4\n## 1     6.0 -11.5 19  90\n## 2   -11.0 -18.5 25 -36\n## 3    90.2 -17.0 17   3\n## 4    -4.0 -15.0 13  54\n## 5     0.0 -14.0 20  35\n## 6     0.5 -11.5 19  37\n## 7   -10.0 -19.0 21 -42\n## 8     0.0 -23.0  5 -35\n## 9    20.0 -22.0  8 -20\n## 10 -100.0 -21.4  7 -15\n## 11 -100.0 -21.5 15 -40\n## 12   13.0 -17.2 18   2\n## 13   -5.0 -18.5 15  18\n## 14   10.0 -18.0 14  50\n## 15   -8.0 -14.0 16  56\n## 16    0.6 -13.0 26  21\n## 17  -40.0 -20.0 22 -50\n## \n## $usekernel\n## [1] FALSE\n## \n## $varnames\n## [1] \"x1\" \"x2\" \"x3\" \"x4\"\n## \n## attr(,\"class\")\n## [1] \"NaiveBayes\"\n\n获取预测结果，并查看混淆矩阵：\n\npred &lt;- predict(fit)$class\n## Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n## observation 10\ntable(pred, df$y)\n##     \n## pred 1 2 3\n##    1 7 0 1\n##    2 0 3 0\n##    3 0 1 5\n\n只有两个分错了。\n如果要预测新的数据，只需要predict(fit, newdata = xxx)即可。"
  },
  {
    "objectID": "1021-cluster.html#系统聚类层次聚类hierarchical-clustering",
    "href": "1021-cluster.html#系统聚类层次聚类hierarchical-clustering",
    "title": "21  R语言聚类分析",
    "section": "21.1 系统聚类（层次聚类,Hierarchical clustering）",
    "text": "21.1 系统聚类（层次聚类,Hierarchical clustering）\n\n# 没安装flexclust包的需要先安装\ndata(nutrient, package = \"flexclust\")\nrow.names(nutrient) &lt;- tolower(row.names(nutrient))\n\ndim(nutrient) # 27行5列\n## [1] 27  5\n\npsych::headTail(nutrient)\n##                 energy protein fat calcium iron\n## beef braised       340      20  28       9  2.6\n## hamburger          245      21  17       9  2.7\n## beef roast         420      15  39       7    2\n## beef steak         375      19  32       9  2.6\n## ...                ...     ... ...     ...  ...\n## salmon canned      120      17   5     159  0.7\n## sardines canned    180      22   9     367  2.5\n## tuna canned        170      25   7       7  1.2\n## shrimp canned      110      23   1      98  2.6\n\n层次聚类在R语言中非常简单，通过hclust实现。\n\n# 聚类前先进行标准化\nnutrient.scaled &lt;- scale(nutrient)\nh.clust &lt;- hclust(dist(nutrient.scaled,method = \"euclidean\"), # 计算距离有不同方法\n                  method = \"average\" # 层次聚类有不同方法\n                  )\n\n下面就是画图，简单点可以直接用plot()。\n\nplot(h.clust,hang = -1,main = \"层次聚类\", sub=\"\", \n     xlab=\"\", cex.lab = 1.0, cex.axis = 1.0, cex.main = 2)\n\n\n\n\n关于更加精细化的细节修改，下面会介绍。或者可以借助其他R包快速绘制好看的聚类分析图形。\n如何选择聚类的个数呢？\n可以通过R包NbClust实现。\n\nlibrary(NbClust)\n\nnc &lt;- NbClust(nutrient.scaled, distance = \"euclidean\",\n              min.nc = 2, # 最小聚类数\n              max.nc = 10, # 最大聚类树\n              method = \"average\"\n              )\n\n\n\n## *** : The Hubert index is a graphical method of determining the number of clusters.\n##                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n##                 significant increase of the value of the measure i.e the significant peak in Hubert\n##                 index second differences plot. \n## \n\n\n\n## *** : The D index is a graphical method of determining the number of clusters. \n##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n##                 second differences plot) that corresponds to a significant increase of the value of\n##                 the measure. \n##  \n## ******************************************************************* \n## * Among all indices:                                                \n## * 5 proposed 2 as the best number of clusters \n## * 5 proposed 3 as the best number of clusters \n## * 2 proposed 4 as the best number of clusters \n## * 4 proposed 5 as the best number of clusters \n## * 1 proposed 8 as the best number of clusters \n## * 1 proposed 9 as the best number of clusters \n## * 5 proposed 10 as the best number of clusters \n## \n##                    ***** Conclusion *****                            \n##  \n## * According to the majority rule, the best number of clusters is  2 \n##  \n##  \n## *******************************************************************\n\n输出日志里给出了评判准则以及最终结果：Hubert index和D index使用图形的方式判断最佳聚类个数，拐点明显的可视作最佳聚类个数。\n它给出的结论是最佳聚类数是2。我们也可以通过条形图查看这些评判准则的具体数量。\n\nbarplot(table(nc$Best.nc[1,]),\n        xlab = \"聚类数目\",\n        ylab = \"评判准则个数\"\n        )\n\n\n\n\n从条形图中可以看出，聚类数目为2,3,5,10时，评判准则个数最多，为5个，这里我们可以选择5个。\n\n# 把聚类树划分为5类\ncluster &lt;- cutree(h.clust, k=5)\n\n# 查看每一类有多少例\ntable(cluster)\n## cluster\n##  1  2  3  4  5 \n##  7 16  1  2  1\n\n把最终结果画出来：\n\nplot(h.clust, hang = -1,main = \"\",xlab = \"\")\nrect.hclust(h.clust, k=5) # 添加矩形，方便观察"
  },
  {
    "objectID": "1021-cluster.html#聚类分析可视化",
    "href": "1021-cluster.html#聚类分析可视化",
    "title": "21  R语言聚类分析",
    "section": "21.2 聚类分析可视化",
    "text": "21.2 聚类分析可视化\n上面使用默认的plot函数进行聚类树的可视化，下面继续扩展聚类树的可视化。\n默认的聚类树可视化函数已经非常好用，有非常多的自定义设置，可以轻松实现好看的聚类树可视化。\n\nop &lt;- par(bg = \"grey90\")\n\nplot(h.clust, main = \"层次聚类\", sub=\"\", xlab = \"\",\n     col = \"#487AA1\", col.main = \"#45ADA8\", col.lab = \"#7C8071\",\n     col.axis = \"#F38630\", lwd = 2, lty = 1, hang = -1, axes = FALSE)\n# add axis\naxis(side = 2, at = 0:5, col = \"#F38630\",\n     labels = FALSE, lwd = 2)\n# add text in margin\nmtext(0:5, side = 2, at = 0:5,\n      line = 1, col = \"#A38630\", las = 2)\n\n\n\n\npar(op)\n\n如果对默认的可视化效果不满意，可以先用as.dendrogram()转化一下，再画图可以指定更多细节。\n\ndhc &lt;- as.dendrogram(h.clust)\nplot(dhc,type = \"triangle\") # 比如换个类型\n\n\n\n\n可以提取部分树进行查看，使用cut指定某个高度以上或以下的树进行查看。\n\nop &lt;- par(mfrow = c(2, 1))\n\n# 高度在3以上的树\nplot(cut(dhc, h = 3)$upper, main = \"Upper tree of cut at h=3\")\n\n# 高度在3以下的树\nplot(cut(dhc, h = 3)$lower[[2]],\n     main = \"Second branch of lower tree with cut at h=3\")\n\n\n\n\npar(op)\n\n每一个节点都有不同的属性，比如颜色、形状等，我们可以用函数修改每个节点的属性。\n比如修改标签的颜色。\n\n# 按照上面画出来的结果，我们可以分为5类，所以准备好5个颜色\nlabelColors = c(\"#CDB380\", \"#036564\", \"#EB6841\", \"#EDC951\", \"#487AA1\")\n\n# 把聚类树分为5个类\nclusMember &lt;- cutree(h.clust,k=5)\n\n# 给标签增加不同的颜色\ncolLab &lt;- function(n) {\n  if (is.leaf(n)) {\n    a &lt;- attributes(n)\n    labCol &lt;- labelColors[clusMember[which(names(clusMember) == a$label)]]\n    attr(n, \"nodePar\") &lt;- c(a$nodePar,\n                            list(cex=1.5, # 节点形状大小\n                                 pch=20, # 节点形状\n                                 col=labCol, # 节点颜色\n                                 lab.col=labCol, # 标签颜色\n                                 lab.font=2, # 标签字体，粗体斜体粗斜体\n                                 lab.cex=1 # 标签大小\n                                 )\n                            )\n  }\n  n\n}\n\n# 把自定义标签颜色应用到聚类树中\ndiyDendro = dendrapply(dhc, colLab)    \n\n# 画图\nplot(diyDendro, main = \"DIY Dendrogram\")  \n\n# 加图例\nlegend(\"topright\", \n     legend = c(\"Cluster 1\",\"Cluster 2\",\"Cluster 3\",\"Cluster 4\",\"Cluster 5\"), \n     col = c(\"#CDB380\", \"#036564\", \"#EB6841\", \"#EDC951\", \"#487AA1\"), \n     pch = c(20,20,20,20,20), bty = \"n\", pt.cex = 2, cex = 1 , \n     text.col = \"black\", horiz = FALSE, inset = c(0, 0.1))\n\n\n\n\n如果想要更加精美的聚类分析可视化，可以参考之前的几篇推文：\n\n又是聚类分析可视化\nR语言可视化聚类树\nR语言画好看的聚类树\n\n参考资料：\n\nR帮助文档\nhttps://r-graph-gallery.com/31-custom-colors-in-dendrogram.html\nhttps://www.gastonsanchez.com/visually-enforced/how-to/2012/10/0/Dendrograms/"
  },
  {
    "objectID": "1021-cluster.html#快速聚类划分聚类partitioning-clustering",
    "href": "1021-cluster.html#快速聚类划分聚类partitioning-clustering",
    "title": "21  R语言聚类分析",
    "section": "21.3 快速聚类（划分聚类,partitioning clustering）",
    "text": "21.3 快速聚类（划分聚类,partitioning clustering）\n\n21.3.1 K-means聚类\nK-means聚类，K均值聚类，是快速聚类的一种。比层次聚类更适合大样本的数据。在R语言中可以通过kmeans()实现K均值聚类。\n使用K均值聚类处理178种葡萄酒中13种化学成分的数据集。\n\ndata(wine, package = \"rattle\")\ndf &lt;- scale(wine[,-1])\n\npsych::headTail(df)\n##     Alcohol Malic   Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids\n## 1      1.51 -0.56  0.23      -1.17      1.91    0.81       1.03         -0.66\n## 2      0.25  -0.5 -0.83      -2.48      0.02    0.57       0.73         -0.82\n## 3       0.2  0.02  1.11      -0.27      0.09    0.81       1.21          -0.5\n## 4      1.69 -0.35  0.49      -0.81      0.93    2.48       1.46         -0.98\n## ...     ...   ...   ...        ...       ...     ...        ...           ...\n## 175    0.49  1.41  0.41       1.05      0.16   -0.79      -1.28          0.55\n## 176    0.33  1.74 -0.39       0.15      1.42   -1.13      -1.34          0.55\n## 177    0.21  0.23  0.01       0.15      1.42   -1.03      -1.35          1.35\n## 178    1.39  1.58  1.36        1.5     -0.26   -0.39      -1.27          1.59\n##     Proanthocyanins Color   Hue Dilution Proline\n## 1              1.22  0.25  0.36     1.84    1.01\n## 2             -0.54 -0.29   0.4     1.11    0.96\n## 3              2.13  0.27  0.32     0.79    1.39\n## 4              1.03  1.18 -0.43     1.18    2.33\n## ...             ...   ...   ...      ...     ...\n## 175           -0.32  0.97 -1.13    -1.48    0.01\n## 176           -0.42  2.22 -1.61    -1.48    0.28\n## 177           -0.23  1.83 -1.56     -1.4     0.3\n## 178           -0.42  1.79 -1.52    -1.42   -0.59\n\n进行K均值聚类时，需要在一开始就指定聚类的个数，我们也可以通过NbClust包实现这个过程。\n\nlibrary(NbClust)\n\nset.seed(123)\nnc &lt;- NbClust(df, min.nc = 2, max.nc = 15, method = \"kmeans\")# 方法选择kmeans\n\n\n\n## *** : The Hubert index is a graphical method of determining the number of clusters.\n##                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n##                 significant increase of the value of the measure i.e the significant peak in Hubert\n##                 index second differences plot. \n## \n\n\n\n## *** : The D index is a graphical method of determining the number of clusters. \n##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n##                 second differences plot) that corresponds to a significant increase of the value of\n##                 the measure. \n##  \n## ******************************************************************* \n## * Among all indices:                                                \n## * 2 proposed 2 as the best number of clusters \n## * 19 proposed 3 as the best number of clusters \n## * 1 proposed 14 as the best number of clusters \n## * 1 proposed 15 as the best number of clusters \n## \n##                    ***** Conclusion *****                            \n##  \n## * According to the majority rule, the best number of clusters is  3 \n##  \n##  \n## *******************************************************************\n\n结果中给出了划分依据以及最佳的聚类数目为3个，可以画图查看结果：\n\ntable(nc$Best.nc[1,])\n## \n##  0  1  2  3 14 15 \n##  2  1  2 19  1  1\n\nbarplot(table(nc$Best.nc[1,]),\n        xlab = \"聚类数目\",\n        ylab = \"评判准则个数\"\n        )\n\n\n\n\n可以看到聚类数目为3是最佳的选择。\n确定最佳聚类个数过程也可以通过非常好用的R包factoextra实现。\n\nlibrary(factoextra)\n## Loading required package: ggplot2\n## Warning: package 'ggplot2' was built under R version 4.2.3\n## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nset.seed(123)\nfviz_nbclust(df, kmeans, k.max = 15)\n\n\n\n\n这个结果给出的最佳聚类个数也是3个。\n下面进行K均值聚类，聚类数目设为3.\n\nset.seed(123)\nfit.km &lt;- kmeans(df, centers = 3, nstart = 25)\nfit.km\n## K-means clustering with 3 clusters of sizes 51, 62, 65\n## \n## Cluster means:\n##      Alcohol      Malic        Ash Alcalinity   Magnesium     Phenols\n## 1  0.1644436  0.8690954  0.1863726  0.5228924 -0.07526047 -0.97657548\n## 2  0.8328826 -0.3029551  0.3636801 -0.6084749  0.57596208  0.88274724\n## 3 -0.9234669 -0.3929331 -0.4931257  0.1701220 -0.49032869 -0.07576891\n##    Flavanoids Nonflavanoids Proanthocyanins      Color        Hue   Dilution\n## 1 -1.21182921    0.72402116     -0.77751312  0.9388902 -1.1615122 -1.2887761\n## 2  0.97506900   -0.56050853      0.57865427  0.1705823  0.4726504  0.7770551\n## 3  0.02075402   -0.03343924      0.05810161 -0.8993770  0.4605046  0.2700025\n##      Proline\n## 1 -0.4059428\n## 2  1.1220202\n## 3 -0.7517257\n## \n## Clustering vector:\n##   [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [38] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 1 3 3 3 3 3 3 3 3 3 3 3 2\n##  [75] 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 1 3 3 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n## [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n## \n## Within cluster sum of squares by cluster:\n## [1] 326.3537 385.6983 558.6971\n##  (between_SS / total_SS =  44.8 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n\n结果很详细，K均值聚类聚为3类，每一类数量分别是51,62,65。然后还给出了聚类中心，每一个观测分别属于哪一个类。\n不管是哪一种聚类方法，factoextra配合factomineR都可以给出非常好看的可视化结果。\n\nfviz_cluster(fit.km, data = df)\n\n\n\n\n有非常多的细节可以调整，大家在使用的时候可以自己尝试，和之前推文中介绍的PCA美化一样，也是支持ggplot2语法的。\n\nfviz_cluster(fit.km, data = df, \n             ellipse = T, # 增加椭圆\n             ellipse.type = \"t\", # 椭圆类型\n             geom = \"point\", # 只显示点不要文字\n             palette = \"lancet\", # 支持超多配色方案\n             ggtheme = theme_bw() # 支持更换主题\n             )\n\n\n\n\n\n\n21.3.2 围绕中心点的划分PAM\nK均值聚类是基于均值的，所以对异常值很敏感。一个更稳健的方法是围绕中心点的划分（PAM）。用一个最有代表性的观测值代表这一类(有点类似于主成分)。K均值聚类一般使用欧几里得距离，而PAM可以使用任意的距离来计算。因此，PAM可以容纳混合数据类型，并且不仅限于连续变量。\n我们还是用葡萄酒数据进行演示。PAM聚类可以通过cluster包中的pam()实现。\n\nlibrary(cluster)\n\nset.seed(123)\nfit.pam &lt;- pam(wine[-1,], k=3 # 聚为3类\n               , stand = T # 聚类前进行标准化\n               )\nfit.pam\n## Medoids:\n##      ID Type Alcohol Malic  Ash Alcalinity Magnesium Phenols Flavanoids\n## 36   35    1   13.48  1.81 2.41       20.5       100    2.70       2.98\n## 107 106    2   12.25  1.73 2.12       19.0        80    1.65       2.03\n## 149 148    3   13.32  3.24 2.38       21.5        92    1.93       0.76\n##     Nonflavanoids Proanthocyanins Color  Hue Dilution Proline\n## 36           0.26            1.86  5.10 1.04     3.47     920\n## 107          0.37            1.63  3.40 1.00     3.17     510\n## 149          0.45            1.25  8.42 0.55     1.62     650\n## Clustering vector:\n##   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21 \n##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n##  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41 \n##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n##  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61 \n##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   2 \n##  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81 \n##   2   2   1   2   1   2   2   2   1   2   1   2   1   1   2   2   2   2   1   2 \n##  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 \n##   2   2   3   2   2   2   2   2   2   2   2   2   2   2   1   1   2   1   2   2 \n## 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 \n##   2   2   2   2   2   2   2   2   1   2   2   2   2   2   2   2   2   2   2   2 \n## 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 \n##   1   2   2   2   2   2   2   2   2   3   3   3   3   3   3   3   3   3   3   3 \n## 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 \n##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n## 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 \n##   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n## Objective function:\n##    build     swap \n## 3.537365 3.504175 \n## \n## Available components:\n##  [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n##  [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"\n\nMedoids给出了中心点，用第35个观测代表第1类，第107个观测代表第2类，第149个观测代表第3类。Clustering vector给出了每一个观测分别属于哪一个类。结果可以画出来：\n\nclusplot(fit.pam, main = \"PAM cluster\")\n\n\n\n\n同样也可以用factoextra包实现可视化。\n\nfviz_cluster(fit.pam, \n             ellipse = T, # 增加椭圆\n             ellipse.type = \"t\", # 椭圆类型\n             geom = \"point\", # 只显示点不要文字\n             palette = \"aaas\", # 支持超多配色方案\n             ggtheme = theme_bw() # 支持更换主题\n             )\n\n\n\n\n以后会给大家带来factoextra和factomineR包的详细介绍。"
  },
  {
    "objectID": "1022-pca.html#加载数据",
    "href": "1022-pca.html#加载数据",
    "title": "22  R语言主成分分析",
    "section": "22.1 加载数据",
    "text": "22.1 加载数据\n使用R语言自带的iris鸢尾花数据进行演示。\n\nstr(iris)\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\npsych::headTail(iris)\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 1            5.1         3.5          1.4         0.2    setosa\n## 2            4.9           3          1.4         0.2    setosa\n## 3            4.7         3.2          1.3         0.2    setosa\n## 4            4.6         3.1          1.5         0.2    setosa\n## ...          ...         ...          ...         ...      &lt;NA&gt;\n## 147          6.3         2.5            5         1.9 virginica\n## 148          6.5           3          5.2           2 virginica\n## 149          6.2         3.4          5.4         2.3 virginica\n## 150          5.9           3          5.1         1.8 virginica\n\n首先给大家介绍下R自带的主成分分析函数。"
  },
  {
    "objectID": "1022-pca.html#相关性检验",
    "href": "1022-pca.html#相关性检验",
    "title": "22  R语言主成分分析",
    "section": "22.2 相关性检验",
    "text": "22.2 相关性检验\n在进行PCA之前可以先进行相关性分析，看看相关系数：\n\ncor(iris[,-5])\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\n## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\n## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\n## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000"
  },
  {
    "objectID": "1022-pca.html#kmo和bartlett球形检验",
    "href": "1022-pca.html#kmo和bartlett球形检验",
    "title": "22  R语言主成分分析",
    "section": "22.3 KMO和Bartlett球形检验",
    "text": "22.3 KMO和Bartlett球形检验\n使用psych实现，关于这两个检验的解读大家自行学习~\n\npsych::KMO(iris[,-5])\n## Kaiser-Meyer-Olkin factor adequacy\n## Call: psych::KMO(r = iris[, -5])\n## Overall MSA =  0.54\n## MSA for each item = \n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##         0.58         0.27         0.53         0.63\n\n这个检验主要反应样本量够不够，Overall MSA是总体的检验统计量，然后是每个变量的检验统计量。 MSA越大越好。一般要求大于0.5才可以（没有绝对标准，根据实际情况来）。\n\npsych::cortest.bartlett(iris[,-5])\n## R was not square, finding R from data\n## $chisq\n## [1] 706.9592\n## \n## $p.value\n## [1] 1.92268e-149\n## \n## $df\n## [1] 6\n\np.value小于0.05，表明数据可以进行主成分分析。"
  },
  {
    "objectID": "1022-pca.html#r自带的pca",
    "href": "1022-pca.html#r自带的pca",
    "title": "22  R语言主成分分析",
    "section": "22.4 R自带的PCA",
    "text": "22.4 R自带的PCA\n主成分的实现可以通过分步计算，主要就是标准化-求相关矩阵-计算特征值和特征向量。\nR中自带了prcomp()进行主成分分析，这就是工具的魅力，一次完成多步需求。\n使用prcomp()进行主成分分析：\n\n# R自带函数\npca.res &lt;- prcomp(iris[,-5], scale. = T, # 标准化\n                  center = T # 中心化\n                  )\n\n# 查看标准差、特征向量（回归系数）\npca.res\n## Standard deviations (1, .., p=4):\n## [1] 1.7083611 0.9560494 0.3830886 0.1439265\n## \n## Rotation (n x k) = (4 x 4):\n##                     PC1         PC2        PC3        PC4\n## Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\n## Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\n## Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\n## Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n主成分就是根据这几个系数(这几个系数也叫主成分载荷)算出来的：\nPC1 = 0.5210659Sepal.Length - 0.2693474Sepal.Width + 0.5804131Petal.Length + 0.5648565Petal.Width\n后面的主成分计算方法以此类推。\n\n# 样本得分score\nhead(pca.res$x)\n##            PC1        PC2         PC3          PC4\n## [1,] -2.257141 -0.4784238  0.12727962  0.024087508\n## [2,] -2.074013  0.6718827  0.23382552  0.102662845\n## [3,] -2.356335  0.3407664 -0.04405390  0.028282305\n## [4,] -2.291707  0.5953999 -0.09098530 -0.065735340\n## [5,] -2.381863 -0.6446757 -0.01568565 -0.035802870\n## [6,] -2.068701 -1.4842053 -0.02687825  0.006586116\n\n\n# 查看标准差、方差贡献率、累积方差贡献率\nsummary(pca.res)\n## Importance of components:\n##                           PC1    PC2     PC3     PC4\n## Standard deviation     1.7084 0.9560 0.38309 0.14393\n## Proportion of Variance 0.7296 0.2285 0.03669 0.00518\n## Cumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\nStandard deviation:标准差\nProportion of Variance:方差贡献率\nCumulative Proportion:累积方差贡献率\n\n关于主成分分析中的各种术语解读，我推荐知乎上的一篇文章：主成分分析各类术语的白话解读"
  },
  {
    "objectID": "1022-pca.html#结果可视化",
    "href": "1022-pca.html#结果可视化",
    "title": "22  R语言主成分分析",
    "section": "22.5 结果可视化",
    "text": "22.5 结果可视化\n默认的主成分分析结果可视化：\n\nbiplot(pca.res)\n\n\n\n\n碎石图可以帮助确认最佳的主成分个数，可以使用默认的screeplot()实现：\n\n# 默认是条形图，我们改为折线图，其实就是方差贡献度的可视化\nscreeplot(pca.res, type = \"lines\")\n\n\n\n\n可以看到用2-3个主成分就挺好了。\n\n一般来说，主成分的保留个数可以按照以下原则确定： 1. 以累积贡献率确定，当前K个主成分的累积贡献率达到某一特定值（一般选70%或者80%都行）时，则保留前K个主成分； 2. 以特征值大小来确定：如果主成分的特征值大于1，就保留这个主成分。\n\n但是保留几个主成分并没有绝对的标准，大家根据自己的实际情况来！\n今天只是小试牛刀，后面会为大家带来更加详细的主成分分析可视化。"
  },
  {
    "objectID": "1022-pca.html#主成分分析可视化超详细",
    "href": "1022-pca.html#主成分分析可视化超详细",
    "title": "22  R语言主成分分析",
    "section": "22.6 主成分分析可视化(超详细)",
    "text": "22.6 主成分分析可视化(超详细)\n网络上很多R语言教程都是基于R语言实战进行修改，今天为大家介绍更好用的R包，在之前聚类分析中也经常用到：factoextra和factoMineR，关于主成分分析的可视化，大家比较常见的可能是ggbiplot，这几个R包都挺不错，大家可以比较下。\n这两个R包的函数可以直接使用prcomp()函数的结果，也可以使用FactoMineR的PCA()函数进行，结果更加详细。\n\n22.6.1 进行PCA分析\n使用R语言自带的iris鸢尾花数据进行演示。\n\nrm(list = ls())\nlibrary(factoextra)\n## Loading required package: ggplot2\n## Warning: package 'ggplot2' was built under R version 4.2.3\n## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nlibrary(FactoMineR)\n\npca.res &lt;- PCA(iris[,-5], graph = F, scale.unit = T) # 简简单单1行代码实现主成分分析\npca.res\n## **Results for the Principal Component Analysis (PCA)**\n## The analysis was performed on 150 individuals, described by 4 variables\n## *The results are available in the following objects:\n## \n##    name               description                          \n## 1  \"$eig\"             \"eigenvalues\"                        \n## 2  \"$var\"             \"results for the variables\"          \n## 3  \"$var$coord\"       \"coord. for the variables\"           \n## 4  \"$var$cor\"         \"correlations variables - dimensions\"\n## 5  \"$var$cos2\"        \"cos2 for the variables\"             \n## 6  \"$var$contrib\"     \"contributions of the variables\"     \n## 7  \"$ind\"             \"results for the individuals\"        \n## 8  \"$ind$coord\"       \"coord. for the individuals\"         \n## 9  \"$ind$cos2\"        \"cos2 for the individuals\"           \n## 10 \"$ind$contrib\"     \"contributions of the individuals\"   \n## 11 \"$call\"            \"summary statistics\"                 \n## 12 \"$call$centre\"     \"mean of the variables\"              \n## 13 \"$call$ecart.type\" \"standard error of the variables\"    \n## 14 \"$call$row.w\"      \"weights for the individuals\"        \n## 15 \"$call$col.w\"      \"weights for the variables\"\n\n结果信息丰富，可以通过不断的$获取，也可以通过特定函数提取，下面介绍。\n\n\n22.6.2 特征值可视化\n获取特征值、方差贡献率和累积方差贡献率，可以看到和上一篇的结果是一样的：\n\nget_eigenvalue(pca.res)\n##       eigenvalue variance.percent cumulative.variance.percent\n## Dim.1 2.91849782       72.9624454                    72.96245\n## Dim.2 0.91403047       22.8507618                    95.81321\n## Dim.3 0.14675688        3.6689219                    99.48213\n## Dim.4 0.02071484        0.5178709                   100.00000\n\n结果中的这几个概念在上一篇已经解释过了：R语言主成分分析\n通过这几个值，可以确定主成分个数，当然也可以通过碎石图（就是方差解释度的可视化）直观的观察：\n\nfviz_eig(pca.res,addlabels = T,ylim=c(0,100))\n\n\n\n\n\n\n22.6.3 提取变量结果\n通过get_pca_var()`函数实现：\n\nres.var &lt;- get_pca_var(pca.res)\nres.var$cor\n##                   Dim.1      Dim.2       Dim.3       Dim.4\n## Sepal.Length  0.8901688 0.36082989 -0.27565767 -0.03760602\n## Sepal.Width  -0.4601427 0.88271627  0.09361987  0.01777631\n## Petal.Length  0.9915552 0.02341519  0.05444699  0.11534978\n## Petal.Width   0.9649790 0.06399985  0.24298265 -0.07535950\nres.var$coord          \n##                   Dim.1      Dim.2       Dim.3       Dim.4\n## Sepal.Length  0.8901688 0.36082989 -0.27565767 -0.03760602\n## Sepal.Width  -0.4601427 0.88271627  0.09361987  0.01777631\n## Petal.Length  0.9915552 0.02341519  0.05444699  0.11534978\n## Petal.Width   0.9649790 0.06399985  0.24298265 -0.07535950\nres.var$contrib       \n##                  Dim.1       Dim.2     Dim.3     Dim.4\n## Sepal.Length 27.150969 14.24440565 51.777574  6.827052\n## Sepal.Width   7.254804 85.24748749  5.972245  1.525463\n## Petal.Length 33.687936  0.05998389  2.019990 64.232089\n## Petal.Width  31.906291  0.44812296 40.230191 27.415396\nres.var$cos2        \n##                  Dim.1       Dim.2       Dim.3        Dim.4\n## Sepal.Length 0.7924004 0.130198208 0.075987149 0.0014142127\n## Sepal.Width  0.2117313 0.779188012 0.008764681 0.0003159971\n## Petal.Length 0.9831817 0.000548271 0.002964475 0.0133055723\n## Petal.Width  0.9311844 0.004095980 0.059040571 0.0056790544\n\n\nres.var$cor:变量和主成分的相关系数\nres.var$coord: 变量在主成分投影上的坐标，下面会结合图说明，因为进行了标准化，所以和相关系数结果一样，其数值代表了主成分和变量之间的相关性\nres.var$cos2: 是coord的平方，也是表示主成分和变量间的相关性，同一个变量所有cos2的总和是1\nres.var$contrib: 变量对主成分的贡献\n\n这几个结果都可以进行可视化。\n\n\n22.6.4 变量结果可视化\n使用fviz_pca_var()对变量结果进行可视化：\n\nfviz_pca_var(pca.res)\n\n\n\n\nres.var$coord是变量在主成分投影上的坐标，Sepal.Width在Dim.1的坐标是-0.4601427，在Dim.2的坐标是0.88271627，根据这两个坐标就画出来Sepal.Width那根线了，以此类推~\n\n22.6.4.1 变量和主成分的cos2可视化\ncos2是coord的平方，也是表示主成分和变量间的相关性，所以首先可以画相关图：\n\nlibrary(\"corrplot\")\n## corrplot 0.92 loaded\ncorrplot(res.var$cos2, is.corr = F)\n\n\n\n\n可以看到Petal.Length、Petal.Width和Dim1的相关性比较强，Sepal.Width和Dim2的相关性比较强。\n通过fviz_cos2()查看变量在不同主成分的总和，以下是不同变量在第1和第2主成分的加和，如果把axes = 1:2改成axes = 1:4，就会变成都是1（这个数据最多4个主成分，同一变量的cos2在所有主成分的总和是1）。\n\nfviz_cos2(pca.res, choice = \"var\", axes = 1:2)\n\n\n\n\n可以通过col.var = \"cos2\"参数给不同变量按照cos2的数值大小上色：\n\nfviz_pca_var(pca.res, col.var = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE \n             )\n\n\n\n\n\n# 黑白版本\nfviz_pca_var(pca.res, alpha.var = \"cos2\")\n\n\n\n\n\n\n22.6.4.2 变量对主成分的贡献可视化\n\nres.var$contrib\n##                  Dim.1       Dim.2     Dim.3     Dim.4\n## Sepal.Length 27.150969 14.24440565 51.777574  6.827052\n## Sepal.Width   7.254804 85.24748749  5.972245  1.525463\n## Petal.Length 33.687936  0.05998389  2.019990 64.232089\n## Petal.Width  31.906291  0.44812296 40.230191 27.415396\n\n首先也是可以通过画相关性图进行可视化：\n\nlibrary(\"corrplot\")\ncorrplot(res.var$contrib, is.corr=FALSE) \n\n\n\n\n通过fviz_contrib()可视化变量对不同主成分的贡献：\n\n# 对第1主成分的贡献\nfviz_contrib(pca.res, choice = \"var\", axes = 1)\n\n\n\n\n\n# 对第1和第2主成分的贡献\nfviz_contrib(pca.res, choice = \"var\", axes = 1:2)\n\n\n\n\n通过col.var = \"contrib\"参数给不同变量按照contrib的数值大小上色：\n\nfviz_pca_var(pca.res, col.var = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")\n             )\n\n\n\n\n\n\n\n22.6.5 Dimension description\n\nres.desc &lt;- dimdesc(pca.res, axes = c(1,2), proba = 0.05)\n# Description of dimension 1\nres.desc$Dim.1\n## \n## Link between the variable and the continuous variables (R-square)\n## =================================================================================\n##              correlation       p.value\n## Petal.Length   0.9915552 3.369916e-133\n## Petal.Width    0.9649790  6.609632e-88\n## Sepal.Length   0.8901688  2.190813e-52\n## Sepal.Width   -0.4601427  3.139724e-09\n\n\n\n22.6.6 提取样本结果\n使用get_pca_ind()提取样本结果，和变量结果类似：\n\nres.ind &lt;- get_pca_ind(pca.res)\n\nhead(res.ind$coord)          \n##       Dim.1      Dim.2       Dim.3       Dim.4\n## 1 -2.264703  0.4800266 -0.12770602 -0.02416820\n## 2 -2.080961 -0.6741336 -0.23460885 -0.10300677\n## 3 -2.364229 -0.3419080  0.04420148 -0.02837705\n## 4 -2.299384 -0.5973945  0.09129011  0.06595556\n## 5 -2.389842  0.6468354  0.01573820  0.03592281\n## 6 -2.075631  1.4891775  0.02696829 -0.00660818\nhead(res.ind$contrib)      \n##       Dim.1      Dim.2       Dim.3       Dim.4\n## 1 1.1715796 0.16806554 0.074085470 0.018798188\n## 2 0.9891845 0.33146674 0.250034006 0.341474919\n## 3 1.2768164 0.08526419 0.008875320 0.025915633\n## 4 1.2077372 0.26029781 0.037858004 0.140000650\n## 5 1.3046313 0.30516562 0.001125175 0.041530572\n## 6 0.9841236 1.61748779 0.003303827 0.001405371\nhead(res.ind$cos2)          \n##       Dim.1      Dim.2        Dim.3        Dim.4\n## 1 0.9539975 0.04286032 0.0030335249 1.086460e-04\n## 2 0.8927725 0.09369248 0.0113475382 2.187482e-03\n## 3 0.9790410 0.02047578 0.0003422122 1.410446e-04\n## 4 0.9346682 0.06308947 0.0014732682 7.690193e-04\n## 5 0.9315095 0.06823959 0.0000403979 2.104697e-04\n## 6 0.6600989 0.33978301 0.0001114335 6.690714e-06\n\n3个概念和变量的解释也是类似的，只不过上面是变量（列）和主成分的关系，现在是样本（观测，行）和主成分的关系。\n\n\n22.6.7 样本结果可视化\n样本的结果可视化可能是更常见的PCA图形，通过fviz_pca_ind()实现：\n\nfviz_pca_ind(pca.res)\n\n\n\n\n这个图是通过res.ind$coord里面的坐标实现的，其实就是不同样本在不同主成分的上面的得分score。\n默认的可视化比较简陋，但是可以通过超多参数实现各种精细化的控制，比如把不同的属性映射给点的大小和颜色，实现各种花里胡哨的效果。\n比如通过组别上色，就是大家最常见的PCA可视化图形：\n\n# 经典图形，是不是很熟悉？\nfviz_pca_ind(pca.res,\n             geom.ind = \"point\", # 只显示点，不要文字\n             col.ind = iris$Species, # 按照组别上色\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), # 自己提供颜色，或者使用主题\n             addEllipses = TRUE, # 添加置信椭圆\n             legend.title = \"Groups\"\n             )\n\n\n\n\n\n22.6.7.1 样本的cos2可视化\n使用方法和变量的cos2可视化基本一样，通过更改参数值即可实现：\n\nfviz_pca_ind(pca.res,\n             col.ind = \"cos2\", # 按照cos2上色\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE    \n             )\n\n\n\n\n可以更改点的大小、颜色等，只要设置合适的参数即可：\n\nfviz_pca_ind(pca.res, \n             pointsize = \"cos2\", # 把cos2的大小映射给点的大小\n             pointshape = 21, \n             fill = \"#E7B800\",\n             repel = TRUE \n             )\n\n\n\n\n同时更改点的大小和颜色当然也是支持的：\n\nfviz_pca_ind(pca.res, \n             col.ind = \"cos2\", # 控制颜色\n             pointsize = \"contrib\", # 控制大小\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE \n             )\n\n\n\n\n使用参数choice = \"ind\"可视化样本对不同主成分的cos2：\n\n# axes选择主成分\nfviz_cos2(pca.res, choice = \"ind\", axes = 1:2)\n\n\n\n\n\n\n22.6.7.2 样本对主成分的贡献可视化\n和变量对主成分的贡献可视化非常类似，简单演示下：\n\nfviz_contrib(pca.res, choice = \"ind\", axes = 1:2)\n\n\n\n\n\n\n\n22.6.8 biplot\n双标图…\n同时展示变量和样本和主成分的关系，超级多的自定义可视化细节。\n\n# 同时有箭头和椭圆\nfviz_pca_biplot(pca.res, \n                col.ind = iris$Species, \n                palette = \"jco\", \n                addEllipses = TRUE, \n                label = \"var\",\n                col.var = \"black\", \n                repel = TRUE,\n                legend.title = \"Species\"\n                ) \n\n\n\n\n\nfviz_pca_biplot(pca.res, \n                # 组别映射给点的填充色\n                geom.ind = \"point\",\n                pointshape = 21,\n                pointsize = 2.5,\n                fill.ind = iris$Species,\n                col.ind = \"black\",\n                # 通过自定义分组给变量上色\n                col.var = factor(c(\"sepal\", \"sepal\", \"petal\", \"petal\")),\n                # 自定义图例标题\n                legend.title = list(fill = \"Species\", color = \"Clusters\"),\n                repel = TRUE        \n             )+\n  ggpubr::fill_palette(\"jco\")+ # 选择点的填充色的配色\n  ggpubr::color_palette(\"npg\") # 选择变量颜色的配色\n\n\n\n\n\nfviz_pca_biplot(pca.res, \n                # 自定义样本部分\n                geom.ind = \"point\",\n                fill.ind = iris$Species, # 填充色\n                col.ind = \"black\", # 边框色\n                pointshape = 21, # 点的形状\n                pointsize = 2, \n                palette = \"jco\",\n                addEllipses = TRUE,\n                # 自定义变量部分\n                alpha.var =\"contrib\", col.var = \"contrib\",\n                gradient.cols = \"RdYlBu\",\n                \n                # 自定义图例标题\n                legend.title = list(fill = \"Species\", color = \"Contrib\",\n                                    alpha = \"Contrib\")\n                )\n\n\n\n\nfviz_xxx系列可视化函数底层是ggscatter的封装，这个函数来自ggpubr包，所有ggpubr支持的特性都可以给fviz_xxx函数使用，这也是这几个函数功能强大的原因，毕竟底层都是ggplot2!\n下载会继续给大家介绍如何提取PCA的数据，并使用ggplot2可视化，以及三维PCA图的实现。\nfactoextra和factoMineR在聚类分析、主成分分析、因子分析等方面都可以使用。\n参考资料：http://www.sthda.com/"
  },
  {
    "objectID": "1022-pca.html#主成分分析可视化3d版",
    "href": "1022-pca.html#主成分分析可视化3d版",
    "title": "22  R语言主成分分析",
    "section": "22.7 主成分分析可视化3d版",
    "text": "22.7 主成分分析可视化3d版\n之前详细介绍了R语言中的主成分分析，以及超级详细的主成分分析可视化方法，主要是基于factoextra和factoMineR两个神包。\n今天说一下如何提取数据用ggplot2画PCA图，以及三维PCA图。\n\n22.7.1 提取数据\n还是使用鸢尾花数据集。\n\nrm(list = ls())\n\npca.res &lt;- prcomp(iris[,-5], scale. = T, center = T)\npca.res\n## Standard deviations (1, .., p=4):\n## [1] 1.7083611 0.9560494 0.3830886 0.1439265\n## \n## Rotation (n x k) = (4 x 4):\n##                     PC1         PC2        PC3        PC4\n## Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\n## Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\n## Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\n## Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n在上一篇中提到过，经典的PCA图的横纵坐标其实就是不同样本在不同主成分中的得分，只要提取出来就可以用gplot2画了。\n\n# 提取得分\ntmp &lt;- as.data.frame(pca.res$x)\nhead(tmp)\n##         PC1        PC2         PC3          PC4\n## 1 -2.257141 -0.4784238  0.12727962  0.024087508\n## 2 -2.074013  0.6718827  0.23382552  0.102662845\n## 3 -2.356335  0.3407664 -0.04405390  0.028282305\n## 4 -2.291707  0.5953999 -0.09098530 -0.065735340\n## 5 -2.381863 -0.6446757 -0.01568565 -0.035802870\n## 6 -2.068701 -1.4842053 -0.02687825  0.006586116\n\n和原数据拼到一起就可以画图了：\n\ntmp$species &lt;- iris$Species\nhead(tmp)\n##         PC1        PC2         PC3          PC4 species\n## 1 -2.257141 -0.4784238  0.12727962  0.024087508  setosa\n## 2 -2.074013  0.6718827  0.23382552  0.102662845  setosa\n## 3 -2.356335  0.3407664 -0.04405390  0.028282305  setosa\n## 4 -2.291707  0.5953999 -0.09098530 -0.065735340  setosa\n## 5 -2.381863 -0.6446757 -0.01568565 -0.035802870  setosa\n## 6 -2.068701 -1.4842053 -0.02687825  0.006586116  setosa\n\n\nlibrary(ggplot2)\nlibrary(ggsci)\n\nggplot(tmp, aes(PC1, PC2))+\n  geom_point(aes(color = species))+\n  stat_ellipse(aes(fill=species), alpha = 0.2,\n               geom =\"polygon\",type = \"norm\")+\n  scale_fill_aaas()+\n  scale_color_aaas()+\n  theme_bw()\n\n\n\n\n\n\n22.7.2 3d版\n其实就是使用3个主成分，之前介绍过一种：使用R语言美化PCA图，使用方法非常简单，也是在文献中学习到的。\n\n\n\n\n\n\n\n\n\n今天再介绍下scatterplot3d包。\n\nlibrary(scatterplot3d)\n\nscatterplot3d(tmp[,1:3], # 第1-3主成分\n              # 颜色长度要和样本长度一样，且对应！\n              color = rep(c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),each=50),\n              pch = 15,\n              lty.hide = 2\n              )\nlegend(\"topleft\",c('Setosa','Versicolor','Virginica'),\nfill=c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),box.col=NA)"
  },
  {
    "objectID": "1023-factoranalysis.html#演示数据",
    "href": "1023-factoranalysis.html#演示数据",
    "title": "23  R语言因子分析",
    "section": "23.1 演示数据",
    "text": "23.1 演示数据\n数据来自于孙振球医学统计学第4版例22-2.\n某医院为了评价医疗工作质量，收集了三年的门诊人次、出院人数、病床利用率、病床周转次数、平均住院天数、治愈好转率、病死率、诊断符合率、抢救成功率9个指标，采用因子分析方法，探讨其综合评价体系。\n\ndf &lt;- foreign::read.spss(\"datasets/例22-02.sav\",to.data.frame = T,\n                         reencode = \"utf-8\")\n## re-encoding from utf-8\nnames(df) &lt;- c(\"年\",\"月\",\"门诊人次\",\"出院人数\",\"病床利用率\",\"病床周转次数\",\n               \"平均住院天数\",\"治愈好转率\",\"病死率\",\"诊断符合率\",\"抢救成功率\")\n\nstr(df)\n## 'data.frame':    36 obs. of  11 variables:\n##  $ 年          : num  1991 1991 1991 1991 1991 ...\n##  $ 月          : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ 门诊人次    : num  4.34 3.45 4.38 4.18 4.32 4.13 4.57 4.31 4.06 4.43 ...\n##  $ 出院人数    : num  389 271 385 377 378 349 361 209 425 458 ...\n##  $ 病床利用率  : num  99.1 88.3 104 99.5 102 ...\n##  $ 病床周转次数: num  1.23 0.85 1.21 1.19 1.19 1.1 1.14 0.52 0.93 0.95 ...\n##  $ 平均住院天数: num  25.5 23.6 26.5 26.9 27.6 ...\n##  $ 治愈好转率  : num  93.2 94.3 92.5 93.9 93.2 ...\n##  $ 病死率      : num  3.56 2.44 4.02 2.92 1.99 4.38 2.73 3.65 3.09 4.21 ...\n##  $ 诊断符合率  : num  97.5 97.9 98.5 99.4 99.7 ...\n##  $ 抢救成功率  : num  61.7 73.3 76.8 63.2 80 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \".·.\" \".....˴.\" \"..Ժ....\" ...\npsych::headTail(df)\n##       年  月 门诊人次 出院人数 病床利用率 病床周转次数 平均住院天数 治愈好转率\n## 1   1991   1     4.34      389      99.06         1.23        25.46      93.15\n## 2   1991   2     3.45      271      88.28         0.85        23.55      94.31\n## 3   1991   3     4.38      385     103.97         1.21        26.54      92.53\n## 4   1991   4     4.18      377      99.48         1.19        26.89      93.86\n## ...  ... ...      ...      ...        ...          ...          ...        ...\n## 33  1993   9      3.9      555      80.58          1.1        23.08      94.38\n## 34  1993  10     3.62      554      87.21          1.1         22.5      92.43\n## 35  1993  11     3.75      586      90.31         1.12        23.73      92.47\n## 36  1993  12     3.77      627      86.47         1.24        23.22      91.17\n##     病死率 诊断符合率 抢救成功率\n## 1     3.56      97.51      61.66\n## 2     2.44      97.94      73.33\n## 3     4.02      98.48      76.79\n## 4     2.92      99.41      63.16\n## ...    ...        ...        ...\n## 33    2.06      96.82      91.79\n## 34    3.22      97.16      87.77\n## 35    2.07      97.74      93.89\n## 36     3.4      98.98       89.8"
  },
  {
    "objectID": "1023-factoranalysis.html#判断需要提取的因子个数",
    "href": "1023-factoranalysis.html#判断需要提取的因子个数",
    "title": "23  R语言因子分析",
    "section": "23.2 判断需要提取的因子个数",
    "text": "23.2 判断需要提取的因子个数\nR中自带了factanal()进行因子分析，不过不如psych包好用，我们这里使用psych包演示。\n\n# 只用后面9列数据\ndf.use &lt;- df[,-c(1,2)]\n\nlibrary(psych)\n\n# 碎石图\nfa.parallel(df.use, fa = \"both\",fm=\"ml\")\n\n\n\n## Parallel analysis suggests that the number of factors =  3  and the number of components =  3\n\n通过参数fa = \"both\"同时给出了PCA和因子分析的碎石图，根据因子分析碎石图的结果，建议我们提取3个因子。\n但是提取几个因子并没有绝对的标准，我们可以结合多种方法或专业知识，和提取主成分的方法类似，可以参考之前的介绍的方法：R语言主成分分析\n除此之外，还可以结合特征值大小、累计贡献率来确定使用几个因子。\n下面我们首先用9个因子进行因子分析，看看结果再说。\nrotate参数确定旋转方法，有多种不同的选择，比如不旋转、正交旋转法（比如最大方差法）、斜交旋转法等，\nfm参数选择因子计算方法，比如最大似然法ml、主轴迭代法pa、加权最小二乘wls、广义加权最小二乘gls、最小残差minres，等。\n\n# 进行因子分析，首先9个因子用一下看看结果再说，最大似然法，不旋转\nfa.res &lt;- fa(df.use, nfactors = 9, rotate = \"none\", fm=\"ml\")\nfa.res\n## Factor Analysis using method =  ml\n## Call: fa(r = df.use, nfactors = 9, rotate = \"none\", fm = \"ml\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                ML1   ML2   ML3   ML4   ML5   ML6 ML7 ML8 ML9   h2   u2 com\n## 门诊人次      0.14  0.68  0.09  0.37 -0.06 -0.03   0   0   0 0.63 0.37 1.7\n## 出院人数      0.68 -0.30  0.19  0.32  0.06  0.03   0   0   0 0.69 0.31 2.1\n## 病床利用率    0.58  0.52 -0.09 -0.31  0.04  0.05   0   0   0 0.71 0.29 2.6\n## 病床周转次数  0.90  0.20 -0.01 -0.01  0.01 -0.01   0   0   0 0.85 0.15 1.1\n## 平均住院天数 -0.53  0.43  0.38  0.05  0.02  0.09   0   0   0 0.62 0.38 2.9\n## 治愈好转率   -0.03 -0.07  0.69 -0.18  0.13 -0.02   0   0   0 0.53 0.47 1.2\n## 病死率       -0.33  0.08 -0.46  0.18  0.24  0.09   0   0   0 0.42 0.58 2.9\n## 诊断符合率   -0.31  0.54  0.02  0.02 -0.15  0.07   0   0   0 0.41 0.59 1.8\n## 抢救成功率    0.41 -0.64  0.07 -0.03 -0.12  0.10   0   0   0 0.60 0.40 1.9\n## \n##                        ML1  ML2  ML3  ML4  ML5  ML6  ML7  ML8  ML9\n## SS loadings           2.27 1.75 0.89 0.40 0.12 0.04 0.00 0.00 0.00\n## Proportion Var        0.25 0.19 0.10 0.04 0.01 0.00 0.00 0.00 0.00\n## Cumulative Var        0.25 0.45 0.55 0.59 0.60 0.61 0.61 0.61 0.61\n## Proportion Explained  0.41 0.32 0.16 0.07 0.02 0.01 0.00 0.00 0.00\n## Cumulative Proportion 0.41 0.74 0.90 0.97 0.99 1.00 1.00 1.00 1.00\n## \n## Mean item complexity =  2\n## Test of the hypothesis that 9 factors are sufficient.\n## \n## The degrees of freedom for the null model are  36  and the objective function was  3.82 with Chi Square of  119.03\n## The degrees of freedom for the model are -9  and the objective function was  0.46 \n## \n## The root mean square of the residuals (RMSR) is  0.04 \n## The df corrected root mean square of the residuals is  NA \n## \n## The harmonic number of observations is  36 with the empirical chi square  3.42  with prob &lt;  NA \n## The total number of observations was  36  with Likelihood Chi Square =  11.69  with prob &lt;  NA \n## \n## Tucker Lewis Index of factoring reliability =  2.377\n## Fit based upon off diagonal values = 0.99\n## Measures of factor score adequacy             \n##                                                    ML1  ML2  ML3   ML4   ML5\n## Correlation of (regression) scores with factors   0.94 0.90 0.79  0.70  0.42\n## Multiple R square of scores with factors          0.88 0.80 0.63  0.50  0.18\n## Minimum correlation of possible factor scores     0.77 0.61 0.26 -0.01 -0.65\n##                                                     ML6 ML7 ML8 ML9\n## Correlation of (regression) scores with factors    0.26   0   0   0\n## Multiple R square of scores with factors           0.07   0   0   0\n## Minimum correlation of possible factor scores     -0.86  -1  -1  -1\n\nh2是公因子方差，表示因子对每个变量的解释度，u2=1-h2，表示不能被因子解释的比例。\n看结果中的Cumulative Var，累积方差解释，可以看到在使用3个因子时，累计贡献度是0.55,4个因子是0.59，结合碎石图，我们选择用4个因子。"
  },
  {
    "objectID": "1023-factoranalysis.html#进行因子分析",
    "href": "1023-factoranalysis.html#进行因子分析",
    "title": "23  R语言因子分析",
    "section": "23.3 进行因子分析",
    "text": "23.3 进行因子分析\n选择4个因子\n\n# 选择4个因子，不旋转，最大似然法\nfa.res &lt;- fa(df.use, nfactors = 4, rotate = \"none\", fm=\"ml\")\nfa.res\n## Factor Analysis using method =  ml\n## Call: fa(r = df.use, nfactors = 4, rotate = \"none\", fm = \"ml\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                ML3   ML1   ML2   ML4   h2    u2 com\n## 门诊人次      0.61  0.78  0.11 -0.01 1.00 0.005 1.9\n## 出院人数     -0.40  0.31  0.34 -0.59 0.72 0.276 3.1\n## 病床利用率   -0.30  0.56  0.25  0.49 0.71 0.289 2.9\n## 病床周转次数 -0.55  0.75  0.35  0.01 1.00 0.005 2.3\n## 平均住院天数  0.67 -0.13  0.16  0.26 0.57 0.435 1.5\n## 治愈好转率    0.15 -0.39  0.91  0.00 1.00 0.005 1.4\n## 病死率        0.14 -0.07 -0.47  0.10 0.26 0.743 1.3\n## 诊断符合率    0.45  0.11 -0.10  0.36 0.36 0.642 2.2\n## 抢救成功率   -0.56 -0.12  0.05 -0.46 0.55 0.455 2.1\n## \n##                        ML3  ML1  ML2  ML4\n## SS loadings           1.94 1.79 1.40 1.02\n## Proportion Var        0.22 0.20 0.16 0.11\n## Cumulative Var        0.22 0.41 0.57 0.68\n## Proportion Explained  0.32 0.29 0.23 0.17\n## Cumulative Proportion 0.32 0.61 0.83 1.00\n## \n## Mean item complexity =  2.1\n## Test of the hypothesis that 4 factors are sufficient.\n## \n## The degrees of freedom for the null model are  36  and the objective function was  3.82 with Chi Square of  119.03\n## The degrees of freedom for the model are 6  and the objective function was  0.24 \n## \n## The root mean square of the residuals (RMSR) is  0.04 \n## The df corrected root mean square of the residuals is  0.09 \n## \n## The harmonic number of observations is  36 with the empirical chi square  3.43  with prob &lt;  0.75 \n## The total number of observations was  36  with Likelihood Chi Square =  6.84  with prob &lt;  0.34 \n## \n## Tucker Lewis Index of factoring reliability =  0.931\n## RMSEA index =  0.055  and the 90 % confidence intervals are  0 0.235\n## BIC =  -14.67\n## Fit based upon off diagonal values = 0.99\n## Measures of factor score adequacy             \n##                                                    ML3  ML1  ML2  ML4\n## Correlation of (regression) scores with factors   1.00 1.00 1.00 0.87\n## Multiple R square of scores with factors          0.99 1.00 0.99 0.75\n## Minimum correlation of possible factor scores     0.99 0.99 0.99 0.50\n\n选择4个因子，最终的累积方差解释是0.68，再看因子载荷矩阵，因子1（ML1）在病床周转、门诊人次、病床利用率等具有较高的载荷，因子2在治愈好转率方面具有很大的载荷，因子3在门诊人次，平均住院天数、抢救成功率具有较高的载荷，因子4在出院人数具有较高的载荷。\n从专业角度来看，并没有发现什么规律，好像不能很好的解释专业意义。\n所以我们需要进行因子旋转！"
  },
  {
    "objectID": "1023-factoranalysis.html#因子旋转",
    "href": "1023-factoranalysis.html#因子旋转",
    "title": "23  R语言因子分析",
    "section": "23.4 因子旋转",
    "text": "23.4 因子旋转\n通过因子旋转我们可以更容易找到内在规律，使得结果更加容易结合专业背景进行解释。\n\n# 选择4个因子，最大方差旋转，最大似然法\nfa.res &lt;- fa(df.use, nfactors = 4, rotate = \"varimax\", fm=\"ml\")\nfa.res\n## Factor Analysis using method =  ml\n## Call: fa(r = df.use, nfactors = 4, rotate = \"varimax\", fm = \"ml\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                ML3   ML1   ML2   ML4   h2    u2 com\n## 门诊人次     -0.31  0.23 -0.03  0.92 1.00 0.005 1.4\n## 出院人数      0.75  0.16  0.24  0.27 0.72 0.276 1.6\n## 病床利用率   -0.10  0.83  0.03  0.07 0.71 0.289 1.0\n## 病床周转次数  0.46  0.84  0.09  0.26 1.00 0.005 1.8\n## 平均住院天数 -0.64 -0.23  0.24  0.21 0.57 0.435 1.8\n## 治愈好转率   -0.09 -0.09  0.98 -0.10 1.00 0.005 1.1\n## 病死率       -0.20 -0.18 -0.42 -0.06 0.26 0.743 1.9\n## 诊断符合率   -0.56  0.02 -0.10  0.18 0.36 0.642 1.3\n## 抢救成功率    0.70 -0.04  0.04 -0.21 0.55 0.455 1.2\n## \n##                        ML3  ML1  ML2  ML4\n## SS loadings           2.15 1.58 1.29 1.12\n## Proportion Var        0.24 0.18 0.14 0.12\n## Cumulative Var        0.24 0.41 0.56 0.68\n## Proportion Explained  0.35 0.26 0.21 0.18\n## Cumulative Proportion 0.35 0.61 0.82 1.00\n## \n## Mean item complexity =  1.4\n## Test of the hypothesis that 4 factors are sufficient.\n## \n## The degrees of freedom for the null model are  36  and the objective function was  3.82 with Chi Square of  119.03\n## The degrees of freedom for the model are 6  and the objective function was  0.24 \n## \n## The root mean square of the residuals (RMSR) is  0.04 \n## The df corrected root mean square of the residuals is  0.09 \n## \n## The harmonic number of observations is  36 with the empirical chi square  3.43  with prob &lt;  0.75 \n## The total number of observations was  36  with Likelihood Chi Square =  6.84  with prob &lt;  0.34 \n## \n## Tucker Lewis Index of factoring reliability =  0.931\n## RMSEA index =  0.055  and the 90 % confidence intervals are  0 0.235\n## BIC =  -14.67\n## Fit based upon off diagonal values = 0.99\n## Measures of factor score adequacy             \n##                                                    ML3  ML1  ML2  ML4\n## Correlation of (regression) scores with factors   0.93 0.96 1.00 0.98\n## Multiple R square of scores with factors          0.86 0.92 0.99 0.96\n## Minimum correlation of possible factor scores     0.72 0.85 0.99 0.91\n\n此时我们再看因子载荷阵，因子3在门诊人次、出院人数、病床周转、平均住院天数、诊断符合率、抢救成功率等多个指标上具有较大的载荷，因子2在治愈好转率、病死率上载荷最大，因子1在病床利用率、病床周转率这两个指标上载荷最高，因子4在门诊人次、出院人数这两个指标的载荷最大。\n因此可以认为因子3反映了反映了医疗工作质量各个方面的情况，称为综合因子；因子1反应病床利用情况，可以成为病床利用因子；因子2反映了医疗水平，称为水平因子；因子4反应了就诊患者数量，称为数量因子。\n可以把结果可视化：\n\nfa.diagram(fa.res)\n\n\n\n\n\nfactor.plot(fa.res)\n\n\n\n\n关于因子分析，我并没有找到好用的可视化R包，如果大家知道，欢迎评论区留言。"
  },
  {
    "objectID": "roc-binominal.html#方法1proc",
    "href": "roc-binominal.html#方法1proc",
    "title": "24  二分类资料ROC曲线绘制",
    "section": "24.1 方法1：pROC",
    "text": "24.1 方法1：pROC\n使用pROC包，不过使用这个包需要注意，一定要指定direction，否则可能会得出错误的结果。\n这个R包计算AUC是基于中位数的，哪一组的中位数大就计算哪一组的AUC，在计算时千万要注意！\n关于这个R包的详细使用，请参考文章：用pROC实现ROC曲线分析\n使用pROC包的aSAH数据，其中outcome列是结果变量，1代表Good，2代表Poor。\n\nlibrary(pROC)\n## Type 'citation(\"pROC\")' for a citation.\n## \n## Attaching package: 'pROC'\n## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\n\ndata(aSAH)\ndim(aSAH)\n## [1] 113   7\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n计算AUC及可信区间：\n\nres &lt;- roc(aSAH$outcome,aSAH$s100b,ci=T,auc=T)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nres\n## \n## Call:\n## roc.default(response = aSAH$outcome, predictor = aSAH$s100b,     auc = T, ci = T)\n## \n## Data: aSAH$s100b in 72 controls (aSAH$outcome Good) &lt; 41 cases (aSAH$outcome Poor).\n## Area under the curve: 0.7314\n## 95% CI: 0.6301-0.8326 (DeLong)\n\n\nplot(res,legacy.axes = TRUE)\n\n\n\n\n可以显示最佳截点，比如AUC最大的点：\n\nplot(res,\n     legacy.axes = TRUE,\n     thresholds=\"best\", # AUC最大的点\n     print.thres=\"best\") \n\n\n\n\n可以显示AUC的可信区间：\n\nrocobj &lt;- plot.roc(aSAH$outcome, aSAH$s100b,\n                   main=\"Confidence intervals\", \n                   percent=TRUE,ci=TRUE, \n                   print.auc=TRUE\n                   ) \n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\nciobj &lt;- ci.se(rocobj,\n               specificities=seq(0, 100, 5)\n               )\n\nplot(ciobj, type=\"shape\", col=\"#1c61b6AA\")\nplot(ci(rocobj, of=\"thresholds\", thresholds=\"best\")) \n\n\n\n\n多条ROC曲线画在一起：\n\nrocobj1 &lt;- plot.roc(aSAH$outcome, aSAH$s100,percent=TRUE, col=\"#1c61b6\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nrocobj2 &lt;- lines.roc(aSAH$outcome, aSAH$ndka, percent=TRUE, col=\"#008600\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\nlegend(\"bottomright\", legend=c(\"S100B\", \"NDKA\"), col=c(\"#1c61b6\", \"#008600\"), lwd=2)\n\n\n\n\n两条ROC曲线的比较，可以添加P值：\n\nrocobj1 &lt;- plot.roc(aSAH$outcome, aSAH$s100,percent=TRUE, col=\"#1c61b6\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nrocobj2 &lt;- lines.roc(aSAH$outcome, aSAH$ndka, percent=TRUE, col=\"#008600\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\nlegend(\"bottomright\", legend=c(\"S100B\", \"NDKA\"), col=c(\"#1c61b6\", \"#008600\"), lwd=2)\n\ntestobj &lt;- roc.test(rocobj1, rocobj2)\n\ntext(50, 50, labels=paste(\"p-value =\", format.pval(testobj$p.value)), adj=c(0, .5))"
  },
  {
    "objectID": "roc-binominal.html#方法2rocr",
    "href": "roc-binominal.html#方法2rocr",
    "title": "24  二分类资料ROC曲线绘制",
    "section": "24.2 方法2：ROCR",
    "text": "24.2 方法2：ROCR\n使用ROCR，如果你只是为了画一条ROC曲线，这是我最推荐的方法了，美观又简单！\n\nlibrary(ROCR)\n\n使用非常简单，3句代码，其中第2句是关键，可以更改各种参数，然后就可以画出各种不同的图形：\n\npred &lt;- prediction(aSAH$s100b,aSAH$outcome)\nperf &lt;- performance(pred, \"tpr\",\"fpr\")\nauc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4)\n\nplot(perf,lwd=2,col=\"tomato\")\nabline(0,1,lty=2)\nlegend(\"bottomright\", legend=\"AUC of s100b: 0.7314\", col=\"tomato\", lwd=2,bty = \"n\")\n\n\n\n\n添加箱线图：\n\nperf &lt;- performance(pred, \"tpr\", \"fpr\")\nperf\n## A performance instance\n##   'False positive rate' vs. 'True positive rate' (alpha: 'Cutoff')\n##   with 51 data points\n\nplot(perf,\n     avg=\"threshold\",\n     spread.estimate=\"boxplot\")\n\n\n\n\n还可以绘制PR曲线，召回率recall为横坐标，精确率precision 为纵坐标：\n\nperf &lt;- performance(pred, \"prec\", \"rec\")\nplot(perf,\n     avg= \"threshold\",\n     colorize=TRUE,\n     lwd= 3,\n     main= \"Precision-Recall plot\")\nplot(perf,\n     lty=3,\n     col=\"grey78\",\n     add=TRUE)\n\n\n\n\n还可以把特异度为横坐标，灵敏度为纵坐标：\n\nperf &lt;- performance(pred, \"sens\", \"spec\")\nplot(perf,\n     avg= \"threshold\",\n     colorize=TRUE,\n     lwd= 3,\n     main=\"Sensitivity/Specificity plots\")\nplot(perf,\n     lty=3,\n     col=\"grey78\",\n     add=TRUE)\n\n\n\n\n这个包还可以计算非常多其他的指标，各种图都能画，大家可以自己探索。"
  },
  {
    "objectID": "roc-binominal.html#方法3tidymodels",
    "href": "roc-binominal.html#方法3tidymodels",
    "title": "24  二分类资料ROC曲线绘制",
    "section": "24.3 方法3：tidymodels",
    "text": "24.3 方法3：tidymodels\n使用tidymodels。这个包很有来头，它是R中专门做机器学习的，可以到公众号：医学和生信笔记中查看更多关于它的教程，它也是目前R语言机器学习领域两大当红辣子鸡之一！另一个是mlr3。\n\nsuppressPackageStartupMessages(library(tidymodels))\n\n它很优雅，如果你要计算AUC，那么就是roc_auc()函数：\n\naSAH %&gt;% roc_auc(outcome, s100b,event_level=\"second\")\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc binary         0.731\n\n如果你是要画ROC曲线，那么就是roc_curve()函数：\n\naSAH %&gt;% roc_curve(outcome, s100b,event_level=\"second\") %&gt;% \n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path(size=1.2,color=\"firebrick\") +\n  geom_abline(lty = 3) +\n  coord_equal() +\n  theme_bw()\n\n\n\n\n还有太多方法可以画ROC了，不过pROC和ROCR基本上技能解决99%的问题了。\n最后，给大家看看cran中比较常见的画ROC曲线的包，大家有兴趣可以自己探索：\n\nlibrary(pkgsearch) \n\nrocPkg &lt;-  pkg_search(query=\"ROC\",size=200)\n\nrocPkgShort &lt;- rocPkg %&gt;% \n               filter(maintainer_name != \"ORPHANED\") %&gt;%\n               select(score, package, downloads_last_month) %&gt;%\n               arrange(desc(downloads_last_month))\nhead(rocPkgShort,20)\n## # A data frame: 20 × 3\n##      score package         downloads_last_month\n##  *   &lt;dbl&gt; &lt;chr&gt;                          &lt;int&gt;\n##  1 12277.  pROC                          193252\n##  2  4486.  caTools                        95766\n##  3   974.  ROCR                           48643\n##  4   407.  riskRegression                 12488\n##  5  2630.  PRROC                           9574\n##  6  2291.  cvAUC                           3439\n##  7  1829.  plotROC                         3382\n##  8   345.  mlr3viz                         2944\n##  9  1871.  survivalROC                     2542\n## 10   383.  PresenceAbsence                 2474\n## 11  1800.  precrec                         2316\n## 12  1783.  timeROC                         2287\n## 13   115.  RcmdrPlugin.EZR                 2206\n## 14   178.  WVPlots                         2021\n## 15   466.  ROCit                           1749\n## 16   210.  logcondens                      1637\n## 17   152.  PredictABEL                     1156\n## 18    51.3 wrProteo                         925\n## 19   151.  MLeval                           855\n## 20   165.  cubfits                          622\n\npROC高居榜首，遥遥领先！"
  },
  {
    "objectID": "roc-survive.html#加载r包和数据",
    "href": "roc-survive.html#加载r包和数据",
    "title": "25  生存资料ROC曲线绘制",
    "section": "25.1 加载R包和数据",
    "text": "25.1 加载R包和数据\n\nrm(list = ls())\nlibrary(timeROC)\nlibrary(survival)\n\nload(file = \"./datasets/timeROC.RData\")"
  },
  {
    "objectID": "roc-survive.html#多个时间点roc",
    "href": "roc-survive.html#多个时间点roc",
    "title": "25  生存资料ROC曲线绘制",
    "section": "25.2 多个时间点ROC",
    "text": "25.2 多个时间点ROC\n首先看一下数据结构，对于多个时间点的ROC，需要3列数据：time, event, marker(比如你计算得到的risk score)。\n看一下画图所需的数据长什么样子，event这一列，0代表living，1代表dead，futime这一列单位是年，也可以改成其他的。\n\nstr(df)\n## 'data.frame':    297 obs. of  3 variables:\n##  $ event    : num  0 0 1 0 0 1 0 0 0 0 ...\n##  $ riskScore: num  -0.249 -0.511 -0.211 -0.427 0.279 ...\n##  $ futime   : num  3.03 1.16 1.82 1.52 1.34 ...\n\n\n# 构建timeroc\n\nROC &lt;- timeROC(T=df$futime,   \n               delta=df$event,   \n               marker=df$riskScore,   \n               cause=1,                #阳性结局指标数值\n               weighting=\"marginal\",   #计算方法，默认为marginal\n               times=c(1, 2, 3),       #时间点，选取1年，3年和5年的生存率\n               iid=TRUE)\n\nROC   #查看模型变量信息\n## Time-dependent-Roc curve estimated using IPCW  (n=297, without competing risks). \n##     Cases Survivors Censored AUC (%)   se\n## t=1    57       203       37   71.02 3.68\n## t=2    66       106      125   69.23 3.94\n## t=3    68        74      155   65.53 4.85\n## \n## Method used for estimating IPCW:marginal \n## \n## Total computation time : 0.07  secs.\n\n画图很简单：\n\nplot(ROC, \n     time=1, col=\"red\", lwd=2, title = \"\")   #time是时间点，col是线条颜色\nplot(ROC,\n     time=2, col=\"blue\", add=TRUE, lwd=2)    #add指是否添加在上一张图中\nplot(ROC,\n     time=3, col=\"orange\", add=TRUE, lwd=2)\n\n#添加标签信息\nlegend(\"bottomright\",\n       c(paste0(\"AUC at 1 year: \",round(ROC[[\"AUC\"]][1],2)), \n         paste0(\"AUC at 2 year: \",round(ROC[[\"AUC\"]][2],2)), \n         paste0(\"AUC at 3 year: \",round(ROC[[\"AUC\"]][3],2))),\n       col=c(\"red\", \"blue\", \"orange\"),\n       lty=1, lwd=2,bty = \"n\")"
  },
  {
    "objectID": "roc-survive.html#多指标roc",
    "href": "roc-survive.html#多指标roc",
    "title": "25  生存资料ROC曲线绘制",
    "section": "25.3 多指标ROC",
    "text": "25.3 多指标ROC\n首先也是看一下所需要的数据结构，其中futime和event是必须的，另外的几列是你想要用来画ROC曲线图的指标，可以自己添加，在这里我使用了riskScore, gender, TNM分期。 在gender这一列，1是female，2是male，t,n,m这3列，数字代表不同的分期\n\nstr(df2)\n## 'data.frame':    297 obs. of  8 variables:\n##  $ event    : num  0 0 1 0 0 1 0 0 0 0 ...\n##  $ age      : int  59 63 65 73 59 66 56 42 61 48 ...\n##  $ riskScore: num  -0.249 -0.511 -0.211 -0.427 0.279 ...\n##  $ futime   : num  3.03 1.16 1.82 1.52 1.34 ...\n##  $ gender   : num  2 2 2 1 2 2 1 2 2 2 ...\n##  $ t        : num  4 4 4 3 3 3 5 3 NA 4 ...\n##  $ n        : num  1 5 1 1 1 1 3 1 NA 1 ...\n##  $ m        : num  1 1 1 1 1 3 1 1 3 3 ...\n\n多指标的ROC曲线非常简单，就是构建多个ROC，依次添加即可：\n\n# riskScore的ROC曲线\nROC.risk &lt;- timeROC(T=df2$futime,\n                    delta=df2$event,   \n                    marker=df2$riskScore,   \n                    cause=1,                \n                    weighting=\"marginal\",   \n                    times=3,   \n                    iid=TRUE)\n\n\n# gender的ROC曲线\nROC.gender &lt;- timeROC(T=df2$futime,   \n                      delta=df2$event,   \n                      marker=df2$gender,   \n                      cause=1,   \n                      weighting=\"marginal\",   \n                      times=3,   \n                      iid=TRUE)\n\n\n# age的ROC曲线\nROC.age &lt;- timeROC(T=df2$futime,   \n                   delta=df2$event,   \n                   marker=df2$age,   \n                   cause=1,   \n                   weighting=\"marginal\",   \n                   times=3,   \n                   iid=TRUE)\n\n\n# T分期的ROC曲线\nROC.T &lt;- timeROC(T=df2$futime,\n                 delta=df2$event,  \n                 marker=df2$t,   \n                 cause=1, \n                 weighting=\"marginal\", \n                 times=3, \n                 iid=TRUE)\n\n\n# N分期的ROC曲线\nROC.N &lt;- timeROC(T=df2$futime,   \n                 delta=df2$event,   \n                 marker=df2$n,   \n                 cause=1,   \n                 weighting=\"marginal\",   \n                 times=3,   \n                 iid=TRUE)\n\n\n# M分期的ROC曲线\nROC.M &lt;- timeROC(T=df2$futime,   \n                 delta=df2$event,   \n                 marker=df2$m,   \n                 cause=1,   \n                 weighting=\"marginal\",   \n                 times=3,   \n                 iid=TRUE)\n\n把每个曲线拼在一起即可，添加一个图例：\n\nplot(ROC.risk, time = 3, col=\"#E41A1C\", lwd=2, title = \"\")\nplot(ROC.gender, time = 3, col=\"#A65628\", lwd=2, add = T)\nplot(ROC.age, time = 3, col=\"#4DAF4A\", lwd=2, add = T)\nplot(ROC.T, time = 3, col=\"#377EB8\", lwd=2, add = T)\nplot(ROC.N, time = 3, col=\"#984EA3\", lwd=2, add = T)\nplot(ROC.M, time = 3, col=\"#FFFF33\", lwd=2, add = T)\nlegend(\"bottomright\",\n       c(paste0(\"Risk score: \",round(ROC.risk[[\"AUC\"]][2],2)), \n         paste0(\"gender: \",round(ROC.gender[[\"AUC\"]][2],2)), \n         paste0(\"age: \",round(ROC.age[[\"AUC\"]][2],2)),\n         paste0(\"T: \",round(ROC.T[[\"AUC\"]][2],2)),\n         paste0(\"N: \",round(ROC.N[[\"AUC\"]][2],2)),\n         paste0(\"M: \",round(ROC.M[[\"AUC\"]][2],2))\n         ),\n       col=c(\"#E41A1C\", \"#A65628\", \"#4DAF4A\",\"#377EB8\",\"#984EA3\",\"#FFFF33\"),\n       lty=1, lwd=2,bty = \"n\")"
  },
  {
    "objectID": "roc-bestcut.html#proc",
    "href": "roc-bestcut.html#proc",
    "title": "26  ROC曲线的最佳截点",
    "section": "26.1 pROC",
    "text": "26.1 pROC\n只能用于二分类数据，不能用于生存数据。\n使用pROC包需要注意，一定要指定direction，否则可能会得出错误的结果。\n这个R包计算AUC是基于中位数的，哪一组的中位数大就计算哪一组的AUC，在计算时千万要注意！\n关于这个包的详细介绍，请参考文章：用pROC实现ROC曲线分析\n使用pROC包的aSAH数据，其中outcome列是结果变量，1代表Good，2代表Poor。\n\nlibrary(pROC)\n## Type 'citation(\"pROC\")' for a citation.\n## \n## Attaching package: 'pROC'\n## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\n\ndata(aSAH)\ndim(aSAH)\n## [1] 113   7\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n计算AUC及可信区间：\n\nres &lt;- pROC::roc(aSAH$outcome,aSAH$s100b,ci=T,auc=T)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nres\n## \n## Call:\n## roc.default(response = aSAH$outcome, predictor = aSAH$s100b,     auc = T, ci = T)\n## \n## Data: aSAH$s100b in 72 controls (aSAH$outcome Good) &lt; 41 cases (aSAH$outcome Poor).\n## Area under the curve: 0.7314\n## 95% CI: 0.6301-0.8326 (DeLong)\n\n显示最佳截点，比如AUC最大的点：\n\nplot(res,\n     legacy.axes = TRUE,\n     thresholds=\"best\", # AUC最大的点\n     print.thres=\"best\") \n\n\n\n\n最佳截点是0.205，特异度是0.806，敏感度是0.634。"
  },
  {
    "objectID": "roc-bestcut.html#cutoff",
    "href": "roc-bestcut.html#cutoff",
    "title": "26  ROC曲线的最佳截点",
    "section": "26.2 cutoff",
    "text": "26.2 cutoff\ncutoff包中的roc函数也可以用于确定二分类数据ROC曲线的最佳截点，这个R包还可以用于连续性变量最佳截点的计算，之前专门介绍过：最佳截断值确定之cutoff\n\nlibrary(cutoff)\n## \n## Attaching package: 'cutoff'\n## The following object is masked from 'package:pROC':\n## \n##     roc\n\ncutoff::roc(aSAH$s100b, aSAH$outcome)\n##                      type  auc cutoff sensitivity specificity\n## 1 positive classification 0.73   0.22   0.6341463   0.8055556\n\n该包给出的最佳截点是0.22，敏感度是0.6341463，特异度是0.8055556。和pROC的结果有一点点差别，问题不大。"
  },
  {
    "objectID": "roc-bestcut.html#optimalcutpoints",
    "href": "roc-bestcut.html#optimalcutpoints",
    "title": "26  ROC曲线的最佳截点",
    "section": "26.3 optimalcutpoints",
    "text": "26.3 optimalcutpoints\nOptimalCutpoints包也是用于二分类数据ROC曲线的最佳截点，不能用于生存数据。\n该包中最重要的函数是optimal.cutpoints()、control.cutpoints()、summary.optimal.cutpoints()和plot.optimal.cutpoints()函数。optimal.cutpoints()函数根据所选的准则计算最佳切点以及其准确性度量。可以选择多个准则来选择最佳切点。control.cutpoints()函数用于设置每种方法特定的几个参数，例如成本值或诊断准确性度量的最小值。summary.optimal.cutpoints()和plot.optimal.cutpoints()函数分别生成数值和图形输出。数值输出包括有关最佳切点的信息，包括选择最佳值的方法，以及最佳切点的数量（在某些情况下可能有多个值）和最佳切点及其准确性度量的估计值。\n使用起来也是非常简单，没有难度：\n\nlibrary(OptimalCutpoints)\n\noc_youden &lt;- optimal.cutpoints(X=\"s100b\",\n                               status = \"outcome\",\n                               tag.healthy=\"Good\",\n                               method=\"Youden\",#支持的标准超多\n                               data=aSAH\n                               )\nsummary(oc_youden)\n## \n## Call:\n## optimal.cutpoints.default(X = \"s100b\", status = \"outcome\", tag.healthy = \"Good\", \n##     methods = \"Youden\", data = aSAH)\n## \n## Area under the ROC curve (AUC):  0.731 (0.63, 0.833) \n## \n## CRITERION: Youden\n## Number of optimal cutoffs: 1\n## \n##                     Estimate\n## cutoff             0.2200000\n## Se                 0.6341463\n## Sp                 0.8055556\n## PPV                0.6500000\n## NPV                0.7945205\n## DLR.Positive       3.2613240\n## DLR.Negative       0.4541632\n## FP                14.0000000\n## FN                15.0000000\n## Optimal criterion  0.4397019\n\n给出的结果也是和pROC以及cutoff包是一致的。\n画出来的图也是和pROC一样的：\n\nplot(oc_youden)\n\n\n\n## Press return for next page....\n\n\n\n\n除此之外，这个包还可以指定协变量，还可以使用最大化敏感度/特异度等指标确定最佳截点，大家可以去查看帮助文档。"
  },
  {
    "objectID": "roc-bestcut.html#cutpointr",
    "href": "roc-bestcut.html#cutpointr",
    "title": "26  ROC曲线的最佳截点",
    "section": "26.4 cutpointr",
    "text": "26.4 cutpointr\ncutpointr是一个用于整洁计算“最佳”切点的R包。它支持多种计算切点的方法，并包括几个可以通过选择切点来最大化或最小化的度量标准。cutpointr可以自动通过自助法计算最佳切点的变异性，并返回各种性能指标的袋外估计值。\n支持支二分类数据不支持生存数据。\n\ninstall.packages(\"cutpointr\")\n\n这个包也支持多种标准，使用时也要注意方向。\n\nlibrary(cutpointr)\n## \n## Attaching package: 'cutpointr'\n## The following object is masked from 'package:cutoff':\n## \n##     roc\n## The following objects are masked from 'package:pROC':\n## \n##     auc, roc\n\ncp &lt;- cutpointr(data=aSAH, \n                x=s100b, \n                class=outcome, \n                method = maximize_metric, # 最大化指标\n                metric = youden) # 选择指标\n## Assuming the positive class is Poor\n## Assuming the positive class has higher x values\nsummary(cp)\n## Method: maximize_metric \n## Predictor: s100b \n## Outcome: outcome \n## Direction: &gt;= \n## \n##     AUC   n n_pos n_neg\n##  0.7314 113    41    72\n## \n##  optimal_cutpoint youden    acc sensitivity specificity tp fn fp tn\n##              0.22 0.4397 0.7434      0.6341      0.8056 26 15 14 58\n## \n## Predictor summary: \n##     Data Min.    5% 1st Qu. Median      Mean 3rd Qu.   95% Max.        SD NAs\n##  Overall 0.03 0.046    0.09   0.14 0.2469912    0.33 0.722 2.07 0.2721603   0\n##     Good 0.04 0.040    0.08   0.11 0.1615278    0.17 0.470 0.50 0.1308548   0\n##     Poor 0.03 0.070    0.12   0.30 0.3970732    0.56 0.860 2.07 0.3751949   0\n\n画图：\n\nplot(cp)\n\n\n\n\n除此之外，还有很多其他函数没介绍，大家可以参考github：https://github.com/Thie1e/cutpointr"
  },
  {
    "objectID": "roc-bestcut.html#survavalroc",
    "href": "roc-bestcut.html#survavalroc",
    "title": "26  ROC曲线的最佳截点",
    "section": "26.5 survavalROC",
    "text": "26.5 survavalROC\n能做time-dependent-ROC分析的R包不多，找了好久也没发现一个R包可以完成time-dependent-ROC的所有分析，timeROC是比较全能的了，但是不能计算最佳截点，survavalROC可以计算最佳截点，但是又不能同时计算多个时间点的ROC曲线。\n所以最佳截点我们可以通过survivalROC包实现。\n\nload(file = \"./datasets/timeROC.RData\")\nlibrary(survivalROC)\n\n# 1年的最佳截点\nroc1 &lt;- survivalROC(Stime = df$futime,\n                   status = df$event,\n                   marker = df$riskScore,\n                   method = \"KM\",\n                   predict.time = 1 # 时间选1年\n                   )\n\nroc1$cut.values[which.max(roc1$TP - roc1$FP)] # 最佳截点的值，基于约登指数计算出来\n## [1] -0.07986499\n\n最佳截点是-0.07986499，就是这么简单，下面就是画图：\n\nplot(roc1$FP, roc1$TP, type=\"l\", xlim=c(0,1), ylim=c(0,1),   \n  xlab=paste( \"FP\", \"\\n\", \"AUC = \",round(roc1$AUC,3)), \n  ylab=\"TP\")\n  abline(0,1)\n\n\n\n\n除了以上介绍的R包，还有ThresholdROC也是同样的用法，这里就不介绍了，大家感兴趣的自己学习一下。\n公众号后台回复ROC即可获取ROC曲线合集，回复最佳截点即可获取最佳截断值相关推文合集。"
  },
  {
    "objectID": "roc-smooth.html#二分类资料的平滑roc曲线",
    "href": "roc-smooth.html#二分类资料的平滑roc曲线",
    "title": "27  平滑ROC曲线",
    "section": "27.1 二分类资料的平滑ROC曲线",
    "text": "27.1 二分类资料的平滑ROC曲线\n直接通过pROC即可实现，这个包详细使用请参考：用pROC实现ROC曲线分析\npROC包中提供了一个aSAH数据集，这是一个动脉瘤性蛛网膜下腔出血的数据集，一共113行，7列。其中：\n\ngos6：格拉斯哥量表评分\noutcome：结果变量\ngender：性别\nage：年龄\nwfns：世界神经外科医师联合会公认的神经学量表评分\ns100b：生物标志物\nndka：生物标志物\n\n\nlibrary(pROC)\ndata(\"aSAH\")\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n可以直接在roc函数中指定，也可以使用smooth函数：\n\nroc(aSAH$outcome, aSAH$wfns, smooth = T)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n## \n## Call:\n## roc.default(response = aSAH$outcome, predictor = aSAH$wfns, smooth = T)\n## \n## Data: aSAH$wfns in 72 controls (aSAH$outcome Good) &lt; 41 cases (aSAH$outcome Poor).\n## Smoothing: binormal \n## Area under the curve: 0.8454\n# 或者\nrr &lt;- roc(aSAH$outcome, aSAH$wfns)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nrrs &lt;- smooth(rr)\n\n画图：\n\nplot(rrs,auc.polygon=T,auc.polygon.col=\"steelblue\",\n     print.auc=T,\n     print.auc.x=1,print.auc.y=0.9,print.auc.col=\"firebrick\",\n     print.auc.cex=2)"
  },
  {
    "objectID": "roc-smooth.html#生存资料的平滑roc曲线",
    "href": "roc-smooth.html#生存资料的平滑roc曲线",
    "title": "27  平滑ROC曲线",
    "section": "27.2 生存资料的平滑ROC曲线",
    "text": "27.2 生存资料的平滑ROC曲线\n不考虑时间因素的ROC曲线可以使用pROC包中的smooth参数实现平滑版的曲线。time-dependent ROC目前还没发现比较好的方法可以直接实现，只能使用ggplot2曲线救国了。\n\nrm(list = ls())\nlibrary(timeROC)\nlibrary(survival)\n\nload(file = \"./datasets/timeROC.RData\")\n\n\n27.2.1 方法1：ggplot2\n首先看一下数据结构，对于多个时间点的ROC，需要3列数据：time, event, marker(比如你计算得到的risk score)\n首先还是借助timeROC包构建多个时间点的ROC对象：\n\nROC &lt;- timeROC(T = df$futime,   \n               delta = df$event,   \n               marker = df$riskScore,   \n               cause = 1,                \n               weighting = \"marginal\",   \n               times = c(1, 2, 3),       \n               iid = TRUE)\n\nROC   #查看模型变量信息\n## Time-dependent-Roc curve estimated using IPCW  (n=297, without competing risks). \n##     Cases Survivors Censored AUC (%)   se\n## t=1    57       203       37   71.02 3.68\n## t=2    66       106      125   69.23 3.94\n## t=3    68        74      155   65.53 4.85\n## \n## Method used for estimating IPCW:marginal \n## \n## Total computation time : 0.08  secs.\n\n默认的画出来也还可以，但是就是觉得可以更好看一点。\n\nplot(ROC, \n     time=1, col=\"red\", lwd=2, title = \"\")   #time是时间点，col是线条颜色\nplot(ROC,\n     time=2, col=\"blue\", add=TRUE, lwd=2)    #add指是否添加在上一张图中\nplot(ROC,\n     time=3, col=\"orange\", add=TRUE, lwd=2)\n\n#添加标签信息\nlegend(\"bottomright\",\n       c(paste0(\"AUC at 1 year: \",round(ROC[[\"AUC\"]][1],2)), \n         paste0(\"AUC at 2 year: \",round(ROC[[\"AUC\"]][2],2)), \n         paste0(\"AUC at 3 year: \",round(ROC[[\"AUC\"]][3],2))),\n       col=c(\"red\", \"blue\", \"orange\"),\n       lty=1, lwd=2,bty = \"n\")   \n\n\n\n\n下面使用ggplot2画图。\n首先是提取数据，这个提取数据比起lasso的真的是很简单了，不过现在提取lasso的数据也很简单。\n\ndf_plot &lt;- data.frame(tpr = as.numeric(ROC$TP),\n                 fpr = as.numeric(ROC$FP),\n                 year = rep(c(\"1-year\",\"2-year\",\"3-year\"),each = nrow(ROC$TP)))\n\nhead(df_plot)\n##          tpr         fpr   year\n## 1 0.00000000 0.000000000 1-year\n## 2 0.00000000 0.004926108 1-year\n## 3 0.01809868 0.004926108 1-year\n## 4 0.03681243 0.004926108 1-year\n## 5 0.03681243 0.009852217 1-year\n## 6 0.05425138 0.009852217 1-year\n\n下面是画图代码，平滑曲线，说简单确实简单，如果对ggplot2不熟悉，确实也很难想到：\n\nlibrary(ggplot2)\n\np &lt;- ggplot(df_plot, aes(fpr, tpr, color = year)) +\n  geom_smooth(se=FALSE, linewidth=1.2)+ # 这就是平滑曲线的关键\n  geom_abline(slope = 1, intercept = 0, color = \"grey10\",linetype = 2) +\n  scale_color_manual(values = c(\"#E41A1C\",\"#377EB8\",\"#4DAF4A\"),\n                     name = NULL, \n                     labels = c(paste0(\"AUC at 1 year: \",round(ROC[[\"AUC\"]][1],2)), \n                                paste0(\"AUC at 2 year: \",round(ROC[[\"AUC\"]][2],2)), \n                                paste0(\"AUC at 3 year: \",round(ROC[[\"AUC\"]][3],2)))\n                     ) + \n  coord_fixed(ratio = 1) +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\") +\n  theme_minimal(base_size = 14, base_family = \"sans\") +\n  theme(legend.position = c(0.7,0.15), \n        panel.border = element_rect(fill = NA),\n        axis.text = element_text(color = \"black\"))\n\np\n## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n27.2.2 方法2：risksetROC\n使用risksetROC包实现。\n使用survival包中的pbc数据集。\n\nlibrary(risksetROC)\n## Loading required package: MASS\nlibrary(survival)\ndata(pbc)\nstr(pbc)\n## 'data.frame':    418 obs. of  20 variables:\n##  $ id      : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ fudays  : int  400 4500 1012 1925 1504 2503 1832 2466 2400 51 ...\n##  $ status  : int  2 0 2 2 1 2 0 2 2 2 ...\n##  $ drug    : int  1 1 1 1 2 2 2 2 1 2 ...\n##  $ age     : int  21464 20617 25594 19994 13918 24201 20284 19379 15526 25772 ...\n##  $ sex     : int  1 1 0 1 1 1 1 1 1 1 ...\n##  $ ascites : int  1 0 0 0 0 0 0 0 0 1 ...\n##  $ hepatom : int  1 1 0 1 1 1 1 0 0 0 ...\n##  $ spiders : int  1 1 0 1 1 0 0 0 1 1 ...\n##  $ edema   : num  1 0 0.5 0.5 0 0 0 0 0 1 ...\n##  $ bili    : num  14.5 1.1 1.4 1.8 3.4 0.8 1 0.3 3.2 12.6 ...\n##  $ chol    : int  261 302 176 244 279 248 322 280 562 200 ...\n##  $ albumin : num  2.6 4.14 3.48 2.54 3.53 3.98 4.09 4 3.08 2.74 ...\n##  $ copper  : int  156 54 210 64 143 50 52 52 79 140 ...\n##  $ alkphos : num  1718 7395 516 6122 671 ...\n##  $ sgot    : num  137.9 113.5 96.1 60.6 113.2 ...\n##  $ trig    : int  172 88 55 92 72 63 213 189 88 143 ...\n##  $ platelet: int  190 221 151 183 136 NA 204 373 251 302 ...\n##  $ protime : num  12.2 10.6 12 10.3 10.9 11 9.7 11 11 11.5 ...\n##  $ stage   : int  4 3 4 4 3 3 3 3 2 4 ...\n\n做一些准备工作。只使用前312行数据，生存状态用1表示终点事件，0表示删失，然后建立cox模型，计算出线性预测值作为marker：\n\npbc1 &lt;- pbc[1:312,]\nsurvival.status &lt;- ifelse(pbc1$status==2,1,0)\nsurvival.time &lt;- pbc1$fudays\n\npbc1$status1 &lt;- survival.status\n\nfit &lt;- coxph(Surv(survival.time, status1) ~ log(bili)+\n               log(protime)+edema+albumin+age,\n             data = pbc1\n             )\neta &lt;- fit$linear.predictors\n\n使用方法和timeROC以及survivalROC基本上是一样的：\n\nnobs &lt;- length(survival.time[survival.status==1])\nspan &lt;- 1.0*(nobs^(-0.2))\n\n# 3种方法都试一下，然后画在一起\nROC.CC90 &lt;- risksetROC(Stime = survival.time, status = survival.status,\n                       marker = eta, predict.time = 90, method = \"Cox\",\n                       main=\"time-denpendent ROC with riksetROC\",\n                       lty=2, lwd=2,col=\"red\"\n                       )\nROC.SS90 &lt;- risksetROC(Stime = survival.time, status = survival.status,\n                       marker = eta, predict.time = 90, method = \"Schoenfeld\",\n                       plot = F, span = span\n                       )\nROC.LL90 &lt;- risksetROC(Stime = survival.time, status = survival.status,\n                       marker = eta, predict.time = 90, method = \"LocalCox\",\n                       plot = F, span = span\n                       )\n\nlines(ROC.SS90$FP, ROC.SS90$TP, lty=3, lwd=2, col=\"darkblue\")\nlines(ROC.LL90$FP, ROC.LL90$TP, lty=4, lwd=2, col=\"green\")\nlegend(0.6,0.25, lty = c(2,3,4),col = c(\"red\",\"darkblue\",\"green\"),\n       legend = c(\"Cox\",\"Schoenfeld\",\"LocalCox\"), bty = \"n\")\n\n\n\n\n完美的平滑曲线，不用自己实现。"
  },
  {
    "objectID": "roc-compare.html#二分类资料的roc比较",
    "href": "roc-compare.html#二分类资料的roc比较",
    "title": "28  ROC曲线的显著性检验",
    "section": "28.1 二分类资料的ROC比较",
    "text": "28.1 二分类资料的ROC比较\n可以通过pROC包实现的，使用其中roc.test()函数可实现两个ROC的Delong检验。\n使用pROC包的aSAH数据，其中outcome列是结果变量，1代表Good，2代表Poor。\n\nlibrary(pROC)\n## Type 'citation(\"pROC\")' for a citation.\n## \n## Attaching package: 'pROC'\n## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\n\ndata(aSAH)\ndim(aSAH)\n## [1] 113   7\n\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n构建两个ROC对象，然后直接比较即可：\n\nroc1 &lt;- roc(aSAH$outcome,aSAH$s100b)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nroc2 &lt;- roc(aSAH$outcome,aSAH$ndka)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\nres &lt;- roc.test(roc1,roc2)\nres\n## \n##  DeLong's test for two correlated ROC curves\n## \n## data:  roc1 and roc2\n## Z = 1.3908, p-value = 0.1643\n## alternative hypothesis: true difference in AUC is not equal to 0\n## 95 percent confidence interval:\n##  -0.04887061  0.28769174\n## sample estimates:\n## AUC of roc1 AUC of roc2 \n##   0.7313686   0.6119580\n\n这个函数里面有个method参数：delong/bootstrap/venkatraman，默认是delong，delong和bootstrap用于比较AUC，如果只是ROC曲线的比较，需要用venkatraman。关于这几种方法的具体原理，大家可以去翻相关的论文~\nroc.test只能用于两个ROC的比较，如果是多个比较，可以使用MedCalc软件，这个是和SPSS类似的软件，只要点点点即可。\n当然也是可以直接画在图里的：\n\nrocobj1 &lt;- plot.roc(aSAH$outcome, aSAH$s100,percent=TRUE, col=\"#1c61b6\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nrocobj2 &lt;- lines.roc(aSAH$outcome, aSAH$ndka, percent=TRUE, col=\"#008600\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\nlegend(\"bottomright\", legend=c(\"S100B\", \"NDKA\"), col=c(\"#1c61b6\", \"#008600\"), lwd=2)\n\ntestobj &lt;- roc.test(rocobj1, rocobj2)\n\ntext(50, 50, labels=paste(\"p-value =\", format.pval(testobj$p.value)), adj=c(0, .5))\n\n\n\n\n当然你也可以用其他非参数检验的方法进行比较，比如mann whitney u检验。"
  },
  {
    "objectID": "roc-compare.html#生存资料roc的比较",
    "href": "roc-compare.html#生存资料roc的比较",
    "title": "28  ROC曲线的显著性检验",
    "section": "28.2 生存资料ROC的比较",
    "text": "28.2 生存资料ROC的比较\n使用timeROC包实现。\n还是用之前推文中用过的例子，获取数据请翻看之前的推文~\n\nrm(list = ls())\nlibrary(timeROC)\nlibrary(survival)\n\nload(file = \"./datasets/timeROC.RData\")\n\n使用其中的df2这个数据：\n\nstr(df2)\n## 'data.frame':    297 obs. of  8 variables:\n##  $ event    : num  0 0 1 0 0 1 0 0 0 0 ...\n##  $ age      : int  59 63 65 73 59 66 56 42 61 48 ...\n##  $ riskScore: num  -0.249 -0.511 -0.211 -0.427 0.279 ...\n##  $ futime   : num  3.03 1.16 1.82 1.52 1.34 ...\n##  $ gender   : num  2 2 2 1 2 2 1 2 2 2 ...\n##  $ t        : num  4 4 4 3 3 3 5 3 NA 4 ...\n##  $ n        : num  1 5 1 1 1 1 3 1 NA 1 ...\n##  $ m        : num  1 1 1 1 1 3 1 1 3 3 ...\n\n构建几个timeROC:\n\n# riskScore的ROC曲线\nROC.risk &lt;- timeROC(T=df2$futime,\n                    delta=df2$event,   \n                    marker=df2$riskScore,   \n                    cause=1,                \n                    weighting=\"marginal\",   \n                    times=3,  # c(1,2) \n                    iid=TRUE)\n\n\n# age的ROC曲线\nROC.age &lt;- timeROC(T=df2$futime,   \n                   delta=df2$event,   \n                   marker=df2$age,   \n                   cause=1,   \n                   weighting=\"marginal\",   \n                   times=3,   # c(1,2)\n                   iid=TRUE)\n\n比较就用compare()函数即可：\n\ncompare(ROC.risk, ROC.age)\n## $p_values_AUC\n##       t=0       t=3 \n##        NA 0.4544231\n\n同时使用多个时间点也是可以的：\n\n# riskScore的ROC曲线\nROC.risk &lt;- timeROC(T=df2$futime,\n                    delta=df2$event,   \n                    marker=df2$riskScore,   \n                    cause=1,                \n                    weighting=\"marginal\",   \n                    times=c(1,2),\n                    iid=TRUE)\n\n\n# age的ROC曲线\nROC.age &lt;- timeROC(T=df2$futime,   \n                   delta=df2$event,   \n                   marker=df2$age,   \n                   cause=1,   \n                   weighting=\"marginal\",   \n                   times=c(1,2),\n                   iid=TRUE)\n\ncompare(ROC.risk, ROC.age)\n## $p_values_AUC\n##        t=1        t=2 \n## 0.09758546 0.27995259\n\ncompare(ROC.risk, ROC.age, adjusted = T) # 计算调整p值\n## $p_values_AUC\n##                     t=1       t=2\n## Non-adjusted 0.09758546 0.2799526\n## Adjusted     0.14983636 0.3984702\n## \n## $Cor\n##           [,1]      [,2]\n## [1,] 1.0000000 0.7750774\n## [2,] 0.7750774 1.0000000\n\n画图就不演示了，可以参考前面的内容。"
  },
  {
    "objectID": "roc-attention.html#准备数据",
    "href": "roc-attention.html#准备数据",
    "title": "29  R语言计算AUC(ROC)注意事项",
    "section": "29.1 准备数据",
    "text": "29.1 准备数据\n假如我们要用ca125的值预测患者是不是癌症，虚构一个数据：\n\nset.seed(20220840)\nca125_1 &lt;- c(rnorm(10,80,20),rnorm(20,50,10))\nca125_2 &lt;- c(rnorm(10,20,20),rnorm(20,70,10))\nclass=c(rep(1:0,c(10,20)))\n\ntumor &lt;- c(rep(c(\"癌症\",\"非癌症\"),c(10,20)))\n\ndf &lt;- data.frame(`class`=class,`ca125_1`=ca125_1,`ca125_2`=ca125_2,\n                 `tumor`=tumor\n                 )\npsych::headTail(df)\n##     class ca125_1 ca125_2  tumor\n## 1       1   51.88    5.96   癌症\n## 2       1   82.46   10.59   癌症\n## 3       1  113.67  -19.22   癌症\n## 4       1   63.49    6.08   癌症\n## ...   ...     ...     ...   &lt;NA&gt;\n## 27      0   53.33   74.34 非癌症\n## 28      0   59.92   51.11 非癌症\n## 29      0   46.46   64.21 非癌症\n## 30      0   30.03    66.5 非癌症"
  },
  {
    "objectID": "roc-attention.html#rocr",
    "href": "roc-attention.html#rocr",
    "title": "29  R语言计算AUC(ROC)注意事项",
    "section": "29.2 ROCR",
    "text": "29.2 ROCR\nROCR的使用非常简单，两步完成，需要提供用来预测结果的指标以及真实结果。\n关于这个包计算的阳性结果的AUC还是阴性结果的AUC，它的规则是这样的：\nROCR默认计算顺序靠后的类别的AUC。如果提供给labels的值是有序因子型变量，排在前面的默认是阴性结果（negtive），排在后面的默认是阳性结果（positive），默认计算阳性结果（排序靠后）的AUC。如果是无序因子、数值、字符、逻辑型变量，会按照R语言的默认排序，比如按照数字大小、首字母顺序等，也是计算排序靠后的类别的AUC。\n比如我们这个df数据集，其中的tumor这一列是字符型，默认的顺序是：癌症，非癌症，是按照首字母顺序排列的，因此在计算AUC时，默认是计算的非癌症的AUC。\n\nlibrary(ROCR)\n\npred &lt;- prediction(predictions = ca125_1, # 预测指标\n                   labels = tumor # 真实结果\n                   )\n\nperformance(pred, \"auc\")@y.values[[1]]\n## [1] 0.075\n\n但是大多数时候我们需要的都是阳性结果（比如这里我们想计算癌症的AUC，而不是非癌症）的AUC，所以我建议大家在使用R包计算AUC或者画ROC曲线时，手动指定顺序！\n\npred &lt;- prediction(predictions = ca125_1, # 预测指标\n                   labels = tumor # 真实结果\n                   ,label.ordering = c(\"非癌症\",\"癌症\") # 此时就是计算癌症的AUC\n                   )\n\nperformance(pred, \"auc\")@y.values[[1]]\n## [1] 0.925"
  },
  {
    "objectID": "roc-attention.html#proc",
    "href": "roc-attention.html#proc",
    "title": "29  R语言计算AUC(ROC)注意事项",
    "section": "29.3 pROC",
    "text": "29.3 pROC\n\nlibrary(pROC)\n\npROC包计算AUC也需要：用来预测结果的指标以及真实结果。\n这个包计算pROC略有不同，它是根据中位数来的，谁的中位数大，就计算谁的AUC，比如我们的这个例子，计算下中位数看看：\n\n# 把ca125_1按照tumor的两个类别进行分组，然后分别计算中位数\ntapply(ca125_1, tumor, median)\n##     癌症   非癌症 \n## 81.34426 49.99926\n\n结果是癌症组的中位数＞非癌症组的中位数，所以是计算癌症的AUC。\n计算AUC：\n\nroc(response=tumor, predictor=ca125_1)\n## Setting levels: control = 癌症, case = 非癌症\n## Setting direction: controls &gt; cases\n## \n## Call:\n## roc.default(response = tumor, predictor = ca125_1)\n## \n## Data: ca125_1 in 10 controls (tumor 癌症) &gt; 20 cases (tumor 非癌症).\n## Area under the curve: 0.925\n\n再来看看ca125_2这一列指标：\n\n# 把ca125_2按照tumor的两个类别进行分组，然后分别计算中位数\ntapply(ca125_2, tumor, median)\n##     癌症   非癌症 \n## 13.52771 69.69272\n\n结果是癌症组的中位数＜非癌症组，所以是计算非癌症的AUC。\n\nroc(response=tumor, predictor=ca125_2)\n## Setting levels: control = 癌症, case = 非癌症\n## Setting direction: controls &lt; cases\n## \n## Call:\n## roc.default(response = tumor, predictor = ca125_2)\n## \n## Data: ca125_2 in 10 controls (tumor 癌症) &lt; 20 cases (tumor 非癌症).\n## Area under the curve: 0.9\n\n如果想要手动指定，需要设置levels和direction：\n\n# 此时计算的就是癌症的AUC\nroc(response=tumor, predictor=ca125_2,\n    levels=c(\"非癌症\", \"癌症\"), # 这个顺序随便设定，重要的是direction\n    direction = \"&lt;\" # 手动设定非癌症 &lt; 癌症\n    )\n## \n## Call:\n## roc.default(response = tumor, predictor = ca125_2, levels = c(\"非癌症\",     \"癌症\"), direction = \"&lt;\")\n## \n## Data: ca125_2 in 20 controls (tumor 非癌症) &lt; 10 cases (tumor 癌症).\n## Area under the curve: 0.1"
  },
  {
    "objectID": "roc-attention.html#yardstick",
    "href": "roc-attention.html#yardstick",
    "title": "29  R语言计算AUC(ROC)注意事项",
    "section": "29.4 yardstick",
    "text": "29.4 yardstick\n最后给大家演示下yardstick包的做法，其中truth必须提供因子型，使用event_level指定到底是计算谁的AUC：\n\ndf$tumor &lt;- factor(df$tumor,levels = c(\"癌症\",\"非癌症\"))\n\nlibrary(yardstick)\nroc_auc(data = df, \n        tumor,\n        ca125_1,\n        event_level=\"first\")\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc binary         0.925\n\n画ROC曲线：\n\nyardstick::roc_curve(df, truth=tumor,\n                   ca125_1,\n                   event_level=\"first\") |&gt; \n  ggplot2::autoplot()\n\n\n\n\nR语言中的ROC曲线R包都有这样的潜规则，大家在使用的时候一定要注意哦~\n示例数据还提供了用数值表示的结果变量class，感兴趣的可以试试看，是不是和我说的一样！"
  },
  {
    "objectID": "roc-many.html#准备数据",
    "href": "roc-many.html#准备数据",
    "title": "30  多指标联合诊断的ROC曲线",
    "section": "30.1 准备数据",
    "text": "30.1 准备数据\n\nlibrary(pROC)\n## Type 'citation(\"pROC\")' for a citation.\n## \n## Attaching package: 'pROC'\n## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\n\ndata(aSAH)\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n其中outcome是结果变量，是二分类的，其余列是预测变量。"
  },
  {
    "objectID": "roc-many.html#多指标联合诊断的roc",
    "href": "roc-many.html#多指标联合诊断的roc",
    "title": "30  多指标联合诊断的ROC曲线",
    "section": "30.2 多指标联合诊断的ROC",
    "text": "30.2 多指标联合诊断的ROC\n假如现在我想使用s100b/ndka/age这3个变量来预测结果，该如何画出这3个变量联合诊断的ROC曲线呢？\n首先，使用这3个变量建立逻辑回归：\n\nf &lt;- glm(outcome ~ s100b + ndka + age, data = aSAH, family = binomial())\n\n然后，计算逻辑回归给出的概率：\n\n# 等价于直接使用 f$fitted\npred &lt;- predict(f, newdata = aSAH, type = \"response\")\n\naSAH$pred &lt;- pred\n\n用这个pred就可以画ROC曲线了：\n\nlibrary(yardstick)\nlibrary(ggplot2)\n\nroc_curve(data=aSAH, outcome, pred,event_level = \"second\") |&gt; \n  autoplot()"
  },
  {
    "objectID": "roc-many.html#测试集怎么办",
    "href": "roc-many.html#测试集怎么办",
    "title": "30  多指标联合诊断的ROC曲线",
    "section": "30.3 测试集怎么办？",
    "text": "30.3 测试集怎么办？\n很简单，只要把predict中的数据集换成测试集即可：\n\n# 换成测试集即可\npred &lt;- predict(f, newdata = 你的测试集, type = \"response\")\n\n剩下的就都一样了！"
  },
  {
    "objectID": "roc-bootstrap.html#fbroc",
    "href": "roc-bootstrap.html#fbroc",
    "title": "31  bootstrap ROC/AUC",
    "section": "31.1 fbroc",
    "text": "31.1 fbroc\n先介绍一个最简单的，用fbroc这个包实现，因为你在必应或者谷歌搜索bootstrap ROC in R，前几个结果中就是这个包。\n\nlibrary(fbroc)\n## Loading required package: ggplot2\n\n这个包在使用时需要把结果变量变为逻辑型：\n\noutcome1 &lt;- ifelse(aSAH$outcome == \"Good\",FALSE,TRUE)\n\n然后1行代码即可实现，默认是1000次bootstrap：\n\nset.seed(123)\nresult.boot &lt;- boot.roc(aSAH$s100b, outcome1)\nresult.boot\n## \n## Bootstraped uncached ROC Curve with 41 positive and 72 negative samples. \n##  \n## The AUC is 0.73.\n##  \n## 1000 bootstrap samples will be calculated. \n## The results use up 0 MB of memory.\n\n获取1000次bootstrap AUC的可信区间，还同时给出了标准误：\n\nset.seed(123)\nperf(result.boot, \"auc\", conf.level = 0.95)\n## \n## \n##                 Bootstrapped ROC performance metric\n## \n## Metric: AUC\n## Bootstrap replicates: 1000\n## Observed: 0.731\n## Std. Error: 0.052\n## 95% confidence interval:\n## 0.625 0.824\n\n把这1000条ROC曲线画在一起，就得到bootstrap ROC了：\n\nplot(result.boot)\n\n\n\n\n这个是我目前找到的最简单的方法。"
  },
  {
    "objectID": "roc-bootstrap.html#tidyverse",
    "href": "roc-bootstrap.html#tidyverse",
    "title": "31  bootstrap ROC/AUC",
    "section": "31.2 tidyverse",
    "text": "31.2 tidyverse\n后面的方法就是根据开头说的思路，一步一步的实现了。\n先说个tidy的方法，借助tidyverse和tidymodels实现。\n\nlibrary(yardstick)\nlibrary(rsample)\nlibrary(tidyverse)\n\n先说下如何在tidymodels中绘制ROC曲线，详情可参考：tidymodels-yardstick：衡量模型性能\n在tidymodels中画一条ROC曲线非常简单，首先是计算画图需要的数据：\n\nroc_data &lt;- roc_curve(aSAH, outcome, s100b,event_level = \"second\")\nroc_data\n## # A tibble: 52 × 3\n##    .threshold specificity sensitivity\n##         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n##  1    -Inf         0            1    \n##  2       0.03      0            1    \n##  3       0.04      0            0.976\n##  4       0.05      0.0694       0.976\n##  5       0.06      0.111        0.976\n##  6       0.07      0.139        0.976\n##  7       0.08      0.222        0.902\n##  8       0.09      0.306        0.878\n##  9       0.1       0.389        0.829\n## 10       0.11      0.486        0.780\n## # ℹ 42 more rows\n\n然后是画图：\n\nautoplot(roc_data)\n\n\n\n\n接下来只要使用bootstrap生成1000个自助集就可以很方便的绘制1000条ROC曲线了。\n生成1000个自助集：\n\nset.seed(123)\nasb &lt;- bootstraps(aSAH, times = 1000)\nasb\n## # Bootstrap sampling \n## # A tibble: 1,000 × 2\n##    splits           id           \n##    &lt;list&gt;           &lt;chr&gt;        \n##  1 &lt;split [113/44]&gt; Bootstrap0001\n##  2 &lt;split [113/43]&gt; Bootstrap0002\n##  3 &lt;split [113/47]&gt; Bootstrap0003\n##  4 &lt;split [113/41]&gt; Bootstrap0004\n##  5 &lt;split [113/37]&gt; Bootstrap0005\n##  6 &lt;split [113/37]&gt; Bootstrap0006\n##  7 &lt;split [113/39]&gt; Bootstrap0007\n##  8 &lt;split [113/38]&gt; Bootstrap0008\n##  9 &lt;split [113/33]&gt; Bootstrap0009\n## 10 &lt;split [113/42]&gt; Bootstrap0010\n## # ℹ 990 more rows\n\n定义一个函数，获取自助集：这是tidymodels中的常见操作，可参考：tidymodels数据划分\n\nff &lt;- function(split){analysis(split)}\n\n下面就是提取1000个自助集的数据，对每个自助集进行1次ROC分析，以获取画图数据：\n\nplot_data &lt;- asb %&gt;% \n  mutate(boot_data = map(splits, ff)) %&gt;% \n  unnest(boot_data) %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(outcome, s100b,event_level = \"second\") \n\ndim(plot_data)\n## [1] 40007     4\nhead(plot_data)\n## # A tibble: 6 × 4\n## # Groups:   id [1]\n##   id            .threshold specificity sensitivity\n##   &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n## 1 Bootstrap0001    -Inf         0            1    \n## 2 Bootstrap0001       0.04      0            1    \n## 3 Bootstrap0001       0.05      0.0779       1    \n## 4 Bootstrap0001       0.06      0.143        1    \n## 5 Bootstrap0001       0.07      0.195        1    \n## 6 Bootstrap0001       0.08      0.312        0.944\n\n最后把1000条ROC曲线画在一起即可：也就是大家需要的bootstrap ROC：\n\nggplot()+\n  # 自助集的ROC曲线，共1000条\n  geom_path(data = plot_data,\n            mapping=aes(1-specificity, sensitivity,group=id),color = \"grey\")+\n  # 原始数据的ROC曲线\n  geom_path(data = roc_data, mapping = aes(1-specificity, sensitivity),\n            color=\"blue\", linewidth=1.5)+\n  theme_bw()\n\n\n\n\n由于我们已经进行了1000次ROC分析，那自然就可以获得1000个AUC，所以根据这1000个AUC，就可以计算均值、标准差、标准误、可信区间。\n先获取1000个AUC：\n\nboot_auc &lt;- asb %&gt;% \n  mutate(boot_data = map(splits, ff)) %&gt;% \n  unnest(boot_data) %&gt;% \n  group_by(id) %&gt;% \n  roc_auc(outcome, s100b,event_level = \"second\") \n#boot_auc\ndim(boot_auc)\n## [1] 1000    4\nhead(boot_auc)\n## # A tibble: 6 × 4\n##   id            .metric .estimator .estimate\n##   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 Bootstrap0001 roc_auc binary         0.799\n## 2 Bootstrap0002 roc_auc binary         0.721\n## 3 Bootstrap0003 roc_auc binary         0.774\n## 4 Bootstrap0004 roc_auc binary         0.707\n## 5 Bootstrap0005 roc_auc binary         0.743\n## 6 Bootstrap0006 roc_auc binary         0.701\n\n这1000个AUC基本接近正态分布：\n\nggplot(boot_auc, aes(x=.estimate))+\n  geom_density()\n\n\n\n\n计算置信区间，公式如下（数学知识和统计知识，网络搜索或者看课本都可以）：\n\n可信区间下限 = 均值 - z * 标准误\n可信区间上限 = 均值 + z * 标准误\n\n先计算标准误：\n\nsample_mean &lt;- mean(boot_auc$.estimate)\nsample_mean\n## [1] 0.7315554\nsample_size &lt;- nrow(boot_auc)\nstandard_d &lt;- sd(boot_auc$.estimate)\nse &lt;- standard_d/sqrt(sample_size)\nse\n## [1] 0.001544964\n\n计算置信区间：\n\nconf_low &lt;- sample_mean - 1.96 * se\nconf_low\n## [1] 0.7285273\n\nconf_high &lt;- sample_mean + 1.96 * se\nconf_high\n## [1] 0.7345836"
  },
  {
    "objectID": "roc-bootstrap.html#base-r",
    "href": "roc-bootstrap.html#base-r",
    "title": "31  bootstrap ROC/AUC",
    "section": "31.3 base R",
    "text": "31.3 base R\n和tidy的方法没有本质区别，只是实现方式使用base R语法而已。这让我想起了某个外国网友对R的评论：目前很多人不是纠结于用R还是用Python，而是纠结于用base R还是tidy R。base R和tidy R真是太割裂了。\n先进行1次bootstrap（获取样本编号）看看效果：\n\nset.seed(123)\nbootset &lt;- sample(nrow(aSAH), size = nrow(aSAH), replace = T)\nbootset\n##   [1]  31  79  51  14  67  42  50  43 101  14  25  90  91  69  91  57  92   9\n##  [19]  93  99  72  26   7  42   9  83  36  78  81  43 103  76  15  32 106 109\n##  [37]   7   9  41  74  23  27  60  53   7  53  27  96  38  89  34  93  69  72\n##  [55]  76  63  13  82  97  91  25  38  21  79  41  47  90  60  95  16  94   6\n##  [73] 107  72  86  86  39  31 112  81  50 113  34   4  13  69  25  52  22  89\n##  [91]  32 110  25  87  35  40 112  30  12  31 110  30  64  99  14  93  96  71\n## [109]  67  23  79  85  37\n\n然后定义一个函数，获取每次的自助集：\n\nget_bootset &lt;- function(data){\n  boot_index &lt;- sample(nrow(data), size = nrow(data), replace = T)\n  bootset &lt;- data[boot_index,]\n  return(bootset)\n}\n\n#set.seed(123)\n#get_bootset(aSAH)\n\n使用bootstrap获取1000个自助集，通过for循环实现：\n\n# 每次结果都不一样\nbootsets &lt;- list()\nfor(i in 1:1000){\n  bootsets[[i]] &lt;- get_bootset(aSAH)\n}\nlength(bootsets)\n## [1] 1000\n\n对每一个自助集进行1次ROC分析，通过for循环实现：\n\nlibrary(pROC)\nrocs &lt;- list()\n\nfor(i in 1:1000){\n  rocs[[i]] &lt;- pROC::roc(bootsets[[i]][,\"outcome\"], bootsets[[i]][,\"s100b\"],\n                   quiet=T)\n}\n\n画1000条ROC曲线，还是通过for循环实现：\n\n# 提供一个画布\nplot(roc(aSAH$outcome, aSAH$s100b),col=\"blue\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\n# 画1000条ROC曲线\nfor(i in 1:1000){\n  lines.roc(rocs[[i]],col=\"grey\")\n}\n\n# 画完1000条把原来的挡住了，重新画一条\nlines.roc(roc(aSAH$outcome, aSAH$s100b),col=\"blue\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\n\n\n\n然后是计算1000个AUC的置信区间，和tidy的方法一样的。\n计算1000个AUC：\n\naucs &lt;- list()\n\nfor(i in 1:1000){\n  aucs[[i]] &lt;- auc(pROC::roc(bootsets[[i]][,\"outcome\"],bootsets[[i]][,\"s100b\"],\n                   quiet=T))\n}\naucs &lt;- unlist(aucs)\n\n计算可信区间：\n\nsample_mean &lt;- mean(aucs)\nsample_mean\n## [1] 0.7312995\nsample_size &lt;- length(aucs)\nstandard_d &lt;- sd(aucs)\nse &lt;- standard_d/sqrt(sample_size)\nse\n## [1] 0.001569356\n\n95%的可信区间，参考课本或者这个知乎的解释\n\nconf_low &lt;- sample_mean - 1.96 * se\nconf_low\n## [1] 0.7282235\n\nconf_high &lt;- sample_mean + 1.96 * se\nconf_high\n## [1] 0.7343754\n\n这种方法由于我没有在每次重抽样时设定种子数，导致结果是不可重复的哈，每次都不太一样~"
  },
  {
    "objectID": "roc-bootstrap.html#boot",
    "href": "roc-bootstrap.html#boot",
    "title": "31  bootstrap ROC/AUC",
    "section": "31.4 boot",
    "text": "31.4 boot\nboot是专门做重抽样的经典R包，在《R语言实战》一书中有详细介绍。\n通过这个包也可以计算bootstrap AUC的置信区间，但是这种方法只能计算指标，不能画ROC曲线。\n\nlibrary(boot)\nlibrary(pROC)\n\n定义一个函数，提取AUC：\n\n# boot的使用方式很奇怪\nget_auc &lt;- function(data, ind, outcome, predictor){\n  d = data[ind,] #这句必须加\n  au &lt;- as.numeric(auc(pROC::roc(d[,outcome], d[,predictor],quiet=T)))\n  au\n}\n\nget_auc(aSAH, outcome=\"outcome\",predictor=\"s100b\")\n## [1] 0.7313686\n\n提供给boot使用即可：\n\nset.seed(123)\nba &lt;- boot(aSAH, get_auc, R = 1000,\n           outcome=\"outcome\",predictor=\"s100b\")\nba\n## \n## ORDINARY NONPARAMETRIC BOOTSTRAP\n## \n## \n## Call:\n## boot(data = aSAH, statistic = get_auc, R = 1000, outcome = \"outcome\", \n##     predictor = \"s100b\")\n## \n## \n## Bootstrap Statistics :\n##      original       bias    std. error\n## t1* 0.7313686 0.0001084232  0.05365581\n\n结果给出了原始的AUC，以及1000次bootstrap得到的AUC的标准误。\n可以对这个结果画个图看看这1000个AUC的分布：\n\nplot(ba)\n\n\n\n\n获取这1000个AUC的置信区间，默认会给出95%的置信区间，并包含4种计算方法的结果：\n\nboot.ci(ba)\n## Warning in boot.ci(ba): bootstrap variances needed for studentized intervals\n## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n## Based on 1000 bootstrap replicates\n## \n## CALL : \n## boot.ci(boot.out = ba)\n## \n## Intervals : \n## Level      Normal              Basic         \n## 95%   ( 0.6261,  0.8364 )   ( 0.6314,  0.8479 )  \n## \n## Level     Percentile            BCa          \n## 95%   ( 0.6148,  0.8313 )   ( 0.6048,  0.8228 )  \n## Calculations and Intervals on Original Scale\n\n4种计算方法的置信区间都有了。\nOVER！"
  },
  {
    "objectID": "多分类数据的ROC曲线.html#yardstick",
    "href": "多分类数据的ROC曲线.html#yardstick",
    "title": "32  多分类数据的ROC曲线",
    "section": "32.1 yardstick",
    "text": "32.1 yardstick\nyardstick作为tidymodels的核心包之一，其使用语法和tidy系列完全一样，并且支持所有的tidy特性，学习成本非常低。而且背靠Rstudio这颗大树，稳定性和可靠性也有保障。\n接下来我们会演示衡量具有3个或更多类别的分类数据的方法。下面使用的示例数据包含了4个类别：\n\nlibrary(tidymodels)\n\ndata(hpc_cv)\ntibble(hpc_cv)\n## # A tibble: 3,467 × 7\n##    obs   pred     VF      F       M          L Resample\n##    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n##  1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n##  2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n##  3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n##  4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n##  5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n##  6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n##  7 VF    VF    0.914 0.0782 0.00767 0.0000354  Fold01  \n##  8 VF    VF    0.918 0.0744 0.00726 0.0000157  Fold01  \n##  9 VF    VF    0.843 0.128  0.0296  0.000192   Fold01  \n## 10 VF    VF    0.920 0.0728 0.00703 0.0000147  Fold01  \n## # ℹ 3,457 more rows\n\n此数据的真实结果（obs）和预测结果（pred）都是因子型，并且数据中包含每个类别的预测概率（VF、F、M、L）。\n计算多分类性能指标的函数与计算二分类性能指标的函数用法基本相同：\n\naccuracy(hpc_cv, obs, pred)\n## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy multiclass     0.709\n\nmcc(hpc_cv, obs, pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 mcc     multiclass     0.515\n\n有一些指标是用来处理仅具有两个类别的数据的，但是可以通过一些方法将其扩展到具有多个类别的数据。比如宏平均（macro-averaging）、宏加权平均（macro-weighted averaging）和微平均（micro-averaging）：\n\n宏平均使用标准的二分类方法计算一组一对多的指标，并对这些指标取平均值。\n宏加权平均执行相同的操作，但平均值会根据每个类别中的样本数加权。\n微平均计算每个类别的贡献并汇总，然后从汇总中计算单个指标。\n\n我们可以基于上面的解释自己计算，也可以使用yardstick中的函数可以帮助我们实现，只需指定estimator参数即可：\n\nsensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n## # A tibble: 1 × 3\n##   .metric     .estimator .estimate\n##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n## 1 sensitivity macro          0.560\n\nsensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n## # A tibble: 1 × 3\n##   .metric     .estimator     .estimate\n##   &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt;\n## 1 sensitivity macro_weighted     0.709\n\nsensitivity(hpc_cv, obs, pred, estimator = \"micro\")\n## # A tibble: 1 × 3\n##   .metric     .estimator .estimate\n##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n## 1 sensitivity micro          0.709\n\n也有一些指标使用预测概率衡量多分类数据，比如ROC曲线，此时必须将所有预测概率列都提供给函数：\n\nroc_auc(hpc_cv, obs, VF, F, M, L) # 默认hand_till法\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc hand_till      0.829\n\n除此之外，ROC曲线还可以使用宏加权平均衡量多分类数据：\n\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro_weighted\")\n## # A tibble: 1 × 3\n##   .metric .estimator     .estimate\n##   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n## 1 roc_auc macro_weighted     0.868\n\n或者使用宏平均法：\n\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro\")\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc macro          0.869\n\n画图也是非常简单，先使用roc_curve计算画图所需数据（注意此函数不能更改计算方法），然后使用ggplot2画图即可：\n\nroc_curve(hpc_cv, obs, VF, F, M, L) %&gt;% \n  autoplot()\n\n\n\n\n结果会同时展示4个类别的ROC曲线，当然也可以自己提取数据画。数就是图，图就是数，只要把数据提取出来，就一定能画出来图。提取数据才是最关键的一步，因为画图无非就是ggplot2而已，随便买本书（我只推荐两本：《ggplot2数据分析与图形艺术》和《R数据可视化手册》）认真看看，ggplot2就学得差不多了。提取数据多数都是列表(list)格式，在tidy的世界中更常见的是tibble和列表列，这就需要你认真学习R语言的基础知识。\n\nplot_data &lt;- roc_curve(hpc_cv, obs, VF, F, M, L)\nclass(plot_data)\n## [1] \"roc_df\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nglimpse(plot_data)\n## Rows: 13,876\n## Columns: 4\n## $ .level      &lt;chr&gt; \"VF\", \"VF\", \"VF\", \"VF\", \"VF\", \"VF\", \"VF\", \"VF\", \"VF\", \"VF\"…\n## $ .threshold  &lt;dbl&gt; -Inf, 4.118856e-18, 1.151238e-16, 1.447982e-16, 1.857902e-…\n## $ specificity &lt;dbl&gt; 0.0000000000, 0.0000000000, 0.0005889282, 0.0011778563, 0.…\n## $ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 0.9…\n\n这个结果一看就知道.level是4个类别，.threshold是预测变量的阈值，剩下两列是敏感性和特异性，也就是画图需要的数据。\n可以只画其中一个类别，我们以VF这个类别为例：\n\nplot_data %&gt;% \n  filter(.level == \"VF\") %&gt;% \n  ggplot(aes(1-specificity, sensitivity))+\n  geom_line()+\n  geom_abline(linetype = 2)+\n  theme_bw()\n\n\n\n\n而且有了数据，你很容易可以实现bootstrap ROC/AUC，无非就是重复多次而已。可参考：bootstrap ROC/AUC\n所有类别都画就是加个分面就行了：\n\nplot_data %&gt;% \n  #filter(.level == \"VF\") %&gt;% \n  ggplot(aes(1-specificity, sensitivity))+\n  geom_line()+\n  geom_abline(linetype = 2)+\n  facet_wrap(vars(.level))+\n  theme_bw()\n\n\n\n\n一模一样。\n所有这些性能指标都可以使用dplyr进行分组计算：\n\n# 每一次重抽样的1-vs-所有ROC曲线\nhpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  roc_curve(obs, VF, F, M, L) %&gt;% \n  autoplot()\n\n\n\n\n以上就是yardstick的简单介绍。后台回复tidymodels即可获取相关推文合集."
  },
  {
    "objectID": "多分类数据的ROC曲线.html#multiroc",
    "href": "多分类数据的ROC曲线.html#multiroc",
    "title": "32  多分类数据的ROC曲线",
    "section": "32.2 multiROC",
    "text": "32.2 multiROC\n也可以实现多分类的ROC曲线，但是很久不更新了，上次更新是2018.6.26。\ngithub地址：https://github.com/WandeRum/multiROC\n\nlibrary(multiROC)\n\n还是用上面的数据作为演示。使用起来很费事，首先得准备正确的格式。需要一个data.frame，包含真实标签和预测数值（可以是概率也可以不是，但肯定不能是预测类别），真实标签必须每个类别都是单独的1列，且列名必须是xxx_true，且必须用1表示阳性，0表示阴性，预测数值的列名必须是xxx_pred。\n首先是真实类别进行独热编码，然后改列名：\n\nhpc_cv1 &lt;- hpc_cv %&gt;% \n  model.matrix(~obs-1, .) %&gt;% \n  bind_cols(hpc_cv %&gt;% select(VF,F,M,L))\n\n# 预测结果必须添加一个  _pred_xx 后缀，不然就报错，太zz了。。。\ncolnames(hpc_cv1) &lt;- c(paste0(c(\"VF\",\"F\",\"M\",\"L\"),\"_true\"),\n                       paste0(c(\"VF\",\"F\",\"M\",\"L\"),\"_pred_rf\")) # _rf是随便加的\n\nhead(hpc_cv1)\n##   VF_true F_true M_true L_true VF_pred_rf  F_pred_rf   M_pred_rf    L_pred_rf\n## 1       1      0      0      0  0.9136340 0.07786694 0.008479147 1.991225e-05\n## 2       1      0      0      0  0.9380672 0.05710623 0.004816447 1.011557e-05\n## 3       1      0      0      0  0.9473710 0.04946767 0.003156287 4.999849e-06\n## 4       1      0      0      0  0.9289077 0.06528949 0.005787179 1.564496e-05\n## 5       1      0      0      0  0.9418764 0.05430830 0.003808013 7.294581e-06\n## 6       1      0      0      0  0.9510978 0.04618223 0.002716177 3.841455e-06\n\n计算AUC，会同时使用宏平均和微平均计算：\n\nroc_res &lt;- multi_roc(hpc_cv1)\n## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n## collapsing to unique 'x' values\n\n## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n## collapsing to unique 'x' values\n\n## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n## collapsing to unique 'x' values\n\n## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n## collapsing to unique 'x' values\nroc_res$AUC\n## $rf\n## $rf$VF\n## [1] 0.9145978\n## \n## $rf$F\n## [1] 0.7912642\n## \n## $rf$M\n## [1] 0.8389398\n## \n## $rf$L\n## [1] 0.9322527\n## \n## $rf$macro\n## [1] 0.8692609\n## \n## $rf$micro\n## [1] 0.9028392\n\n计算画图数据：\n\nplot_roc_df &lt;- plot_roc_data(roc_res)\ndim(plot_roc_df)\n## [1] 38140     5\nhead(plot_roc_df)\n##   Specificity  Sensitivity Group       AUC Method\n## 1           1 0.0000000000    VF 0.9145978     rf\n## 2           1 0.0005652911    VF 0.9145978     rf\n## 3           1 0.0011305822    VF 0.9145978     rf\n## 4           1 0.0016958734    VF 0.9145978     rf\n## 5           1 0.0022611645    VF 0.9145978     rf\n## 6           1 0.0028264556    VF 0.9145978     rf\n\n然后使用ggplot2画图即可：\n\nggplot(plot_roc_df, aes(x = 1-Specificity, y=Sensitivity)) +\n  geom_path(aes(color = Group), linewidth=1.5) +\n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1), \n               colour='grey', linetype = 'dotdash') +\n  theme_bw() + \n  theme(plot.title = element_text(hjust = 0.5), \n        legend.justification=c(1, 0), legend.position=c(.95, .05),\n        legend.title=element_blank(), \n        legend.background = element_rect(fill=NULL, linewidth=0.5, \n                                         linetype=\"solid\",colour =\"black\")\n        )\n\n\n\n\n这个图是全都画在一起的，你也可以使用分面的形式。\n画图是很简单的，但是说实话这个准备数据的过程不太方便~\nOVER！\n后台回复ROC获取超全合集，回复最佳截点即可获取ROC曲线的最佳截点合集。"
  },
  {
    "objectID": "1032-survival.html#生存过程的描述",
    "href": "1032-survival.html#生存过程的描述",
    "title": "33  R语言生存分析",
    "section": "33.1 生存过程的描述",
    "text": "33.1 生存过程的描述\n\nlibrary(survival)\nlibrary(survminer)\n## Loading required package: ggplot2\n## Warning: package 'ggplot2' was built under R version 4.2.3\n## Loading required package: ggpubr\n## \n## Attaching package: 'survminer'\n## The following object is masked from 'package:survival':\n## \n##     myeloma\n\n使用survival包中的lung数据集用于演示，这是一份关于肺癌患者的生存数据。time是生存时间，以天为单位，status是生存状态，1代表删失，2代表死亡。但是一般在生存分析中我们喜欢用1代表死亡，用0代表删失，所以我们更改一下（其实不改也可以，你记住就行）。\n\ndf &lt;- lung\ndf$status &lt;- ifelse(df$status == 2,1,0)\nstr(df)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  1 1 0 1 1 0 1 1 1 1 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\n首先把生存时间和生存状态用Surv()放到一起，可以看到有+的就是截尾数据。\n\nSurv(time = lung$time, event = lung$status)\n##   [1]  306   455  1010+  210   883  1022+  310   361   218   166   170   654 \n##  [13]  728    71   567   144   613   707    61    88   301    81   624   371 \n##  [25]  394   520   574   118   390    12   473    26   533   107    53   122 \n##  [37]  814   965+   93   731   460   153   433   145   583    95   303   519 \n##  [49]  643   765   735   189    53   246   689    65     5   132   687   345 \n##  [61]  444   223   175    60   163    65   208   821+  428   230   840+  305 \n##  [73]   11   132   226   426   705   363    11   176   791    95   196+  167 \n##  [85]  806+  284   641   147   740+  163   655   239    88   245   588+   30 \n##  [97]  179   310   477   166   559+  450   364   107   177   156   529+   11 \n## [109]  429   351    15   181   283   201   524    13   212   524   288   363 \n## [121]  442   199   550    54   558   207    92    60   551+  543+  293   202 \n## [133]  353   511+  267   511+  371   387   457   337   201   404+  222    62 \n## [145]  458+  356+  353   163    31   340   229   444+  315+  182   156   329 \n## [157]  364+  291   179   376+  384+  268   292+  142   413+  266+  194   320 \n## [169]  181   285   301+  348   197   382+  303+  296+  180   186   145   269+\n## [181]  300+  284+  350   272+  292+  332+  285   259+  110   286   270    81 \n## [193]  131   225+  269   225+  243+  279+  276+  135    79    59   240+  202+\n## [205]  235+  105   224+  239   237+  173+  252+  221+  185+   92+   13   222+\n## [217]  192+  183   211+  175+  197+  203+  116   188+  191+  105+  174+  177+\n\n如果只是想要描述一下这份生存数据，可以使用寿命表法或者K-M曲线，在R中可以通过survfit()实现。\n\n# 构建生存曲线\nfit &lt;- survfit(Surv(time, status) ~ 1, data = df)\n\n# 寿命表，surv_summary比默认的summary()更好\nsurv_summary(fit)\n##     time n.risk n.event n.censor       surv     std.err     upper      lower\n## 1      5    228       1        0 0.99561404 0.004395615 1.0000000 0.98707342\n## 2     11    227       3        0 0.98245614 0.008849904 0.9996460 0.96556190\n## 3     12    224       1        0 0.97807018 0.009916654 0.9972662 0.95924368\n## 4     13    223       2        0 0.96929825 0.011786516 0.9919508 0.94716300\n## 5     15    221       1        0 0.96491228 0.012628921 0.9890941 0.94132171\n## 6     26    220       1        0 0.96052632 0.013425540 0.9861367 0.93558107\n## 7     30    219       1        0 0.95614035 0.014184183 0.9830945 0.92992527\n## 8     31    218       1        0 0.95175439 0.014910735 0.9799794 0.92434234\n## 9     53    217       2        0 0.94298246 0.016284897 0.9735659 0.91335978\n## 10    54    215       1        0 0.93859649 0.016939076 0.9702809 0.90794671\n## 11    59    214       1        0 0.93421053 0.017574720 0.9669508 0.90257880\n## 12    60    213       2        0 0.92543860 0.018798180 0.9601711 0.89196244\n## 13    61    211       1        0 0.92105263 0.019389168 0.9567281 0.88670745\n## 14    62    210       1        0 0.91666667 0.019968077 0.9532533 0.88148430\n## 15    65    209       2        0 0.90789474 0.021093908 0.9462168 0.87112471\n## 16    71    207       1        0 0.90350877 0.021642644 0.9426590 0.86598451\n## 17    79    206       1        0 0.89912281 0.022182963 0.9390770 0.86086855\n## 18    81    205       2        0 0.89035088 0.023240987 0.9318456 0.85070391\n## 19    88    203       2        0 0.88157895 0.024272607 0.9245323 0.84062118\n## 20    92    201       1        1 0.87719298 0.024779731 0.9208475 0.83560803\n## 21    93    199       1        0 0.87278498 0.025286647 0.9171308 0.83058337\n## 22    95    198       2        0 0.86396897 0.026285933 0.9096467 0.82058489\n## 23   105    196       1        1 0.85956096 0.026778995 0.9058807 0.81560966\n## 24   107    194       2        0 0.85069951 0.027763443 0.8982733 0.80564534\n## 25   110    192       1        0 0.84626878 0.028250266 0.8944478 0.80068492\n## 26   116    191       1        0 0.84183806 0.028733836 0.8906085 0.79573831\n## 27   118    190       1        0 0.83740733 0.029214392 0.8867559 0.79080503\n## 28   122    189       1        0 0.83297660 0.029692160 0.8828904 0.78588462\n## 29   131    188       1        0 0.82854588 0.030167350 0.8790125 0.78097668\n## 30   132    187       2        0 0.81968442 0.031110783 0.8712208 0.77119665\n## 31   135    185       1        0 0.81525370 0.031579392 0.8673077 0.76632386\n## 32   142    184       1        0 0.81082297 0.032046159 0.8633836 0.76146212\n## 33   144    183       1        0 0.80639224 0.032511243 0.8594487 0.75661112\n## 34   145    182       2        0 0.79753079 0.033436970 0.8515479 0.74694024\n## 35   147    180       1        0 0.79310006 0.033897900 0.8475824 0.74211984\n## 36   153    179       1        0 0.78866934 0.034357720 0.8436072 0.73730913\n## 37   156    178       2        0 0.77980788 0.035274546 0.8356287 0.72771592\n## 38   163    176       3        0 0.76651570 0.036644539 0.8235936 0.71339353\n## 39   166    173       2        0 0.75765425 0.037555674 0.8155273 0.70388809\n## 40   167    171       1        0 0.75322352 0.038010898 0.8114818 0.69914771\n## 41   170    170       1        0 0.74879280 0.038466026 0.8074284 0.69441536\n## 42   173    169       0        1 0.74879280 0.038466026 0.8074284 0.69441536\n## 43   174    168       0        1 0.74879280 0.038466026 0.8074284 0.69441536\n## 44   175    167       1        1 0.74430901 0.038932090 0.8033269 0.68962694\n## 45   176    165       1        0 0.73979804 0.039403839 0.7991969 0.68481391\n## 46   177    164       1        1 0.73528708 0.039875693 0.7950587 0.68000904\n## 47   179    162       2        0 0.72620946 0.040831745 0.7867159 0.67035655\n## 48   180    160       1        0 0.72167065 0.041310284 0.7825326 0.66554231\n## 49   181    159       2        0 0.71259304 0.042268879 0.7741425 0.65593716\n## 50   182    157       1        0 0.70805423 0.042749126 0.7699360 0.65114603\n## 51   183    156       1        0 0.70351542 0.043230132 0.7657221 0.64636237\n## 52   185    155       0        1 0.70351542 0.043230132 0.7657221 0.64636237\n## 53   186    154       1        0 0.69894713 0.043718251 0.7614780 0.64155115\n## 54   188    153       0        1 0.69894713 0.043718251 0.7614780 0.64155115\n## 55   189    152       1        0 0.69434880 0.044213739 0.7572033 0.63671178\n## 56   191    151       0        1 0.69434880 0.044213739 0.7572033 0.63671178\n## 57   192    150       0        1 0.69434880 0.044213739 0.7572033 0.63671178\n## 58   194    149       1        0 0.68968874 0.044723618 0.7528734 0.63180684\n## 59   196    148       0        1 0.68968874 0.044723618 0.7528734 0.63180684\n## 60   197    147       1        1 0.68499698 0.045241530 0.7485112 0.62687218\n## 61   199    145       1        0 0.68027286 0.045767770 0.7441162 0.62190715\n## 62   201    144       2        0 0.67082463 0.046824116 0.7353020 0.61200115\n## 63   202    142       1        1 0.66610051 0.047354439 0.7308831 0.60705997\n## 64   203    140       0        1 0.66610051 0.047354439 0.7308831 0.60705997\n## 65   207    139       1        0 0.66130842 0.047901723 0.7264037 0.60204649\n## 66   208    138       1        0 0.65651633 0.048450680 0.7219163 0.59704112\n## 67   210    137       1        0 0.65172424 0.049001423 0.7174208 0.59204373\n## 68   211    136       0        1 0.65172424 0.049001423 0.7174208 0.59204373\n## 69   212    135       1        0 0.64689665 0.049562270 0.7128898 0.58701260\n## 70   218    134       1        0 0.64206907 0.050125134 0.7083507 0.58198951\n## 71   221    133       0        1 0.64206907 0.050125134 0.7083507 0.58198951\n## 72   222    132       1        1 0.63720491 0.050698710 0.7037752 0.57693155\n## 73   223    130       1        0 0.63230333 0.051283424 0.6991623 0.57183791\n## 74   224    129       0        1 0.63230333 0.051283424 0.6991623 0.57183791\n## 75   225    128       0        2 0.63230333 0.051283424 0.6991623 0.57183791\n## 76   226    126       1        0 0.62728505 0.051898763 0.6944504 0.56661573\n## 77   229    125       1        0 0.62226677 0.052516642 0.6897296 0.56140253\n## 78   230    124       1        0 0.61724849 0.053137208 0.6849999 0.55619818\n## 79   235    123       0        1 0.61724849 0.053137208 0.6849999 0.55619818\n## 80   237    122       0        1 0.61724849 0.053137208 0.6849999 0.55619818\n## 81   239    121       2        0 0.60704604 0.054428498 0.6753847 0.54562217\n## 82   240    119       0        1 0.60704604 0.054428498 0.6753847 0.54562217\n## 83   243    118       0        1 0.60704604 0.054428498 0.6753847 0.54562217\n## 84   245    117       1        0 0.60185761 0.055101203 0.6704957 0.54024596\n## 85   246    116       1        0 0.59666918 0.055777281 0.6655969 0.53487943\n## 86   252    115       0        1 0.59666918 0.055777281 0.6655969 0.53487943\n## 87   259    114       0        1 0.59666918 0.055777281 0.6655969 0.53487943\n## 88   266    113       0        1 0.59666918 0.055777281 0.6655969 0.53487943\n## 89   267    112       1        0 0.59134178 0.056493740 0.6605811 0.52935986\n## 90   268    111       1        0 0.58601437 0.057214008 0.6555547 0.52385081\n## 91   269    110       1        1 0.58068697 0.057938291 0.6505179 0.51835217\n## 92   270    108       1        0 0.57531024 0.058680326 0.6454326 0.51280626\n## 93   272    107       0        1 0.57531024 0.058680326 0.6454326 0.51280626\n## 94   276    106       0        1 0.57531024 0.058680326 0.6454326 0.51280626\n## 95   279    105       0        1 0.57531024 0.058680326 0.6454326 0.51280626\n## 96   283    104       1        0 0.56977841 0.059470446 0.6402172 0.50708954\n## 97   284    103       1        1 0.56424658 0.060265393 0.6349901 0.50138454\n## 98   285    101       2        0 0.55307338 0.061902647 0.6244165 0.48988160\n## 99   286     99       1        0 0.54748678 0.062729652 0.6191120 0.48414791\n## 100  288     98       1        0 0.54190018 0.063562614 0.6137958 0.47842592\n## 101  291     97       1        0 0.53631358 0.064401818 0.6084680 0.47271553\n## 102  292     96       0        2 0.53631358 0.064401818 0.6084680 0.47271553\n## 103  293     94       1        0 0.53060812 0.065283876 0.6030365 0.46687880\n## 104  296     93       0        1 0.53060812 0.065283876 0.6030365 0.46687880\n## 105  300     92       0        1 0.53060812 0.065283876 0.6030365 0.46687880\n## 106  301     91       1        1 0.52477726 0.066212421 0.5974962 0.46090869\n## 107  303     89       1        1 0.51888089 0.067169680 0.5918922 0.45487570\n## 108  305     87       1        0 0.51291674 0.068157318 0.5862225 0.44877769\n## 109  306     86       1        0 0.50695259 0.069153590 0.5805384 0.44269407\n## 110  310     85       2        0 0.49502429 0.071173772 0.5691277 0.43056952\n## 111  315     83       0        1 0.49502429 0.071173772 0.5691277 0.43056952\n## 112  320     82       1        0 0.48898741 0.072223700 0.5633452 0.42444435\n## 113  329     81       1        0 0.48295053 0.073284268 0.5575481 0.41833381\n## 114  332     80       0        1 0.48295053 0.073284268 0.5575481 0.41833381\n## 115  337     79       1        0 0.47683723 0.074383257 0.5516775 0.41214973\n## 116  340     78       1        0 0.47072393 0.075494166 0.5457918 0.40598083\n## 117  345     77       1        0 0.46461064 0.076617562 0.5398911 0.39982704\n## 118  348     76       1        0 0.45849734 0.077754031 0.5339753 0.39368826\n## 119  350     75       1        0 0.45238404 0.078904180 0.5280446 0.38756443\n## 120  351     74       1        0 0.44627074 0.080068634 0.5220991 0.38145549\n## 121  353     73       2        0 0.43404415 0.082443090 0.5101637 0.36928207\n## 122  356     71       0        1 0.43404415 0.082443090 0.5101637 0.36928207\n## 123  361     70       1        0 0.42784352 0.083689321 0.5041055 0.36311858\n## 124  363     69       2        0 0.41544226 0.086235271 0.4919424 0.35083836\n## 125  364     67       1        1 0.40924162 0.087536643 0.4858376 0.34472158\n## 126  371     65       2        0 0.39664957 0.090283246 0.4734305 0.33232098\n## 127  376     63       0        1 0.39664957 0.090283246 0.4734305 0.33232098\n## 128  382     62       0        1 0.39664957 0.090283246 0.4734305 0.33232098\n## 129  384     61       0        1 0.39664957 0.090283246 0.4734305 0.33232098\n## 130  387     60       1        0 0.39003875 0.091834363 0.4669574 0.32579034\n## 131  390     59       1        0 0.38342792 0.093411868 0.4604644 0.31927978\n## 132  394     58       1        0 0.37681710 0.095017143 0.4539514 0.31278928\n## 133  404     57       0        1 0.37681710 0.095017143 0.4539514 0.31278928\n## 134  413     56       0        1 0.37681710 0.095017143 0.4539514 0.31278928\n## 135  426     55       1        0 0.36996588 0.096772712 0.4472339 0.30604732\n## 136  428     54       1        0 0.36311466 0.098561472 0.4404935 0.29932852\n## 137  429     53       1        0 0.35626344 0.100385300 0.4337299 0.29263289\n## 138  433     52       1        0 0.34941222 0.102246185 0.4269433 0.28596045\n## 139  442     51       1        0 0.34256100 0.104146240 0.4201335 0.27931128\n## 140  444     50       1        1 0.33570978 0.106087711 0.4133006 0.27268545\n## 141  450     48       1        0 0.32871582 0.108156668 0.4063345 0.26592397\n## 142  455     47       1        0 0.32172187 0.110274202 0.3993431 0.25918807\n## 143  457     46       1        0 0.31472792 0.112443281 0.3923261 0.25247790\n## 144  458     45       0        1 0.31472792 0.112443281 0.3923261 0.25247790\n## 145  460     44       1        0 0.30757501 0.114769476 0.3851616 0.24561738\n## 146  473     43       1        0 0.30042210 0.117156914 0.3779689 0.23878538\n## 147  477     42       1        0 0.29326919 0.119609626 0.3707476 0.23198214\n## 148  511     41       0        2 0.29326919 0.119609626 0.3707476 0.23198214\n## 149  519     39       1        0 0.28574947 0.122397820 0.3632207 0.22480203\n## 150  520     38       1        0 0.27822975 0.125269565 0.3556585 0.21765764\n## 151  524     37       2        0 0.26319030 0.131289244 0.3404266 0.20347744\n## 152  529     35       0        1 0.26319030 0.131289244 0.3404266 0.20347744\n## 153  533     34       1        0 0.25544941 0.134640748 0.3325916 0.19619977\n## 154  543     33       0        1 0.25544941 0.134640748 0.3325916 0.19619977\n## 155  550     32       1        0 0.24746662 0.138333639 0.3245387 0.18869779\n## 156  551     31       0        1 0.24746662 0.138333639 0.3245387 0.18869779\n## 157  558     30       1        0 0.23921773 0.142427599 0.3162481 0.18095008\n## 158  559     29       0        1 0.23921773 0.142427599 0.3162481 0.18095008\n## 159  567     28       1        0 0.23067424 0.146997865 0.3076975 0.17293157\n## 160  574     27       1        0 0.22213075 0.151765851 0.2990832 0.16497774\n## 161  583     26       1        0 0.21358726 0.156752465 0.2904045 0.15708959\n## 162  588     25       0        1 0.21358726 0.156752465 0.2904045 0.15708959\n## 163  613     24       1        0 0.20468779 0.162428228 0.2814175 0.14887877\n## 164  624     23       1        0 0.19578832 0.168401942 0.2723521 0.14074818\n## 165  641     22       1        0 0.18688885 0.174710378 0.2632068 0.13269961\n## 166  643     21       1        0 0.17798938 0.181396440 0.2539797 0.12473524\n## 167  654     20       1        0 0.16908991 0.188510603 0.2446686 0.11685766\n## 168  655     19       1        0 0.16019044 0.196112784 0.2352708 0.10906995\n## 169  687     18       1        0 0.15129097 0.204274810 0.2257834 0.10137573\n## 170  689     17       1        0 0.14239151 0.213083712 0.2162028 0.09377928\n## 171  705     16       1        0 0.13349204 0.222646211 0.2065248 0.08628565\n## 172  707     15       1        0 0.12459257 0.233094916 0.1967446 0.07890080\n## 173  728     14       1        0 0.11569310 0.244597108 0.1868568 0.07163182\n## 174  731     13       1        0 0.10679363 0.257367445 0.1768548 0.06448724\n## 175  735     12       1        0 0.09789416 0.271686878 0.1667313 0.05747732\n## 176  740     11       0        1 0.09789416 0.271686878 0.1667313 0.05747732\n## 177  765     10       1        0 0.08810474 0.291418720 0.1559751 0.04976720\n## 178  791      9       1        0 0.07831533 0.314346559 0.1450170 0.04229358\n## 179  806      8       0        1 0.07831533 0.314346559 0.1450170 0.04229358\n## 180  814      7       1        0 0.06712742 0.350176075 0.1333431 0.03379322\n## 181  821      6       0        1 0.06712742 0.350176075 0.1333431 0.03379322\n## 182  840      5       0        1 0.06712742 0.350176075 0.1333431 0.03379322\n## 183  883      4       1        0 0.05034557 0.453824434 0.1225342 0.02068546\n## 184  965      3       0        1 0.05034557 0.453824434 0.1225342 0.02068546\n## 185 1010      2       0        1 0.05034557 0.453824434 0.1225342 0.02068546\n## 186 1022      1       0        1 0.05034557 0.453824434 0.1225342 0.02068546\n\n画出生存曲线，横坐标是生存时间，纵坐标是生存率。\n\nggsurvplot(fit,\n           conf.int = TRUE, # 可信区间\n           palette= 'blue', # 更改配色\n           surv.median.line = \"hv\", # 中位生存时间\n           ggtheme = theme_bw() # 更改主题\n           \n)"
  },
  {
    "objectID": "1032-survival.html#生存过程的比较",
    "href": "1032-survival.html#生存过程的比较",
    "title": "33  R语言生存分析",
    "section": "33.2 生存过程的比较",
    "text": "33.2 生存过程的比较\n如果通过某个变量把数据分为多组，然后检验不同组别之间的生存时间（生存曲线）有无差别，则可以通过logrank检验或者breslow检验。\n在R语言中通过survdiff()实现logrank检验。\n\nfit &lt;- survdiff(Surv(time, status) ~ sex, data = df)\nfit\n## Call:\n## survdiff(formula = Surv(time, status) ~ sex, data = df)\n## \n##         N Observed Expected (O-E)^2/E (O-E)^2/V\n## sex=1 138      112     91.6      4.55      10.3\n## sex=2  90       53     73.4      5.68      10.3\n## \n##  Chisq= 10.3  on 1 degrees of freedom, p= 0.001\n\n可以用神包broom提取数据：\n\nbroom::tidy(fit)\n## # A tibble: 2 × 4\n##   sex       N   obs   exp\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 1       138   112  91.6\n## 2 2        90    53  73.4\nbroom::glance(fit)\n## # A tibble: 1 × 3\n##   statistic    df p.value\n##       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n## 1      10.3     1 0.00131\n\n对于不同组别之间生存曲线的检验，也可以通过K-M图示的方法：\n\nfit.logrank &lt;- survfit(Surv(time, status) ~ sex, data = df)\n\n# 这一步输出太多，我注释掉了，可以自己运行看看\n# surv_summary(fit.logrank) # 可以查看寿命表\n\n通过ggsurvplot()进行可视化，非常多的细节可以修改，超级详细的教程可以参考我的另一篇推文：R语言生存曲线的可视化(超详细)\n\nggsurvplot(fit.logrank, \n           data = df,\n           surv.median.line = \"hv\", # Add medians survival\n           \n           # Change legends: title & labels\n           legend.title = \"Sex\",\n           legend.labs = c(\"Male\", \"Female\"),\n           \n           # Add p-value and tervals\n           pval = TRUE, # 这里P值直接写数字也行\n           conf.int = TRUE,\n           \n           # Add risk table\n           risk.table = TRUE, \n           tables.height = 0.2,\n           tables.theme = theme_cleantable(),\n           \n           ncensor.plot = TRUE,\n           \n           # Color palettes. Use custom color: c(\"#E7B800\", \"#2E9FDF\"),\n           # or brewer color (e.g.: \"Dark2\"), or ggsci color (e.g.: \"jco\")\n           palette = c(\"#E7B800\", \"#2E9FDF\"),\n           ggtheme = theme_bw(), # Change ggplot2 theme\n           \n           # Change font size, style and color\n           main = \"Survival curve\",\n           font.main = c(16, \"bold\", \"darkblue\"),\n           font.x = c(14, \"bold.italic\", \"red\"),\n           font.y = c(14, \"bold.italic\", \"darkred\"),\n           font.tickslab = c(12, \"plain\", \"darkgreen\")\n)\n\n\n\n\n自带的surv_cutpoint()可用于寻找最佳切点，但是只能用于连续性数据。\n使用myeloma数据进行演示。\n\nrm(list = ls())\n\n# 0. Load some data\ndata(myeloma)\nhead(myeloma)\n##          molecular_group chr1q21_status treatment event  time   CCND1 CRIM1\n## GSM50986      Cyclin D-1       3 copies       TT2     0 69.24  9908.4 420.9\n## GSM50988      Cyclin D-2       2 copies       TT2     0 66.43 16698.8  52.0\n## GSM50989           MMSET       2 copies       TT2     0 66.50   294.5 617.9\n## GSM50990           MMSET       3 copies       TT2     1 42.67   241.9  11.9\n## GSM50991             MAF           &lt;NA&gt;       TT2     0 65.00   472.6  38.8\n## GSM50992    Hyperdiploid       2 copies       TT2     0 65.20   664.1  16.9\n##          DEPDC1    IRF4   TP53   WHSC1\n## GSM50986  523.5 16156.5   10.0   261.9\n## GSM50988   21.1 16946.2 1056.9   363.8\n## GSM50989  192.9  8903.9 1762.8 10042.9\n## GSM50990  184.7 11894.7  946.8  4931.0\n## GSM50991  212.0  7563.1  361.4   165.0\n## GSM50992  341.6 16023.4 2096.3   569.2\n\n寻找最佳切点：\n\n# 1. Determine the optimal cutpoint of variables\nres.cut &lt;- surv_cutpoint(myeloma, time = \"time\", event = \"event\",\n                         variables = c(\"DEPDC1\", \"WHSC1\", \"CRIM1\") # 找这3个变量的最佳切点\n                         )\n\nsummary(res.cut)\n##        cutpoint statistic\n## DEPDC1    279.8  4.275452\n## WHSC1    3205.6  3.361330\n## CRIM1      82.3  1.968317\n\n查看根据最佳切点进行分组后的数据分布情况：\n\n# 2. Plot cutpoint for DEPDC1\nplot(res.cut, \"DEPDC1\", palette = \"npg\")\n## $DEPDC1\n\n\n\n\n根据最佳切点重新划分数据，这样数据就根据最佳切点变成了高表达/低表达组。\n\n# 3. Categorize variables\nres.cat &lt;- surv_categorize(res.cut)\nhead(res.cat)\n##           time event DEPDC1 WHSC1 CRIM1\n## GSM50986 69.24     0   high   low  high\n## GSM50988 66.43     0    low   low   low\n## GSM50989 66.50     0    low  high  high\n## GSM50990 42.67     1    low  high   low\n## GSM50991 65.00     0    low   low   low\n## GSM50992 65.20     0   high   low   low\n\n根据最佳切点绘制生存曲线：\n\n# 4. Fit survival curves and visualize\nlibrary(\"survival\")\nfit &lt;- survfit(Surv(time, event) ~DEPDC1, data = res.cat)\nggsurvplot(fit, data = res.cat, risk.table = TRUE, conf.int = TRUE)\n\n\n\n\n确定最佳切点的R包还有非常多，其他的后续会再介绍。\n下次继续介绍Cox回归。"
  },
  {
    "objectID": "1032-survival.html#cox回归",
    "href": "1032-survival.html#cox回归",
    "title": "33  R语言生存分析",
    "section": "33.3 Cox回归",
    "text": "33.3 Cox回归\n上次介绍了生存分析中的寿命表、K-M曲线、logrank检验、最佳切点的寻找等，本次主要介绍Cox回归。\n本推文不涉及理论，只有实操，想要了解生存分析的理论的请自行学习。\n使用survival包中的lung数据集用于演示，这是一份关于肺癌患者的生存数据。time是生存时间，以天为单位，status是生存状态，1代表删失，2代表死亡。\n\nrm(list = ls())\nlibrary(survival)\nlibrary(survminer)\n\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\n可以使用cox回归探索危险因素。分类变量需要变为因子型，这样在进行回归时会自动进行哑变量设置。\n\nlung$sex &lt;- factor(lung$sex, labels = c(\"female\",\"male\"))\nlung$ph.ecog &lt;- factor(lung$ph.ecog, labels = c(\"asymptomatic\", \"symptomatic\",\n                                                'in bed &lt;50%','in bed &gt;50%'))\n\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : Factor w/ 2 levels \"female\",\"male\": 1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : Factor w/ 4 levels \"asymptomatic\",..: 2 1 1 2 1 2 3 3 2 3 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\n拟合多因素Cox回归模型，这里我们只用sex/age两个变量做演示：\n\nfit.cox &lt;- coxph(Surv(time, status) ~ sex + age + ph.karno, data = lung)\n\n# 查看结果\nsummary(fit.cox)\n## Call:\n## coxph(formula = Surv(time, status) ~ sex + age + ph.karno, data = lung)\n## \n##   n= 227, number of events= 164 \n##    (1 observation deleted due to missingness)\n## \n##               coef exp(coef)  se(coef)      z Pr(&gt;|z|)   \n## sexmale  -0.497170  0.608249  0.167713 -2.964  0.00303 **\n## age       0.012375  1.012452  0.009405  1.316  0.18821   \n## ph.karno -0.013322  0.986767  0.005880 -2.266  0.02348 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##          exp(coef) exp(-coef) lower .95 upper .95\n## sexmale     0.6082     1.6441    0.4378    0.8450\n## age         1.0125     0.9877    0.9940    1.0313\n## ph.karno    0.9868     1.0134    0.9755    0.9982\n## \n## Concordance= 0.637  (se = 0.025 )\n## Likelihood ratio test= 18.81  on 3 df,   p=3e-04\n## Wald test            = 18.73  on 3 df,   p=3e-04\n## Score (logrank) test = 19.05  on 3 df,   p=3e-04\n\n结果解读和logistic回归的结果解读类似：R语言logistic回归的细节解读\n\ncoef是回归系数，\nexp(coef)是HR值，\nse(coef)是回归系数的标准误，\nz是Wald检验的z值，\nPr(&gt;|z|)是回归系数的P值，\nlower .95/upper .95是HR值的95%可信区间。\n\nConcordance= 0.645是Cox回归的C-index，最后给出了Likelihood ratio test似然比检验的统计量、自由度、P值；Wald test的统计量、自由度、P值；Score (logrank) test的统计量、自由度、P值。\n想获得整洁的结果不需要自己提取，只要用神包broom即可：\n\nbroom::tidy(fit.cox, exponentiate = T, conf.int = T)\n## # A tibble: 3 × 7\n##   term     estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 sexmale     0.608   0.168       -2.96 0.00303    0.438     0.845\n## 2 age         1.01    0.00940      1.32 0.188      0.994     1.03 \n## 3 ph.karno    0.987   0.00588     -2.27 0.0235     0.975     0.998\n\n\nestimate：HR值（exp(coef)）\nstd.error：回归系数的标准误（se(coef)）\nstatistic：Wald检验的z值\np.value：回归系数的P值\nconf.low/conf.high：HR的95%的可信区间\n\n构建好Cox回归后，也可以用函数单独提取想要的结果，以下图片展示了可用于提取模型信息的函数，和logistic回归差不多：\n\n\n\n\n\n\n\n\n\n\n进行Cox回归必须要符合等比例风险假设，关于什么是等比例风险假设，可以参考郑老师的这篇文章：生存分析COX回归，小心你的数据不符合应用条件\n等比例风险的检验可以通过很多方法进行，比如K-M曲线，一般如果有交叉，那么可能不符合等比例风险假设，还可以通过各种残差分布来检验。\n下面是Cox回归的等比例风险假设检验，检验方法是基于Schoenfeld残差：\n\nftest &lt;- cox.zph(fit.cox)\nftest\n##           chisq df      p\n## sex       3.085  1 0.0790\n## age       0.478  1 0.4892\n## ph.karno  8.017  1 0.0046\n## GLOBAL   10.359  3 0.0157\n\n可以看到ph.karno的P值是小于0.05的，其实是不满足等比例风险假设的，下一篇推文会说到不符合等比例风险假设时该怎么办。\n这种方法是基于Schoenfeld残差，检验结果可以通过图示画出来：\n\nlibrary(survminer)\n\nggcoxzph(ftest)\n\n\n\n\n可以看到sex和age的回归系数随着时间变化基本没啥变化，稳定在0水平线上，和上面的检验结果一样。\n还可以通过以下方式查看残差的变化：\n\nggcoxdiagnostics(fit.cox, type = \"schoenfeld\")\n## Warning: `gather_()` was deprecated in tidyr 1.2.0.\n## ℹ Please use `gather()` instead.\n## ℹ The deprecated feature was likely used in the survminer package.\n##   Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n这张图反映的也是回归系数随时间的变化趋势，和上面的图意思一样，如果符合比例风险假设，那么结果应该是一条水平线，从图示来看，这3个变量都是有点问题的，但是真实数据往往不可能是完美的，很少有完全符合要求的数据。\n除了Schoenfeld残差外，ggcoxdiagnostics()还支持其他类型，比如：“martingale”, “deviance”, “score”,“dfbetas” and “scaledsch”在，只需要在type参数中提供合适的类型即可。\ncox回归也是回归分析的一种，可以计算出回归系数和95%的可信区间，因此结果可以通过森林图展示：\n\n# 为了森林图好看点，多选几个变量\nfit.cox &lt;- coxph(Surv(time, status) ~ . , data = lung)\n\nggforest(fit.cox, data = lung,\n         main = \"Hazard ratio\",\n         cpositions = c(0.01, 0.15, 0.35), # 更改前三列的相对位置\n         fontsize = 0.7,\n         refLabel = \"reference\",\n         noDigits = 2\n         )\n\n\n\n\n这个结果如果你觉得不好看，或者你还有其他的森林图想做到统一的样式，可以考虑我公众号中介绍的画森林图的方法进行个性化定制:\n\n画一个好看的森林图\n用更简单的方式画森林图\nR语言画森林图系列3\nR语言画森林图系列4\nggplot2绘制森林图(有亚组和没亚组)\n\n以上是Cox回归的主要内容，大家有问题可以加群或者评论区留言。"
  },
  {
    "objectID": "1032-survival.html#时间依存协变量的cox回归和时间依存系数cox回归",
    "href": "1032-survival.html#时间依存协变量的cox回归和时间依存系数cox回归",
    "title": "33  R语言生存分析",
    "section": "33.4 时间依存协变量的Cox回归和时间依存系数Cox回归",
    "text": "33.4 时间依存协变量的Cox回归和时间依存系数Cox回归\n之前分别介绍了生存分析中的寿命表法、K-M曲线、logrank检验，以及Cox回归的构建、可视化以及比例风险检验的内容。\n本次主要介绍如果数据不符合PH假设时采取的方法。\n关于时依协变量、时依系数的基础知识，大家可以参考这几篇文章：\n\nsurvival包的案例介绍：Using Time Dependent Covariates and Time Dependent Coefcients in the Cox Model\n医咖会：一文详解时依协变量\n7code：含时依协变量的Cox回归\n\n如果不能满足PH假设，可以考虑使用时依协变量或者时依系数Cox回归，时依协变量和时依系数是两个概念，简单来说就是如果一个协变量本身会随着时间而改变，这种叫时依协变量，如果是协变量的系数随着时间改变，这种叫时依系数。\n这里以survival包的veteran数据集为例，演示如何处理此类不符合PH检验的情况。\n\nrm(list = ls())\nlibrary(survival)\nstr(veteran)\n## 'data.frame':    137 obs. of  8 variables:\n##  $ trt     : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ celltype: Factor w/ 4 levels \"squamous\",\"smallcell\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ time    : num  72 411 228 126 118 10 82 110 314 100 ...\n##  $ status  : num  1 1 1 1 1 1 1 1 1 0 ...\n##  $ karno   : num  60 70 60 60 70 20 40 80 50 70 ...\n##  $ diagtime: num  7 5 3 9 11 5 10 29 18 6 ...\n##  $ age     : num  69 64 38 63 65 49 69 68 43 70 ...\n##  $ prior   : num  0 10 0 10 10 0 10 0 0 0 ...\n\n这个数据集中的变量解释如下图：\n\n\n\n\n\n首先构建普通的Cox回归，进行等比例风险假设，这里只选择了trt/prior/karno3个变量，而且trt/prior作为分类变量并没有转换为因子型，因为二分类变量数值型和因子型的结果是一样的，转不转换没啥影响！\n\nfit &lt;- coxph(Surv(time, status) ~ trt + prior + karno, data = veteran)\n\n# 进行PH检验\nzp &lt;- cox.zph(fit)\nzp\n##         chisq df       p\n## trt     0.288  1 0.59125\n## prior   2.168  1 0.14087\n## karno  12.138  1 0.00049\n## GLOBAL 18.073  3 0.00042\n\n可以看到变量karno的P值小于0.05，是不满足PH假设的。\n通过图形化方法查看PH检验的结果：\n\n#op &lt;- par(mfrow=c(1,3))\n#plot(zp)\n#par(op)\n#ggcoxdiagnostics(fit, type = \"schoenfeld\")\nplot(zp[3])\nabline(0,0, col=\"red\") # 0水平线\nabline(h=fit$coef[3], col=\"green\", lwd=2, lty=2) \n\n\n\n\n黑色实线以及两侧的虚线是karno的系数随着时间变化的曲线，绿色虚线是假设karno符合PH检验时的总体估计线，红色实线是参考线。\n这张图反映了karno变量的系数随着时间的改变，karno偏离的比较厉害（上面注释掉的代码可以都运行看看其他变量的情况），系数最开始接近-0.05，然后逐渐趋于0，最后又开始趋向-0.05，所以它的系数是一致在随着时间改变的，不符合比例风险假设。\n\n33.4.1 对时间分层\n这种情况下一个比较简单的解决方式是对时间使用分层函数。根据上面的图示我们知道karno的系数大概分为3层（3段），可以根据两个拐点进行分层，通过survival中的survSplit()实现。\n\nvet2 &lt;- survSplit(Surv(time, status) ~ ., data= veteran, \n                  cut=c(90, 180), # 两个拐点把时间分为3层（3段）\n                  episode= \"tgroup\", \n                  id=\"id\")\nvet2[1:7, c(\"id\", \"tstart\", \"time\", \"status\", \"tgroup\", \"age\", \"karno\")]\n##   id tstart time status tgroup age karno\n## 1  1      0   72      1      1  69    60\n## 2  2      0   90      0      1  64    70\n## 3  2     90  180      0      2  64    70\n## 4  2    180  411      1      3  64    70\n## 5  3      0   90      0      1  38    60\n## 6  3     90  180      0      2  38    60\n## 7  3    180  228      1      3  38    60\n\n结果多了两列：tstart/tgroup。\n受试者1（id编号为1）在第72天的时候死了，所以数据和之前一样。受试者2和3（id为2和3）虽然时间在变，但是直到第3层才死去，karno的值没有变化。\n重新拟合Cox模型，此时tgroup是分好的层，所以要用strata()，另外karno会随着时间变化，和时间有交互，所以用karno:strata(tgroup)。\n\n# 注意此时Surv()的用法！\nfit2 &lt;- coxph(Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), data = vet2)\nfit2\n## Call:\n## coxph(formula = Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), \n##     data = vet2)\n## \n##                                   coef exp(coef)  se(coef)      z        p\n## trt                          -0.011025  0.989035  0.189062 -0.058    0.953\n## prior                        -0.006107  0.993912  0.020355 -0.300    0.764\n## karno:strata(tgroup)tgroup=1 -0.048755  0.952414  0.006222 -7.836 4.64e-15\n## karno:strata(tgroup)tgroup=2  0.008050  1.008083  0.012823  0.628    0.530\n## karno:strata(tgroup)tgroup=3 -0.008349  0.991686  0.014620 -0.571    0.568\n## \n## Likelihood ratio test=63.04  on 5 df, p=2.857e-12\n## n= 225, number of events= 128\n\n结果表明karno这个变量只有在tgroup=1（第1层，前3个月）才有意义，后面两层是没有意义的。\n再次进行PH检验：\n\ncox.zph(fit2)\n##                      chisq df     p\n## trt                   1.72  1 0.189\n## prior                 3.81  1 0.051\n## karno:strata(tgroup)  3.04  3 0.385\n## GLOBAL                8.03  5 0.154\n\n这时karno:strata(tgroup)就满足了等比例风险假设。\n\n\n33.4.2 连续性时依系数变换\n除了对时间进行分层外，还有一种解决方法。\n上面的图中我们可以看出karno系数随时间变化的曲线明显不是线性的，我们可以通过数据变换把它变成类似线性的，比如取log，这种变换通过tt(time transform)函数实现。\n这种方法实际上是通过tt()函数构建了一个时依协变量，但是这样做是为了解决系数随着时间改变的问题（也就是为了解决时依系数的问题）。\n\nfit3 &lt;- coxph(Surv(time, status) ~ trt + prior + karno + tt(karno), # 对karno进行变换\n              data = veteran, \n              tt = function(x, t, ...) x * log(t+20) # 具体变换方式\n              )\nfit3\n## Call:\n## coxph(formula = Surv(time, status) ~ trt + prior + karno + tt(karno), \n##     data = veteran, tt = function(x, t, ...) x * log(t + 20))\n## \n##                coef exp(coef)  se(coef)      z        p\n## trt        0.016478  1.016614  0.190707  0.086  0.93115\n## prior     -0.009317  0.990726  0.020296 -0.459  0.64619\n## karno     -0.124662  0.882795  0.028785 -4.331 1.49e-05\n## tt(karno)  0.021310  1.021538  0.006607  3.225  0.00126\n## \n## Likelihood ratio test=53.84  on 4 df, p=5.698e-11\n## n= 137, number of events= 128\n\n此时karno的时依系数估计为：-0.124662 * log(t + 20)。\n在构建时依协变量时，可以选择x * t、x * log(t)、x * log(t + 20)、x * log(t + 200)等等，没有明确的规定，要结合结果和图示进行选择，可以参考冯国双老师的文章：一文详解时依协变量。\n我们可以把现在的时依系数估计和经过变换后的的PH检验画在一起，看看变换后的效果：\n\n# 变换后的PH检验\nzp &lt;- cox.zph(fit, transform = function(time) log(time + 20))\n\n# 画图\nplot(zp[3])\nabline(0,0, col=\"red\") # 0水平线\nabline(h=fit$coef[3], col=\"green\", lwd=2, lty=2) # 整体估计\nabline(coef(fit3)[3:4],lwd=2,lty=3,col=\"blue\") # 现在的估计\n\n\n\n\n可以看到变换后结果好多了（蓝色虚线，和黑色曲线相比较），虽然还是有一点倾斜。\n以上是两种处理不满足PH假设的方法，实际还有很多种方法，比较常用的是对时间进行分层，其他方法有机会继续介绍。"
  },
  {
    "objectID": "1032-survival.html#参考资料",
    "href": "1032-survival.html#参考资料",
    "title": "33  R语言生存分析",
    "section": "33.5 参考资料",
    "text": "33.5 参考资料\n\nhttp://www.sthda.com/english/wiki/survival-analysis-basics\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html\nsurvival包帮助文档\nhttps://mp.weixin.qq.com/s/2rwxeaF_M0UnqPi2F9JNxA"
  },
  {
    "objectID": "1033-survivalvis.html#演示数据",
    "href": "1033-survivalvis.html#演示数据",
    "title": "34  R语言生存曲线可视化",
    "section": "34.1 演示数据",
    "text": "34.1 演示数据\n使用survival包中的lung数据集用于演示，这是一份关于肺癌患者的生存数据。time是生存时间，以天为单位，status是生存状态，1代表删失，2代表死亡。\n\nlibrary(survival)\nlibrary(survminer)\n## Loading required package: ggplot2\n## Warning: package 'ggplot2' was built under R version 4.2.3\n## Loading required package: ggpubr\n## \n## Attaching package: 'survminer'\n## The following object is masked from 'package:survival':\n## \n##     myeloma\n\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\nfit &lt;- survfit(Surv(time, status) ~ sex, data = lung)"
  },
  {
    "objectID": "1033-survivalvis.html#基本的生存曲线",
    "href": "1033-survivalvis.html#基本的生存曲线",
    "title": "34  R语言生存曲线可视化",
    "section": "34.2 基本的生存曲线",
    "text": "34.2 基本的生存曲线\n最基本的生存曲线：\n\nggsurvplot(fit, data = lung)\n\n\n\n\n删失数据的形状可以更改，默认是+，我们可以改成自己喜欢的：\n\n# 更改删失数据的形状、大小\nggsurvplot(fit, data = lung, censor.shape=\"|\", censor.size = 4)\n\n\n\n\n字体都是可以进行更改的！\n\nggsurvplot(fit, data = lung,\n           surv.median.line = \"hv\", # 中位生存时间\n   title = \"Survival curves\", \n   subtitle = \"Based on Kaplan-Meier estimates\",\n   caption = \"created with survminer\",\n   font.title = c(16, \"bold\", \"darkblue\"), # 大小、粗细、颜色\n   font.subtitle = c(15, \"bold.italic\", \"purple\"),\n   font.caption = c(14, \"plain\", \"orange\"),\n   font.x = c(14, \"bold.italic\", \"red\"),\n   font.y = c(14, \"bold.italic\", \"darkred\"),\n   font.tickslab = c(12, \"plain\", \"darkgreen\"))\n\n\n\n\n累积风险曲线：\n\nggsurvplot(fit,\n           fun = \"cumhaz\", \n           conf.int = TRUE, # 可信区间\n           palette = \"lancet\", # 支持ggsci配色，自定义颜色，brewer palettes中的配色，等\n           ggtheme = theme_bw() # 支持ggplot2及其扩展包的主题\n)\n\n\n\n\n累积事件曲线：\n\nggsurvplot(fit,\n           fun = \"event\", \n           conf.int = TRUE, # 可信区间\n           palette = \"grey\",\n           ggtheme = theme_pubclean() \n)"
  },
  {
    "objectID": "1033-survivalvis.html#增加-risk-table",
    "href": "1033-survivalvis.html#增加-risk-table",
    "title": "34  R语言生存曲线可视化",
    "section": "34.3 增加 risk table",
    "text": "34.3 增加 risk table\n增加多种自定义选项：\n\nggsurvplot(\n  fit,\n  data = lung,\n  size = 1,                 # 更改线条粗细\n  # 配色方案，支持ggsci配色，自定义颜色，brewer palettes中的配色，等\n  palette = \"lancet\",\n  conf.int = TRUE,          # 可信区间\n  pval = TRUE,              # log-rank P值，也可以提供一个数值\n  #计算P值的方法，\n  pval.method = TRUE,       \n  log.rank.weights = \"1\",\n  risk.table = TRUE,        # 增加risk table\n  risk.table.col = \"strata\",# risk table根据分组使用不同颜色\n  legend.labs = c(\"Male\", \"Female\"),    # 图例标签\n  risk.table.height = 0.25, # risk table高度\n  ggtheme = theme_classic2()      # 主题，支持ggplot2及其扩展包的主题\n)\n\n\n\n\n计算P值的方法，可参考https://rpkgs.datanovia.com/survminer/articles/Specifiying_weights_in_log-rank_comparisons.html\n\nggsurvplot(\n   fit,                     \n   data = lung,             \n   risk.table = TRUE,       \n   pval = TRUE,             \n   conf.int = TRUE,         \n   xlim = c(0,500),         # 横坐标轴范围，相当于局部放大\n   xlab = \"Time in days\",   # 横坐标标题\n   break.time.by = 100,     # 横坐标刻度\n   ggtheme = theme_light(), \n   risk.table.y.text.col = T, # risk table文字注释颜色\n   risk.table.y.text = FALSE # risk table显示条形而不是文字\n)\n\n\n\n\nrisk table的各种字体也都是可以更改的！\n\nggsurvplot(fit, data = lung,\n   title = \"Survival curves\", subtitle = \"Based on Kaplan-Meier estimates\",\n   caption = \"created with survminer\",\n   font.title = c(16, \"bold\", \"darkblue\"),\n   font.subtitle = c(15, \"bold.italic\", \"purple\"),\n   font.caption = c(14, \"plain\", \"orange\"),\n   font.x = c(14, \"bold.italic\", \"red\"),\n   font.y = c(14, \"bold.italic\", \"darkred\"),\n   font.tickslab = c(12, \"plain\", \"darkgreen\"),\n   ########## risk table #########,\n   risk.table = TRUE,\n   risk.table.title = \"Note the risk set sizes\",\n   risk.table.subtitle = \"and remember about censoring.\",\n   risk.table.caption = \"source code: website.com\",\n   risk.table.height = 0.45)"
  },
  {
    "objectID": "1033-survivalvis.html#增加删失时间表ncensor-plot",
    "href": "1033-survivalvis.html#增加删失时间表ncensor-plot",
    "title": "34  R语言生存曲线可视化",
    "section": "34.4 增加删失时间表ncensor plot",
    "text": "34.4 增加删失时间表ncensor plot\n\nggsurvplot(fit, data = lung, risk.table = TRUE, ncensor.plot = TRUE)\n\n\n\n\nncensor plot的字体也是支持各种设置的。\n\nggsurvplot(fit, data = lung,\n   title = \"Survival curves\", subtitle = \"Based on Kaplan-Meier estimates\",\n   caption = \"created with survminer\",\n   font.title = c(16, \"bold\", \"darkblue\"),\n   font.subtitle = c(15, \"bold.italic\", \"purple\"),\n   font.caption = c(14, \"plain\", \"orange\"),\n   font.x = c(14, \"bold.italic\", \"red\"),\n   font.y = c(14, \"bold.italic\", \"darkred\"),\n   font.tickslab = c(12, \"plain\", \"darkgreen\"),\n   ########## risk table #########,\n   risk.table = TRUE,\n   risk.table.title = \"Note the risk set sizes\",\n   risk.table.subtitle = \"and remember about censoring.\",\n   risk.table.caption = \"source code: website.com\",\n   risk.table.height = 0.2,\n   ## ncensor plot ##\n   ncensor.plot = TRUE,\n   ncensor.plot.title = \"Number of censorings\",\n   ncensor.plot.subtitle = \"over the time.\",\n   ncensor.plot.caption = \"data available at data.com\",\n   ncensor.plot.height = 0.25)"
  },
  {
    "objectID": "1033-survivalvis.html#超级无敌精细化自定设置",
    "href": "1033-survivalvis.html#超级无敌精细化自定设置",
    "title": "34  R语言生存曲线可视化",
    "section": "34.5 超级无敌精细化自定设置",
    "text": "34.5 超级无敌精细化自定设置\n首先设置好自己的默认样式：\n\nggsurv &lt;- ggsurvplot(\n           fit,                     \n           data = lung,             \n           risk.table = TRUE,       \n           pval = TRUE,             \n           conf.int = TRUE,         \n           palette = c(\"#E7B800\", \"#2E9FDF\"),\n           xlim = c(0,500),         \n           xlab = \"Time in days\",   \n           break.time.by = 100,     \n           ggtheme = theme_light(), \n          risk.table.y.text.col = T,\n          risk.table.height = 0.25, \n          risk.table.y.text = FALSE,\n          ncensor.plot = TRUE,      \n          ncensor.plot.height = 0.25,\n          conf.int.style = \"step\",  # customize style of confidence intervals\n          surv.median.line = \"hv\",  \n          legend.labs = c(\"Male\", \"Female\")    \n        )\nggsurv\n\n\n\n\n自定义一个函数，用来更改各种样式：\n\ncustomize_labels &lt;- function (p, font.title = NULL,\n                              font.subtitle = NULL, font.caption = NULL,\n                              font.x = NULL, font.y = NULL, font.xtickslab = NULL, font.ytickslab = NULL)\n{\n  original.p &lt;- p\n  if(is.ggplot(original.p)) list.plots &lt;- list(original.p)\n  else if(is.list(original.p)) list.plots &lt;- original.p\n  else stop(\"Can't handle an object of class \", class (original.p))\n  .set_font &lt;- function(font){\n    font &lt;- ggpubr:::.parse_font(font)\n    ggtext::element_markdown (size = font$size, face = font$face, colour = font$color)\n  }\n  for(i in 1:length(list.plots)){\n    p &lt;- list.plots[[i]]\n    if(is.ggplot(p)){\n      if (!is.null(font.title)) p &lt;- p + theme(plot.title = .set_font(font.title))\n      if (!is.null(font.subtitle)) p &lt;- p + theme(plot.subtitle = .set_font(font.subtitle))\n      if (!is.null(font.caption)) p &lt;- p + theme(plot.caption = .set_font(font.caption))\n      if (!is.null(font.x)) p &lt;- p + theme(axis.title.x = .set_font(font.x))\n      if (!is.null(font.y)) p &lt;- p + theme(axis.title.y = .set_font(font.y))\n      if (!is.null(font.xtickslab)) p &lt;- p + theme(axis.text.x = .set_font(font.xtickslab))\n      if (!is.null(font.ytickslab)) p &lt;- p + theme(axis.text.y = .set_font(font.ytickslab))\n      list.plots[[i]] &lt;- p\n    }\n  }\n  if(is.ggplot(original.p)) list.plots[[1]]\n  else list.plots\n}\n\n然后分别对上面图形的3个部分（生存曲线、risk table、ncensor plot）进行个性化自定义\n\n# 更改生存曲线的标签\nggsurv$plot &lt;- ggsurv$plot + labs(\n  title    = \"Survival curves\",\n  subtitle = \"Based on Kaplan-Meier estimates\",\n  caption  = \"created with survminer\"\n  )\n\n# 更改risk table的标签\nggsurv$table &lt;- ggsurv$table + labs(\n  title    = \"Note the risk set sizes\",\n  subtitle = \"and remember about censoring.\",\n  caption  = \"source code: website.com\"\n  )\n\n# 更改ncensor plot的标签 \nggsurv$ncensor.plot &lt;- ggsurv$ncensor.plot + labs(\n  title    = \"Number of censorings\",\n  subtitle = \"over the time.\",\n  caption  = \"source code: website.com\"\n  )\n\n# 更改生存曲线，risk table，ncensor plot的字体大小、类型、颜色\n\nggsurv &lt;- customize_labels(\n  ggsurv,\n  font.title    = c(16, \"bold\", \"darkblue\"),\n  font.subtitle = c(15, \"bold.italic\", \"purple\"),\n  font.caption  = c(14, \"plain\", \"orange\"),\n  font.x        = c(14, \"bold.italic\", \"red\"),\n  font.y        = c(14, \"bold.italic\", \"darkred\"),\n  font.xtickslab = c(12, \"plain\", \"darkgreen\")\n)\n\nggsurv"
  },
  {
    "objectID": "1033-survivalvis.html#多个组的生存曲线",
    "href": "1033-survivalvis.html#多个组的生存曲线",
    "title": "34  R语言生存曲线可视化",
    "section": "34.6 多个组的生存曲线",
    "text": "34.6 多个组的生存曲线\n如果你的分类变量是多个组别的（常见的都是两组比较的），会自动画出多条生存曲线。如果你有多个分类自变量，会自动画出所有组合的生存曲线。\n使用colon数据集，其中time是时间，status是生存状态，1为发生终点事件，0为删失，rx是治疗方式，有三种：observation、Levamisole、Levamisole+5-FU，obstruct是肿瘤是否阻塞结肠，有为1，无为0，adhere是肿瘤是否粘附附近器官，有为1，无为0。\n\nrm(list = ls())\nlibrary(survival)\nlibrary(survminer)\n\npsych::headTail(colon)\n##       id study      rx sex age obstruct perfor adhere nodes status differ\n## 1      1     1 Lev+5FU   1  43        0      0      0     5      1      2\n## 2      1     1 Lev+5FU   1  43        0      0      0     5      1      2\n## 3      2     1 Lev+5FU   1  63        0      0      0     1      0      2\n## 4      2     1 Lev+5FU   1  63        0      0      0     1      0      2\n## ...  ...   ...    &lt;NA&gt; ... ...      ...    ...    ...   ...    ...    ...\n## 1855 928     1 Lev+5FU   0  48        1      0      0     4      0      2\n## 1856 928     1 Lev+5FU   0  48        1      0      0     4      0      2\n## 1857 929     1     Lev   0  66        1      0      0     1      0      2\n## 1858 929     1     Lev   0  66        1      0      0     1      0      2\n##      extent surg node4 time etype\n## 1         3    0     1 1521     2\n## 2         3    0     1  968     1\n## 3         3    0     0 3087     2\n## 4         3    0     0 3087     1\n## ...     ...  ...   ...  ...   ...\n## 1855      3    1     1 2072     2\n## 1856      3    1     1 2072     1\n## 1857      3    0     0 1820     2\n## 1858      3    0     0 1820     1\n\n# 两个分类变量\nfit2 &lt;- survfit( Surv(time, status) ~ rx + obstruct, data = colon )\n\n# 结果会给出所有组合的生存曲线\nggsurvplot(fit2, pval = TRUE, \n           risk.table = TRUE,\n           risk.table.height = 0.3\n           )"
  },
  {
    "objectID": "1033-survivalvis.html#多个分类变量分面绘制",
    "href": "1033-survivalvis.html#多个分类变量分面绘制",
    "title": "34  R语言生存曲线可视化",
    "section": "34.7 多个分类变量分面绘制",
    "text": "34.7 多个分类变量分面绘制\n还是以colon数据集为例，这次我们用3个变量：sex/rx/adhere，这3个都是分类变量。\n首先构建生存函数：\n\nfit3 &lt;- survfit(Surv(time, status) ~ sex + rx + adhere, data = colon )\n\n然后把生存曲线保存为一个对象：\n\nggsurv &lt;- ggsurvplot(fit3, data = colon,\n  fun = \"cumhaz\", conf.int = TRUE,\n  risk.table = TRUE, risk.table.col=\"strata\",\n  ggtheme = theme_bw())\n\n接下来就可以分别提取生存曲线（这里是cumhaz，累积风险曲线）、risk table、删失事件表，根据不同的变量进行分面即可：\n\n# 分面累积风险曲线\ncurv_facet &lt;- ggsurv$plot + facet_grid(rx ~ adhere)\ncurv_facet\n\n\n\n\n\n# 分面risk table，和上面的累积风险曲线分面方法一样\nggsurv$table + facet_grid(rx ~ adhere, scales = \"free\")+\n theme(legend.position = \"none\")\n\n\n\n\n\n# risk table另一种分面方法，由于有3个分类变量，可以选择自己需要的分面方法\ntbl_facet &lt;- ggsurv$table + facet_grid(.~ adhere, scales = \"free\")\ntbl_facet + theme(legend.position = \"none\")\n\n\n\n\n\n# 重新安排下布局，把生存曲线和risk table画在一起\ng2 &lt;- ggplotGrob(curv_facet)\ng3 &lt;- ggplotGrob(tbl_facet)\nmin_ncol &lt;- min(ncol(g2), ncol(g3))\ng &lt;- gridExtra::gtable_rbind(g2[, 1:min_ncol], g3[, 1:min_ncol], size=\"last\")\ng$widths &lt;- grid::unit.pmax(g2$widths, g3$widths)\ngrid::grid.newpage()\ngrid::grid.draw(g)\n\n\n\n\n如果想根据某个变量进行分组绘制生存曲线，然后分面展示，也可以用ggsurvplot_facet()实现：\n\nfit &lt;- survfit( Surv(time, status) ~ sex, data = colon )\n\n# 根据rx进行分组，展示每个组内的生存曲线\nggsurvplot_facet(fit, colon, \n                 facet.by = \"rx\",\n                 palette = \"jco\", \n                 pval = TRUE)\n## Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n## ℹ Please use `as_tibble()` instead.\n## ℹ The signature and semantics have changed, see `?as_tibble`.\n## ℹ The deprecated feature was likely used in the survminer package.\n##   Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n## Warning: `select_()` was deprecated in dplyr 0.7.0.\n## ℹ Please use `select()` instead.\n## ℹ The deprecated feature was likely used in the survminer package.\n##   Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n\n\n\n\n还可以根据多个变量进行分面展示：\n\nggsurvplot_facet(fit, colon, facet.by = c(\"rx\", \"adhere\"),\n                palette = \"jco\", pval = TRUE)\n\n\n\n\n\nfit2 &lt;- survfit( Surv(time, status) ~ sex + rx, data = colon )\nggsurvplot_facet(fit2, colon, facet.by = \"adhere\",\n                palette = \"jco\", pval = TRUE)"
  },
  {
    "objectID": "1033-survivalvis.html#同时绘制多个生存函数",
    "href": "1033-survivalvis.html#同时绘制多个生存函数",
    "title": "34  R语言生存曲线可视化",
    "section": "34.8 同时绘制多个生存函数",
    "text": "34.8 同时绘制多个生存函数\n\ndata(colon)\n## Warning in data(colon): data set 'colon' not found\nf1 &lt;- survfit(Surv(time, status) ~ adhere, data = colon)\nf2 &lt;- survfit(Surv(time, status) ~ rx, data = colon)\nfits &lt;- list(sex = f1, rx = f2)\n\n# 一下子画好！在循环出图时有用处\nlegend.title &lt;- list(\"sex\", \"rx\")\nggsurvplot_list(fits, colon, legend.title = legend.title)\n## $sex\n\n\n\n## \n## $rx\n\n\n\n## \n## attr(,\"class\")\n## [1] \"list\"            \"ggsurvplot_list\""
  },
  {
    "objectID": "1033-survivalvis.html#根据某一个变量分组绘制",
    "href": "1033-survivalvis.html#根据某一个变量分组绘制",
    "title": "34  R语言生存曲线可视化",
    "section": "34.9 根据某一个变量分组绘制",
    "text": "34.9 根据某一个变量分组绘制\n比如以colon数据为例，我们想以rx（治疗方式）进行分组，然后看每个组内的生存曲线，可以通过ggsurvplot_group_by()实现。\n\nrm(list = ls())\n\nfit &lt;- survfit( Surv(time, status) ~ sex, data = colon )\n\n# Visualize: grouped by treatment rx\n#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\nggsurv.list &lt;- ggsurvplot_group_by(fit, colon, group.by = \"rx\", risk.table = TRUE,\n                                 pval = TRUE, conf.int = TRUE, palette = \"jco\")\nnames(ggsurv.list)\n## [1] \"rx.Obs::sex\"     \"rx.Lev::sex\"     \"rx.Lev+5FU::sex\"\n\n这个图形和上面的分面展示中的ggsurvplot_facet画出来的图形是一样的，区别就是一个是分面，这个是分开多个图形！\n可以根据多个变量进行分组，比如下面这个情况，会分别绘制6张生存曲线图：\n\n# Visualize: grouped by treatment rx and adhere\nggsurv.list &lt;- ggsurvplot_group_by(fit, colon, group.by = c(\"rx\", \"adhere\"),\n                                 risk.table = TRUE,\n                                 pval = TRUE, conf.int = TRUE, palette = \"jco\")\n\n# 6张图的名字，图没有画出来，感兴趣的可以自己试试看\nnames(ggsurv.list)\n## [1] \"rx:Obs, adhere:0::sex\"     \"rx:Obs, adhere:1::sex\"    \n## [3] \"rx:Lev, adhere:0::sex\"     \"rx:Lev, adhere:1::sex\"    \n## [5] \"rx:Lev+5FU, adhere:0::sex\" \"rx:Lev+5FU, adhere:1::sex\""
  },
  {
    "objectID": "1033-survivalvis.html#在原有生存曲线的基础上增加",
    "href": "1033-survivalvis.html#在原有生存曲线的基础上增加",
    "title": "34  R语言生存曲线可视化",
    "section": "34.10 在原有生存曲线的基础上增加",
    "text": "34.10 在原有生存曲线的基础上增加\n先画好一个生存曲线图，然后在原图的基础上添加新的生存曲线图，类似于base r中常用的add = T，比如在这篇推文中介绍的：多个时间点和多指标生存曲线\n\nlibrary(survival)\n\n# 注意这里的surv_fit，是survfit的封装\nfit &lt;- surv_fit(Surv(time, status) ~ sex, data = lung)\n\n# Visualize survival curves\nggsurvplot(fit, data = lung,\n          risk.table = TRUE, pval = TRUE,\n          surv.median.line = \"hv\", palette = \"jco\")\n\n\n\n\n在上面图形的基础上添加所有人的总的生存曲线：\n\n# Add survival curves of pooled patients (Null model)\n# Use add.all = TRUE option\nggsurvplot(fit, data = lung,\n          risk.table = TRUE, pval = TRUE,\n          surv.median.line = \"hv\", palette = \"jco\",\n          add.all = TRUE)"
  },
  {
    "objectID": "1033-survivalvis.html#多个生存函数画在一起",
    "href": "1033-survivalvis.html#多个生存函数画在一起",
    "title": "34  R语言生存曲线可视化",
    "section": "34.11 多个生存函数画在一起",
    "text": "34.11 多个生存函数画在一起\n比如把PFS和OS的生存曲线画在一张图上。\n\nrm(list = ls())\n# 构建一个示例数据集\nset.seed(123)\ndemo.data &lt;- data.frame(\n   os.time = colon$time,\n   os.status = colon$status,\n   pfs.time = sample(colon$time),\n   pfs.status = colon$status,\n   sex = colon$sex, rx = colon$rx, adhere = colon$adhere\n )\n\n# 总体的PFS和OS生存曲线\npfs &lt;- survfit( Surv(pfs.time, pfs.status) ~ 1, data = demo.data)\nos &lt;- survfit( Surv(os.time, os.status) ~ 1, data = demo.data)\n\n# Combine on the same plot\nfit &lt;- list(PFS = pfs, OS = os)\nggsurvplot_combine(fit, demo.data)\n\n\n\n\n这个情况你用ggsurvplot_list也能画，不过就是分开的两个图形了！\n如果是分类变量会自动画出多条生存曲线：\n\npfs &lt;- survfit( Surv(pfs.time, pfs.status) ~ rx, data = demo.data)\nos &lt;- survfit( Surv(os.time, os.status) ~ rx, data = demo.data)\n# Combine on the same plot\nfit &lt;- list(PFS = pfs, OS = os)\nggsurvplot_combine(fit, demo.data)"
  },
  {
    "objectID": "1033-survivalvis.html#参考资料",
    "href": "1033-survivalvis.html#参考资料",
    "title": "34  R语言生存曲线可视化",
    "section": "34.12 参考资料",
    "text": "34.12 参考资料\n\nsurvminer包帮助文档"
  },
  {
    "objectID": "1034-finegray.html#加载数据和r包",
    "href": "1034-finegray.html#加载数据和r包",
    "title": "35  Fine-Gray检验和竞争风险模型列线图",
    "section": "35.1 加载数据和R包",
    "text": "35.1 加载数据和R包\n探讨骨髓移植和血液移植治疗白血病的疗效，结局事件定义为复发，某些患者因为移植不良反应死亡，定义为竞争风险事件。\n\nrm(list = ls())\ndata(\"bmtcrr\",package = \"casebase\")\nstr(bmtcrr)\n## 'data.frame':    177 obs. of  7 variables:\n##  $ Sex   : Factor w/ 2 levels \"F\",\"M\": 2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : Factor w/ 2 levels \"ALL\",\"AML\": 1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : Factor w/ 4 levels \"CR1\",\"CR2\",\"CR3\",..: 4 2 3 2 2 4 1 1 1 4 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Status: int  2 1 0 2 2 2 0 2 0 1 ...\n##  $ Source: Factor w/ 2 levels \"BM+PB\",\"PB\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ftime : num  0.67 9.5 131.77 24.03 1.47 ...\n\n这个数据一共7个变量，177行。\n\nSex: 性别，F是女，M是男\nD: 疾病类型，ALL是急性淋巴细胞白血病，AML是急性髓系细胞白血病。\nPhase: 不同阶段，4个水平，CR1，CR2，CR3，Relapse。\nAge: 年龄。\nStatus: 结局变量，0=删失，1=复发，2=竞争风险事件。\nSource: 因子变量，2个水平：BM+PB(骨髓移植+血液移植)，PB(血液移植)。\nftime: 生存时间。\n\n\n# 竞争风险分析需要用的R包\nlibrary(cmprsk)\n## Loading required package: survival"
  },
  {
    "objectID": "1034-finegray.html#fine-gray检验单因素分析",
    "href": "1034-finegray.html#fine-gray检验单因素分析",
    "title": "35  Fine-Gray检验和竞争风险模型列线图",
    "section": "35.2 Fine-Gray检验（单因素分析）",
    "text": "35.2 Fine-Gray检验（单因素分析）\n在普通的生存分析中，可以用log-rank检验做单因素分析，在竞争风险模型中，使用Fine-Gray检验进行单因素分析。\n\n\n\n\n\n比如现在我们想要比较不同疾病类型（D）有没有差异，可以进行Fine-Gray检验：\n\nbmtcrr$Status &lt;- factor(bmtcrr$Status)\nf &lt;- cuminc(bmtcrr$ftime, bmtcrr$Status, bmtcrr$D)\nf\n## Tests:\n##        stat         pv df\n## 1 2.8623325 0.09067592  1\n## 2 0.4481279 0.50322531  1\n## Estimates and Variances:\n## $est\n##              20        40        60        80       100       120\n## ALL 1 0.3713851 0.3875571 0.3875571 0.3875571 0.3875571 0.3875571\n## AML 1 0.2414530 0.2663827 0.2810390 0.2810390 0.2810390        NA\n## ALL 2 0.3698630 0.3860350 0.3860350 0.3860350 0.3860350 0.3860350\n## AML 2 0.4439103 0.4551473 0.4551473 0.4551473 0.4551473        NA\n## \n## $var\n##                20          40          60          80         100         120\n## ALL 1 0.003307032 0.003405375 0.003405375 0.003405375 0.003405375 0.003405375\n## AML 1 0.001801156 0.001995487 0.002130835 0.002130835 0.002130835          NA\n## ALL 2 0.003268852 0.003373130 0.003373130 0.003373130 0.003373130 0.003373130\n## AML 2 0.002430406 0.002460425 0.002460425 0.002460425 0.002460425          NA\n\n结果中1代表复发,2代表竞争风险事件。\n第一行统计量=2.8623325, P=0.09067592,表示在控制了竞争风险事件（即第二行计算的统计量和P值）后，两种疾病类型ALL和AML的累计复发风险无统计学差异P=0.09067592。\n第2行说明ALL和AML的累计竞争风险无统计学差异。\n$est表示估计的各时间点ALL和AML组的累计复发率与与累计竞争风险事件发生率（分别用1和2来区分，与第一行第二行一致）。\n$var表示估计的各时间点ALL和AML组的累计复发率与与累计竞争风险事件发生率的方差（分别用1和2来区分，与第一行第二行一致）。\n\n35.2.1 图形展示结果\n对于上述结果可以使用图形展示：\n\nplot(f,xlab = 'Month', ylab = 'CIF',lwd=2,lty=1,\n     col = c('red','blue','black','forestgreen'))\n\n\n\n\n图形解读：\n纵坐标表示累计发生率CIF，横坐标是时间。我们从ALL1对应的红色曲线和AML1对应的蓝色曲线可以得出，ALL组的复发风险较AML 组高，但无统计学意义，P=0.09067592。同理，ALL2对应的黑色曲线在AML2对应的草绿色曲线下方，我们可以得出，ALL组的竞争风险事件发生率较AML组低，同样无统计学意义，P=0.50322531。\n简单来讲，这个图可以用一句话来概括：在控制了竞争风险事件后，ALL和AML累计复发风险无统计学差异P=0.09067592。\n\n\n35.2.2 ggplot2\n这个图不好看，非常的不ggplot，所以我们要用ggplot2重新画它！所以首先要提取数据，因为数就是图，图就是数。但是万能的broom包竟然没有不能提取这个对象的数据，只能手动来，太不优雅了！\n\n# 提取数据\nALL1 &lt;- data.frame(ALL1_t = f[[1]][[1]], ALL1_C = f[[1]][[2]])\nAML1 &lt;- data.frame(AML1_t = f[[2]][[1]], AML1_C = f[[2]][[2]])\nALL2 &lt;- data.frame(ALL2_t = f[[3]][[1]], ALL2_C = f[[3]][[2]])\nAML2 &lt;- data.frame(AML2_t = f[[4]][[1]], AML2_C = f[[4]][[2]])\n\nlibrary(ggplot2)\n\nggplot()+\n  geom_line(data = ALL1, aes(ALL1_t,ALL1_C))+\n  geom_line(data = ALL2, aes(ALL2_t,ALL2_C))+\n  geom_line(data = AML1, aes(AML1_t,AML1_C))+\n  geom_line(data = AML2, aes(AML2_t,AML2_C))+\n  labs(x=\"month\",y=\"cif\")+\n  theme_bw()\n\n\n\n\n但是这种不好上色，所以我们美化一下，变成长数据再画图即可。\n\ntmp &lt;- data.frame(month = c(ALL1$ALL1_t,AML1$AML1_t,ALL2$ALL2_t,AML2$AML2_t),\n                  cif = c(ALL1$ALL1_C,AML1$AML1_C,ALL2$ALL2_C,AML2$AML2_C),\n                  type = rep(c(\"ALL1\",\"AML1\",\"ALL2\",\"AML2\"), c(58,58,58,88))\n                  )\n\nggplot(tmp, aes(month, cif))+\n  geom_line(aes(color=type, group=type),size=1.2)+\n  theme_bw()+\n  theme(legend.position = \"top\")\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead."
  },
  {
    "objectID": "1034-finegray.html#竞争风险模型多因素分析",
    "href": "1034-finegray.html#竞争风险模型多因素分析",
    "title": "35  Fine-Gray检验和竞争风险模型列线图",
    "section": "35.3 竞争风险模型（多因素分析）",
    "text": "35.3 竞争风险模型（多因素分析）\n做完了单因素分析，再看看竞争风险模型的多因素分析。\n首先要把自变量单独放在一个数据框里，使用中发现一个问题，这里如果把分类变量变为因子型不会自动进行哑变量编码，所以需要手动进行哑变量编码！\n但是我这里偷懒了，并没有进行哑变量设置！实际中是需要的哦！！\n\ncovs &lt;- subset(bmtcrr, select = - c(ftime,Status))\ncovs[,c(1:3,5)] &lt;- lapply(covs[,c(1:3,5)],as.integer)\n\nstr(covs)\n## 'data.frame':    177 obs. of  5 variables:\n##  $ Sex   : int  2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : int  1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : int  4 2 3 2 2 4 1 1 1 4 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Source: int  1 1 1 1 1 1 1 1 1 1 ...\n\n指定failcode=1, cencode=0, 分别代表结局事件1与截尾0，其他默认为竞争风险事件2。\n\n# 构建竞争风险模型\nf2 &lt;- crr(bmtcrr$ftime, bmtcrr$Status, covs, failcode=1, cencode=0)\nsummary(f2)\n## Competing Risks Regression\n## \n## Call:\n## crr(ftime = bmtcrr$ftime, fstatus = bmtcrr$Status, cov1 = covs, \n##     failcode = 1, cencode = 0)\n## \n##           coef exp(coef) se(coef)      z p-value\n## Sex     0.0494     1.051   0.2867  0.172 0.86000\n## D      -0.4860     0.615   0.3040 -1.599 0.11000\n## Phase   0.4144     1.514   0.1194  3.470 0.00052\n## Age    -0.0174     0.983   0.0118 -1.465 0.14000\n## Source  0.9526     2.592   0.5469  1.742 0.08200\n## \n##        exp(coef) exp(-coef)  2.5% 97.5%\n## Sex        1.051      0.952 0.599  1.84\n## D          0.615      1.626 0.339  1.12\n## Phase      1.514      0.661 1.198  1.91\n## Age        0.983      1.018 0.960  1.01\n## Source     2.592      0.386 0.888  7.57\n## \n## Num. cases = 177\n## Pseudo Log-likelihood = -267 \n## Pseudo likelihood ratio test = 23.6  on 5 df,\n\n结果解读：在控制了竞争分险事件后，phase变量，即疾病所处阶段是患者复发的独立影响因素(p =0.00052)。"
  },
  {
    "objectID": "1034-finegray.html#列线图",
    "href": "1034-finegray.html#列线图",
    "title": "35  Fine-Gray检验和竞争风险模型列线图",
    "section": "35.4 列线图",
    "text": "35.4 列线图\nregplot包绘制列线图。但是它目前只适用coxph()、lm()和glm()返回的对象。\n因此我们需要对原数据集加权创建一个新数据集用于为竞争风险模型分析，使用mstate包中的crprep()创建加权数据集,然后使用coxph()对加权数据集进行竞争风险模型拟合，这样就可以画列线图了。\n首先是加载数据和R包：\n\nrm(list = ls())\ndata(\"bmtcrr\",package = \"casebase\") # 还是这个数据\n\nlibrary(mstate) # 加权用到的R包\n\nbmtcrr$id &lt;- 1:nrow(bmtcrr) # 创建id\n\n# phase变为2分类，不然列线图不好解释\nbmtcrr$Phase &lt;- factor(ifelse(bmtcrr$Phase==\"Relapse\",1,0)) \nstr(bmtcrr)\n## 'data.frame':    177 obs. of  8 variables:\n##  $ Sex   : Factor w/ 2 levels \"F\",\"M\": 2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : Factor w/ 2 levels \"ALL\",\"AML\": 1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 1 2 1 1 1 2 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Status: int  2 1 0 2 2 2 0 2 0 1 ...\n##  $ Source: Factor w/ 2 levels \"BM+PB\",\"PB\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ftime : num  0.67 9.5 131.77 24.03 1.47 ...\n##  $ id    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n然后是对原数据进行加权：\n\ndf.w &lt;- crprep(\"ftime\", \"Status\",\n               data=bmtcrr, \n               trans=c(1,2),# 要加权的变量，1表示结局事件，2表示竞争风险事件\n               cens=0, # 删失\n               id=\"id\",\n               \n               # 要保留的协变量\n               keep=c(\"Age\",\"Sex\",\"D\",\"Source\",\"Phase\"))\n\nhead(df.w)\n##   id Tstart Tstop status weight.cens Age Sex   D Source Phase count failcode\n## 1  1   0.00  0.67      2   1.0000000  48   M ALL  BM+PB     1     1        1\n## 2  1   0.67  9.50      2   1.0000000  48   M ALL  BM+PB     1     2        1\n## 3  1   9.50 13.07      2   0.9679938  48   M ALL  BM+PB     1     3        1\n## 4  1  13.07 17.23      2   0.8730924  48   M ALL  BM+PB     1     4        1\n## 5  1  17.23 20.83      2   0.8536904  48   M ALL  BM+PB     1     5        1\n## 6  1  20.83 28.53      2   0.8120469  48   M ALL  BM+PB     1     6        1\ndf.w$T&lt;- df.w$Tstop - df.w$Tstart\n\n上述代码已经创建一个加权数据集df.w，此时还需要选择failcode == 1的行，然后我们才可以在此数据集上使用coxph()函数进行竞争风险分析，不然最后画列线图会报错。\n\n# 参考资料\n# https://blog.csdn.net/zhongkeyuanchongqing/article/details/124086113\ndf.w2 &lt;- df.w[df.w$failcode == 1,]\n\n构建cox模型：\n\nm.crr&lt;- coxph(Surv(T,status==1)~Age+Sex+D+Source+Phase,\n             data=df.w2,\n             weight=weight.cens,\n             subset=failcode==1)\nsummary(m.crr)\n## Call:\n## coxph(formula = Surv(T, status == 1) ~ Age + Sex + D + Source + \n##     Phase, data = df.w2, weights = weight.cens, subset = failcode == \n##     1)\n## \n##   n= 686, number of events= 56 \n## \n##              coef exp(coef) se(coef) robust se      z Pr(&gt;|z|)    \n## Age      -0.02174   0.97850  0.01172   0.01208 -1.800 0.071914 .  \n## SexM      0.10551   1.11128  0.27981   0.29571  0.357 0.721247    \n## DAML     -0.53163   0.58764  0.29917   0.30613 -1.737 0.082450 .  \n## SourcePB  1.06564   2.90269  0.53453   0.56000  1.903 0.057051 .  \n## Phase1    1.06140   2.89040  0.27870   0.28129  3.773 0.000161 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##          exp(coef) exp(-coef) lower .95 upper .95\n## Age         0.9785     1.0220    0.9556     1.002\n## SexM        1.1113     0.8999    0.6225     1.984\n## DAML        0.5876     1.7017    0.3225     1.071\n## SourcePB    2.9027     0.3445    0.9686     8.699\n## Phase1      2.8904     0.3460    1.6654     5.016\n## \n## Concordance= 0.737  (se = 0.037 )\n## Likelihood ratio test= 28.33  on 5 df,   p=3e-05\n## Wald test            = 27.27  on 5 df,   p=5e-05\n## Score (logrank) test = 30.49  on 5 df,   p=1e-05,   Robust = 20.2  p=0.001\n## \n##   (Note: the likelihood ratio and score tests assume independence of\n##      observations within a cluster, the Wald and robust score tests do not).\n\n接下来，我们可以使用regplot()函数绘制nomogram。其实你可以绘制多种不同的列线图，可以参考之前的推文：生存资料列线图的4种绘制方法\n\nlibrary(regplot)\nregplot(m.crr,\n        observation=df.w2[df.w2$id==25&df.w2$failcode==1,],\n        failtime = c(36, 60), \n        prfail = T, \n        droplines=T)\n\n## \"observation\" has &gt;1 row. The first  row provides plotted values\n## Regression  m.crr coxph formula:\n## Surv(T, status == 1) `~` Age + Sex + D + Source + Phase\n## Replicate integer weights assumed\n## Note: non-integer weights have been floored\n## [1] \"note: points tables not constructed unless points=TRUE \"\n\n\n在这个列线图中，将数据集中id=25的患者各协变量的取值映射到相应的得分，并计算总得分,并分别计算其在36个月和60个月的累计复发概率，此概率即为控制了竞争风险的累计复发概率，分别为：0.134和0.146。"
  },
  {
    "objectID": "1035-psm.html#准备数据",
    "href": "1035-psm.html#准备数据",
    "title": "36  R语言倾向性评分：匹配",
    "section": "36.1 准备数据",
    "text": "36.1 准备数据\n下面的数据及演示的方法主要参考了这篇文章：10.21037/atm-20-3998。大家感兴趣的可以去阅读原文。\n我们虚构一个数据，用于演示研究吸烟对心血管疾病的影响，性别和年龄作为混杂因素。\n\nCVD：结果变量，1是有心血管疾病，0是没有\nx.Age：年龄\nx.Gender：0是女，1是男\nSmoke：1吸烟，0不吸烟，是我们的处理因素\n\n\nset.seed(2020)\n\nx.Gender &lt;- rep(0:1, c(400,600)) # 400女 600男\nx.Age &lt;- round(abs(rnorm(1000, mean=45, sd=15)))\n\n# 对于这个数据来说，实际PS（tps）是可以计算出来的，如果这里不理解，也问题不大！\nz &lt;- (x.Age - 45) / 15 - (x.Age-45) ^ 2 / 100 + 2 * x.Gender\ntps &lt;- exp(z) / (1+exp(z)) \nSmoke &lt;- as.numeric(runif(1000) &lt; tps)\nz.y &lt;- x.Gender + 0.3*x.Age + 5*Smoke - 20\ny &lt;- exp(z.y) / (1+exp(z.y))\nCVD &lt;- as.numeric(runif(1000) &lt; y)\nx.Age.mask &lt;- rbinom(1000, 1, 0.2) # 随机产生几个缺失值\nx.Age &lt;- ifelse(x.Age.mask==1, NA, x.Age)\n\n# 原始数据长这样：\ndata &lt;- data.frame(x.Age, x.Gender, Smoke, CVD)\nhead(data)\n##   x.Age x.Gender Smoke CVD\n## 1    51        0     1   0\n## 2    50        0     0   0\n## 3    29        0     0   0\n## 4    28        0     0   0\n## 5     3        0     0   0\n## 6    56        0     1   1\n\n首先可以看一下原始数据的基线资料表，用的是tableone这个包，之前也做过介绍，做基线资料表的R包还有非常多，比如：\n\n使用compareGroups包1行代码生成基线资料表\n使用R语言快速绘制三线表\ntableone？table1？傻傻分不清楚\n超强的gtSummary ≈ gt + comparegroups ??\n\n为什么用tableone呢？因为它能计算SMD（后面会介绍这个SMD的作用），而且其他教程都是用的它…\n\nlibrary(tableone)\n\ntable2 &lt;- CreateTableOne(vars = c('x.Age', 'x.Gender', 'CVD'),\n                         data = data,\n                         factorVars = c('x.Gender', 'CVD'),\n                         strata = 'Smoke',\n                         smd=TRUE)\ntable2 &lt;- print(table2,smd=TRUE,\n                showAllLevels = TRUE,\n                noSpaces = TRUE,\n                printToggle = FALSE)\ntable2\n##                    Stratified by Smoke\n##                     level 0               1              p        test SMD    \n##   n                 \"\"    \"549\"           \"451\"          \"\"       \"\"   \"\"     \n##   x.Age (mean (SD)) \"\"    \"42.76 (19.69)\" \"47.04 (8.14)\" \"&lt;0.001\" \"\"   \"0.284\"\n##   x.Gender (%)      \"0\"   \"299 (54.5)\"    \"101 (22.4)\"   \"&lt;0.001\" \"\"   \"0.698\"\n##                     \"1\"   \"250 (45.5)\"    \"350 (77.6)\"   \"\"       \"\"   \"\"     \n##   CVD (%)           \"0\"   \"452 (82.3)\"    \"230 (51.0)\"   \"&lt;0.001\" \"\"   \"0.705\"\n##                     \"1\"   \"97 (17.7)\"     \"221 (49.0)\"   \"\"       \"\"   \"\"\n#write.csv(table2, file = \"Table2_before_matching.csv\")\n\n结果中可以看出x.Age和x.Gender两个变量在两组间是有差异的，其中SMD(standardized mean differences)可以用来衡量协变量在不同组间的差异；除此之外，这两个变量的P值在不同性别间也是小于0.001的，说明不同性别间这两个变量是有明显差别的。\n如果此时直接探讨是否吸烟对CVD的影响，很有可能会得到错误的答案，经典的辛普森悖论就是由于混杂因素的存在才导致出现神奇的结果（比如有种药对男人有效，对女人也有效，但是对全人类就没效了！）。\n所以要想办法解决x.Age和x.Gender两个变量在两组间的差异，达到基线可比的目的。今天要介绍的方法就是倾向性评分匹配。"
  },
  {
    "objectID": "1035-psm.html#matchit包进行psm",
    "href": "1035-psm.html#matchit包进行psm",
    "title": "36  R语言倾向性评分：匹配",
    "section": "36.2 matchIt包进行PSM",
    "text": "36.2 matchIt包进行PSM\nmatchIt包支持非常多计算PS的方法，比如自带的logistic回归、广义可加模型、分类和回归树、神经网络，除了自带的方法，也支持其他方法计算的PS。这些方法通过distance参数指定：\n\ndistance:指定PS的计算方法，默认是logit，即logistic回归，GAMlogit（广义可加模型），rpart（决策树）,nnet(神经网络)，除此之外，也可以是使用其他包或方法计算的PS值！\ndistance.options:当你选择好了方法之后，不同的方法会有不同的额外选项。\n\n下面演示使用logistic回归的方法计算PS，这里我们的处理因素是二分类变量(是否吸烟)，可以通过逻辑回归计算这些协变量（也就是混杂因素）的P值，这个P值就是倾向性评分。倾向性评分就是P值！（有网友指出这样说不对，应该是参数pi表示事件的概率）\n\nlibrary(MatchIt)\n\n# 这里为了方便演示直接删掉了缺失值\ndata.complete &lt;- na.omit(data)\n\n# 因变量是处理因素，自变量是需要平衡的协变量\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\n                 data = data.complete,\n                 distance = \"logit\" # 选择logistic回归\n                 )\n\nm.out\n## A matchit object\n##  - method: 1:1 nearest neighbor matching without replacement\n##  - distance: Propensity score\n##              - estimated with logistic regression\n##  - number of obs.: 831 (original), 738 (matched)\n##  - target estimand: ATT\n##  - covariates: x.Age, x.Gender\n\n上面是一个简单的结果，告诉你匹配方法是1:1无放回最近邻匹配，计算方法是logistic回归，匹配了738例等信息。\n可通过以下方法获得算法估计的PS值：\n\neps &lt;- m.out$distance\nlength(eps)\n## [1] 831\nhead(eps)\n##         1         2         3         4         5         6 \n## 0.2583040 0.2545807 0.1847661 0.1818430 0.1200378 0.2774451\n\n一开始我们已经计算出了实际PS值（tps），所以我们可以画一个tps和估计ps的散点图，以tps为横坐标，以eps为纵坐标：\n\nlibrary(ggplot2)\n## Warning: package 'ggplot2' was built under R version 4.2.3\n\n# 去掉缺失值\ntps.comp &lt;- tps[complete.cases(data)]\nSmoke.comp &lt;- as.factor(Smoke[complete.cases(data)])\ndf &lt;- data.frame(True=tps.comp, Estimated=eps, Smoke=Smoke.comp)\n\nggplot(df, aes(x=True, y=Estimated, colour=Smoke)) +\ngeom_point() +\ngeom_abline(intercept=0,slope=1, colour=\"#990000\", linetype=\"dashed\") +\nexpand_limits(x=c(0,1),y=c(0,1))\n\n\n\n\n可以看到拟合结果非常烂！因为一开始计算tps时用了平方（二次项），但是使用logistic估计ps时并没有用平方。\n我们把公式也变成平方即可，此时再画一个拟合图就完美一致了！如下所示：\n\n# 对x.Age平方\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age +x.Gender,\n                 data=data.complete) \n\neps &lt;- m.out$distance\n\ntps.comp &lt;- tps[complete.cases(data)]\nSmoke.comp &lt;- as.factor(Smoke[complete.cases(data)])\ndf &lt;- data.frame(True=tps.comp, Estimated=eps, Smoke=Smoke.comp)\nggplot(df, aes(x=True, y=Estimated, colour=Smoke)) +\ngeom_point() +\ngeom_abline(intercept=0,slope=1, colour=\"#990000\", linetype=\"dashed\") +\nexpand_limits(x=c(0,1),y=c(0,1))\n\n\n\n\n此时的PS是通过logistic回归计算的，既然PS就是P值，当然你完全可以用glm自己计算，通过以下方法：\n\ntmp &lt;- glm(Smoke~I(x.Age^2)+x.Age +x.Gender, data=data.complete,\n           family = binomial())\n\ntmp.df &lt;- data.frame(estimated = tmp$fitted.values,\n                     true = tps.comp,\n                     Smoke=Smoke.comp)\n\nggplot(tmp.df, aes(true, estimated))+\n  geom_point(aes(color=Smoke))+\n  geom_abline(intercept=0,slope=1, colour=\"#990000\", linetype=\"dashed\") +\n  expand_limits(x=c(0,1),y=c(0,1))\n\n\n\n\n看这个结果和matchit得到的结果完全一样！\n改变matchit()的参数即可使用不同的算法估计PS，比如下面是分类和回归树及神经网络方法：\n\n# cart\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\ndata=data.complete,\ndistance='rpart')\n\n# nnet\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\ndata=data.complete,\ndistance='nnet',\ndistance.options=list(size=16))\n\n默认的只有4种方法，但是完全可以自己通过其他方法计算PS，然后提供给distance参数即可！非常强大！\n\n36.2.1 使用随机森林计算PS\n默认没提供随机森林的算法，我们可以通过其他R包计算，反正PS就是P值，只要拿到P值就可以了！\n\n# 使用随机森林构建模型\nlibrary(randomForest)\n## randomForest 4.7-1.1\n## Type rfNews() to see new features/changes/bug fixes.\n## \n## Attaching package: 'randomForest'\n## The following object is masked from 'package:ggplot2':\n## \n##     margin\ndata.complete$Smoke &lt;- factor(data.complete$Smoke)\n\nrf.out &lt;- randomForest(Smoke~x.Age+x.Gender, data=data.complete)\nrf.out\n## \n## Call:\n##  randomForest(formula = Smoke ~ x.Age + x.Gender, data = data.complete) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 1\n## \n##         OOB estimate of  error rate: 22.5%\n## Confusion matrix:\n##     0   1 class.error\n## 0 360 102   0.2207792\n## 1  85 284   0.2303523\n\n从随机森林结果中提取预测类别为1（有CVD）的概率：\n\neps &lt;- rf.out$votes[,2] # Estimated PS\n\n接下来只要把这个eps提供给distance参数即可：\n\nmatchit(formula=Smoke~x.Age+x.Gender,\n        data=data.complete,\n        distance=eps, # 自己估计的eps\n        method='nearest',\n        replace=TRUE,\n        discard='both',\n        ratio=2)\n## A matchit object\n##  - method: 2:1 nearest neighbor matching with replacement\n##  - distance: User-defined [common support]\n##  - common support: units from both groups dropped\n##  - number of obs.: 831 (original), 520 (matched)\n##  - target estimand: ATT\n##  - covariates: x.Age, x.Gender\n\n其他方法也是同理，只需要提供P值即可，但并不是越复杂的方法效果越好哦！不信的话可以把几种方法得到的eps都画一个散点拟合图看看效果，这个数据是逻辑回归最好哈！"
  },
  {
    "objectID": "1035-psm.html#主要匹配方法选择",
    "href": "1035-psm.html#主要匹配方法选择",
    "title": "36  R语言倾向性评分：匹配",
    "section": "36.3 主要匹配方法选择",
    "text": "36.3 主要匹配方法选择\n在确定了使用哪种算法计算PS后，匹配方法也是需要注意的一个问题，需要注意以下几个方面，首先是匹配方法的选择（method），然后是采样手段（有无放回），相似度的度量（卡钳值或其他），匹配比例（1:1或1：多）。\n\nmethod:\n\n默认的匹配方法是最近邻匹配nearest，其他方法还有\n“exact” (exact matching),\n“full” (optimal full matching),\n“optimal” (optimal pair matching),\n“cardinality”(cardinality and template matching),\n“subclass” (subclassification),\n“genetic” (genetic matching),\n“cem” (coarsened exact matching)\n\n\n每个匹配方法都提供了详细的解释，大家感兴趣的自己查看即可。\n\ncaliper:卡钳值，也就是配对标准，两组的概率值（PS）差距在这个标准内才会配对。这里的卡钳值是PS标准差的倍数，默认是不设置卡钳值。还有一个std.caliper参数，默认是TRUE，如果设置FALSE，你设置的卡钳值就直接是PS的倍数。\nreplace:能否重复匹配，默认是FALSE，意思是假如干预组的1号匹配到了对照组的A，那A就不能再和其他的干预组进行匹配了。\nratio:设置匹配比例，干预组:对照组到底是1比几，默认为1:1。ratio=2即是干预组：对照组是1:2。所以一般要求数据的对照组数量多于干预组才行。如果对照组比干预组多出很多，完全可以设置1:n进行匹配，这样还能损失更少的样本信息，但是一般也不会超过1:4。\nreestimate:如果是TRUE，丢掉没匹配上的样本，PS会使用剩下的样本重新计算PS，如果是FALSE或者不写就不会重新计算PS。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n比如下面是一个有放回的，1:2的，最近邻匹配：\n\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 replace=TRUE,\n                 ratio=2)\n\n可以通过m.out$match.matrix获取配好的对子：\n\nhead(m.out$match.matrix)\n##    [,1]  [,2] \n## 1  \"204\" \"23\" \n## 6  \"283\" \"163\"\n## 10 \"56\"  \"30\" \n## 12 \"41\"  \"28\" \n## 20 \"79\"  \"38\" \n## 26 \"84\"  \"70\"\n\n第一列是干预组的序号，第二列是和干预组配好对的，对照组的序号。\nm.out$discarded查看某个样本是否被丢弃：\n\ntable(m.out$discarded)\n## \n## FALSE \n##   831"
  },
  {
    "objectID": "1035-psm.html#匹配后数据的平衡性检验",
    "href": "1035-psm.html#匹配后数据的平衡性检验",
    "title": "36  R语言倾向性评分：匹配",
    "section": "36.4 匹配后数据的平衡性检验",
    "text": "36.4 匹配后数据的平衡性检验\n检查匹配后的数据，主要是看协变量在不同组间是否已经均衡了（是不是没有差异了）。\n关于这个倾向性评分匹配后数据的平衡性检验，文献中比较推荐使用SMD和VR(variance ratio)，SMD&lt;0.25说明均衡了，VR&gt;2.0或者VR&lt;0.5说明很不均衡（越接近1越均衡）！\n但其实也可以用假设检验，比如t检验、卡方检验等，也是没有统一的标准！\n做一个1:1无放回的最近邻匹配：\n\nm.out &lt;- matchit(Smoke~x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 replace=FALSE,\n                 ratio = 1)\n\n通过summary()查看匹配前后，不同组间协变量的各种统计量。通常建议选择standardize = TRUE查看标准后的各协变量的平衡性指标：\n\nsummary(m.out,standardize = TRUE)\n## \n## Call:\n## matchit(formula = Smoke ~ x.Age + x.Gender, data = data.complete, \n##     method = \"nearest\", distance = \"logit\", replace = FALSE, \n##     ratio = 1)\n## \n## Summary of Balance for All Data:\n##          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance        0.5170        0.3858          0.9050     0.6205    0.2069\n## x.Age          47.0352       42.7619          0.5247     0.1711    0.1306\n## x.Gender        0.7832        0.4502          0.8081          .    0.3330\n##          eCDF Max\n## distance   0.4833\n## x.Age      0.3629\n## x.Gender   0.3330\n## \n## Summary of Balance for Matched Data:\n##          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance        0.5170        0.4380          0.5448     0.7605    0.1298\n## x.Age          47.0352       47.3035         -0.0329     0.1908    0.1072\n## x.Gender        0.7832        0.5610          0.5393          .    0.2222\n##          eCDF Max Std. Pair Dist.\n## distance   0.4255          0.7108\n## x.Age      0.2439          2.2436\n## x.Gender   0.2222          0.5393\n## \n## Sample Sizes:\n##           Control Treated\n## All           462     369\n## Matched       369     369\n## Unmatched      93       0\n## Discarded       0       0\n\n结果主要是3个部分：\n\nSummary of Balance for All Data：原始数据中干预组和对照组的平均PS值和平均协变量，SMD,VR，每个协变量和PS的CDF（cumulative distribution functions）的均值和最大值\nSummary of Balance for Matched Data：匹配后数据的指标\nSample Sizes：样本数量\n\n通过观察比较匹配前后的数据指标可知，x.Age均衡了（0.0329&lt;0.1），但是x.Gender并没有均衡(0.5393&gt;0.1)！\n这个默认的函数在计算SMD的时候会把分类变量按照连续性变量进行计算，所以计算结果是有一些问题的。在一开始计算匹配前数据的SMD时我们用的是tableone这个包，匹配后数据的SMD理论上也是可以用这个包的：\n\n# 首先提取匹配后的数据\nmdata &lt;- match.data(m.out)\n\nlibrary(tableone)\n\ntable5 &lt;- CreateTableOne(vars = c('x.Age', 'x.Gender', 'CVD'),\n                         data = mdata,\n                         factorVars = c('x.Gender', 'CVD'),\n                         strata = 'Smoke',\n                         smd=TRUE)\ntable5 &lt;- print(table5, smd=TRUE, showAllLevels = TRUE, \n                noSpaces = TRUE, printToggle = FALSE)\ntable5\n##                    Stratified by Smoke\n##                     level 0               1              p        test SMD    \n##   n                 \"\"    \"369\"           \"369\"          \"\"       \"\"   \"\"     \n##   x.Age (mean (SD)) \"\"    \"47.30 (18.65)\" \"47.04 (8.14)\" \"0.800\"  \"\"   \"0.019\"\n##   x.Gender (%)      \"0\"   \"162 (43.9)\"    \"80 (21.7)\"    \"&lt;0.001\" \"\"   \"0.487\"\n##                     \"1\"   \"207 (56.1)\"    \"289 (78.3)\"   \"\"       \"\"   \"\"     \n##   CVD (%)           \"0\"   \"287 (77.8)\"    \"190 (51.5)\"   \"&lt;0.001\" \"\"   \"0.572\"\n##                     \"1\"   \"82 (22.2)\"     \"179 (48.5)\"   \"\"       \"\"   \"\"\n\n这个tableone计算的x.Gender的SMD是0.487，也是表明这个变量并没有被均衡。\n但是tableone这个包计算的SMD也是有一些问题的，具体原因大家自己读文献吧：Zhang Z, Kim HJ, Lonjon G, et al. Balance diagnostics after propensity score matching. Ann Transl Med 2019;7:16.\n所以推荐大家使用cobalt包进行平衡性指标的计算。\n\n36.4.1 cobalt包\n使用cobalt包进行平衡性指标的计算，这个包很专业，专门处理这类匹配问题的，大家可以去它的官网学习更多的细节！\n\nlibrary(cobalt)\n##  cobalt (Version 4.4.1, Build Date: 2022-11-03)\n## \n## Attaching package: 'cobalt'\n## The following object is masked from 'package:MatchIt':\n## \n##     lalonde\n\n# m.threshold表示SMD的阈值，小于这个阈值的协变量是平衡的\nbal.tab(m.out, m.threshold = 0.1, un = TRUE)\n## Balance Measures\n##              Type Diff.Un Diff.Adj        M.Threshold\n## distance Distance  0.9050   0.5448                   \n## x.Age     Contin.  0.5247  -0.0329     Balanced, &lt;0.1\n## x.Gender   Binary  0.3330   0.2222 Not Balanced, &gt;0.1\n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1         1\n## Not Balanced, &gt;0.1     1\n## \n## Variable with the greatest mean difference\n##  Variable Diff.Adj        M.Threshold\n##  x.Gender   0.2222 Not Balanced, &gt;0.1\n## \n## Sample sizes\n##           Control Treated\n## All           462     369\n## Matched       369     369\n## Unmatched      93       0\n\nx.Age的SMD和默认是一样的，但是x.Gender是0.2222，比默认的小多了！\n这个结果比默认的结果更可靠，具体原因大家自己去读上面那篇文献。\n计算VR，结果中并没有计算x.Gender的VR，而且根据VR来看，x.Age也没有均衡。\n\nbal.tab(m.out, v.threshold = 2)\n## Balance Measures\n##              Type Diff.Adj V.Ratio.Adj      V.Threshold\n## distance Distance   0.5448      0.7605     Balanced, &lt;2\n## x.Age     Contin.  -0.0329      0.1908 Not Balanced, &gt;2\n## x.Gender   Binary   0.2222           .                 \n## \n## Balance tally for variance ratios\n##                  count\n## Balanced, &lt;2         1\n## Not Balanced, &gt;2     1\n## \n## Variable with the greatest variance ratio\n##  Variable V.Ratio.Adj      V.Threshold\n##     x.Age      0.1908 Not Balanced, &gt;2\n## \n## Sample sizes\n##           Control Treated\n## All           462     369\n## Matched       369     369\n## Unmatched      93       0\n\n\n\n36.4.2 统计检验衡量均衡性\n除了SMD和VR之外，传统的统计检验也可以用于检查匹配后的数据有没有均衡！\n首先取出匹配好的数据：\n\nmdata &lt;- match.data(m.out)\nhead(mdata)\n##    x.Age x.Gender Smoke CVD  distance weights subclass\n## 1     51        0     1   0 0.2583040       1        1\n## 2     50        0     0   0 0.2545807       1        2\n## 6     56        0     1   1 0.2774451       1      229\n## 9     71        0     0   1 0.3397803       1      142\n## 10    47        0     1   1 0.2436248       1      345\n## 12    59        0     1   1 0.2893402       1        2\n\n其中distance是估计的PS，weights是权重，因为我们用的是1:1无放回匹配，所以全都是1。\n下面用t检验看看匹配后干预组和对照组的x.Age有没有差异：\n\nt.test(x.Age ~ Smoke, data = mdata)\n## \n##  Welch Two Sample t-test\n## \n## data:  x.Age by Smoke\n## t = 0.25327, df = 503.47, p-value = 0.8002\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -1.812969  2.349555\n## sample estimates:\n## mean in group 0 mean in group 1 \n##        47.30352        47.03523\n\n结果也是显示x.Age已经没有差异了！\n然后用卡方检验看看x.Gender是否还有差异：\n\nchisq.test(mdata$x.Gender, mdata$Smoke,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  mdata$x.Gender and mdata$Smoke\n## X-squared = 41.342, df = 1, p-value = 1.278e-10\n\n结果显示x.Gender还是有差异的，这个结果也和SMD的判断结果相同。"
  },
  {
    "objectID": "1035-psm.html#结果可视化",
    "href": "1035-psm.html#结果可视化",
    "title": "36  R语言倾向性评分：匹配",
    "section": "36.5 结果可视化",
    "text": "36.5 结果可视化\n默认提供3种图形，但是美观性太差，就不放图了，大家感兴趣的可以自己试试看。\n\nplot(m.out) # 默认QQ图\nplot(m.out, type = 'jitter') # 散点图\nplot(m.out, type = 'hist') # 直方图\n\n默认的不好看，还是用cobalt包进行结果的可视化。\n\ncowplot::plot_grid(\n  bal.plot(m.out, var.name = 'x.Age', which = 'both', grid=TRUE),\n  bal.plot(m.out, var.name = 'x.Gender', which = 'both', grid=TRUE),\n  bal.plot(m.out, var.name = 'x.Age', which = 'both', grid=TRUE, type=\"ecdf\"),\n  # 还有很多参数可调整\n  love.plot(bal.tab(m.out, m.threshold=0.1),\n            stat = \"mean.diffs\",\n            grid=TRUE,\n            stars=\"raw\",\n            abs = F)\n  )\n## Warning: The following aesthetics were dropped during statistical transformation: weight\n## ℹ This can happen when ggplot fails to infer the correct grouping structure in\n##   the data.\n## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n##   variable into a factor?\n\n\n\n\n上面两幅图展示的是协变量在匹配前（unadjusted sample）和匹配后（adjusted sample）的数据中的分布情况，连续型变量默认是画密度图，分类变量默认是画柱状图。\n左下图是累计密度图。右下的love plot图可视化匹配前后协变量的SMD，两条竖线是0.1阈值线，匹配后x.Age在两条竖线之间，说明平衡，x.Gender不在两条竖线之间，说明还是没平衡。"
  },
  {
    "objectID": "1035-psm.html#不平衡怎么办",
    "href": "1035-psm.html#不平衡怎么办",
    "title": "36  R语言倾向性评分：匹配",
    "section": "36.6 不平衡怎么办？",
    "text": "36.6 不平衡怎么办？\n比如这里的x.Gender这个变量就是不平衡的。\n有非常多的方法可以尝试，这里提供5种方法，但是非常有可能你各种方法都试过了还是不平衡！\n首先可以换一种计算PS的方法，可以换算法，换公式（增加二次项、交互项等）。\n\n# 增加二次项，结果依然不平衡\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 replace=FALSE,\n                 ratio=1)\n\n第二可以换匹配方法及对应的参数。\n\n# 还是不平衡\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='genetic',\n                 pop.size=100)\nbal.tab(m.out, m.threshold=0.1)\n\n第三，可以使用精确匹配，性别不平衡，那就在匹配时要求按照性别精确匹配，可以使用参数exact=c('x.Gender')。\n但是这样做的代价是大部分样本都浪费了！只有一小部分才能匹配上！\n\nm.out &lt;- matchit(Smoke~I(x.Age^2)+x.Age+x.Gender,\n                 data=data.complete,\n                 distance='logit',\n                 method='nearest',\n                 exact = c('x.Gender','x.Age'), # 精准！\n                 replace=FALSE,\n                 ratio=1)\n## Warning: Fewer control units than treated units in some 'exact' strata; not all\n## treated units will get a match.\nbal.tab(m.out, m.threshold=0.1)\n## Balance Measures\n##                Type Diff.Adj    M.Threshold\n## distance   Distance        0 Balanced, &lt;0.1\n## I(x.Age^2)  Contin.        0 Balanced, &lt;0.1\n## x.Age       Contin.        0 Balanced, &lt;0.1\n## x.Gender     Binary        0 Balanced, &lt;0.1\n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1         4\n## Not Balanced, &gt;0.1     0\n## \n## Variable with the greatest mean difference\n##    Variable Diff.Adj    M.Threshold\n##  I(x.Age^2)        0 Balanced, &lt;0.1\n## \n## Sample sizes\n##           Control Treated\n## All           462     369\n## Matched       147     147\n## Unmatched     315     222\n\n第四，增加样本量（一切误差问题都可以通过增加样本量解决）。\n第五，匹配后结合其他方法，比如回归、分层等。"
  },
  {
    "objectID": "1035-psm.html#其他问题",
    "href": "1035-psm.html#其他问题",
    "title": "36  R语言倾向性评分：匹配",
    "section": "36.7 其他问题",
    "text": "36.7 其他问题\n这篇推文关于倾向性评分匹配说的还算详细，尤其是matchIt包的使用，但大部分都是基于开头说的那篇文献。\n除此之外，关于倾向性评分，还有一些很重要的问题并没有涉及到。比如：\n\n样本权重不同，匹配后数据如何检查平衡性？\n倾向性评分只能平衡记录到的协变量，对于潜在的、未被记录的误差不能平衡，怎么办？\n处理因素多分组或者是连续型变量时如何处理？\n倾向性评分的加权、回归、分层如何做？\n\n这些问题待以后有时间慢慢解决！"
  },
  {
    "objectID": "1035-psm.html#参考资料",
    "href": "1035-psm.html#参考资料",
    "title": "36  R语言倾向性评分：匹配",
    "section": "36.8 参考资料",
    "text": "36.8 参考资料\n\nhttps://zhuanlan.zhihu.com/p/386501046\nhttps://mp.weixin.qq.com/s/ITWBruRe5LhuPq8TXjxPZQ\nhttps://zhuanlan.zhihu.com/p/559469895\nPropensity score matching with R: conventional methods and new features"
  },
  {
    "objectID": "1036-pssc.html#演示数据",
    "href": "1036-pssc.html#演示数据",
    "title": "37  R语言倾向性评分：回归和分层",
    "section": "37.1 演示数据",
    "text": "37.1 演示数据\n下面这个例子探讨不同学校对学生成绩的影响，这个数据一共有11078行，23列，我们只用其中一部分数据演示倾向性评分回归和分层。\n我们用到以下几个变量：\n\ncatholic：是我们的处理因素，1是天主教（catholic）学校，0是公立（public）学校，\nc5r2mtsc_std：结果变量（因变量），标准化之后的学生成绩，\nrace_white：是否是白人，1是0否，\nw3momed_hsb：妈妈的教育水平，1高中及以下，0大学及以上，\np5hmage：妈妈的年龄，要控制的混杂因素，\nw3momscr：妈妈的成绩，\nw3dadscr：爸爸的成绩。\n\n首先加载数据，已上传到QQ群，需要的加群下载即可。\n\nlibrary(tidyverse)\n\necls &lt;- read.csv(\"datasets/ecls.csv\") %&gt;% \n  dplyr::select(c5r2mtsc_std,catholic,race_white,w3momed_hsb,p5hmage,\n                w3momscr,w3dadscr) %&gt;%\n  na.omit()\n\ndim(ecls)\n## [1] 5548    7\nglimpse(ecls)\n## Rows: 5,548\n## Columns: 7\n## $ c5r2mtsc_std &lt;dbl&gt; 0.98175332, 0.59437751, 0.49061062, 1.45127793, 2.5956991…\n## $ catholic     &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ race_white   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, …\n## $ w3momed_hsb  &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, …\n## $ p5hmage      &lt;int&gt; 47, 41, 43, 38, 47, 41, 31, 38, 26, 38, 27, 40, 33, 36, 4…\n## $ w3momscr     &lt;dbl&gt; 53.50, 34.95, 63.43, 53.50, 61.56, 38.18, 34.95, 63.43, 3…\n## $ w3dadscr     &lt;dbl&gt; 77.50, 53.50, 53.50, 53.50, 77.50, 53.50, 29.60, 33.42, 2…"
  },
  {
    "objectID": "1036-pssc.html#原始数据的概况",
    "href": "1036-pssc.html#原始数据的概况",
    "title": "37  R语言倾向性评分：回归和分层",
    "section": "37.2 原始数据的概况",
    "text": "37.2 原始数据的概况\n首先看一下原始数据的情况。\n\necls %&gt;%\n  group_by(catholic) %&gt;%\n  summarise(n_students = n(),\n            mean_math = mean(c5r2mtsc_std),\n            std_error = sd(c5r2mtsc_std) / sqrt(n_students))\n## # A tibble: 2 × 4\n##   catholic n_students mean_math std_error\n##      &lt;int&gt;      &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1        0       4597     0.156    0.0144\n## 2        1        951     0.221    0.0277\n\n可以看到去公立学校的4597人，去天主教学校的才951人，并且去天主教的学校的学生成绩明显高于去公立学校的学生。\n此时如果不控制混杂因素直接进行t检验，结果是有统计学意义的，但是由于基线资料不可比，一开始两组学生的各种情况就不一样，所以结果很难说明成绩不同到底是不同学校导致的还是混杂因素导致的。\n\nwith(ecls, t.test(c5r2mtsc_std ~ catholic))\n## \n##  Welch Two Sample t-test\n## \n## data:  c5r2mtsc_std by catholic\n## t = -2.0757, df = 1508.1, p-value = 0.03809\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -0.126029105 -0.003564746\n## sample estimates:\n## mean in group 0 mean in group 1 \n##       0.1562757       0.2210727\n\n我们可以看看不同组别间混杂因素的差异，首先是3个连续型变量在两组间的平均值，可以看到都是不一样的：\n\necls %&gt;%\n  group_by(catholic) %&gt;%\n  select(p5hmage, w3momscr, w3dadscr) %&gt;%\n  summarise_all(list(~mean(., na.rm = T)))\n## Adding missing grouping variables: `catholic`\n## # A tibble: 2 × 4\n##   catholic p5hmage w3momscr w3dadscr\n##      &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n## 1        0    37.8     43.8     42.6\n## 2        1    39.8     47.5     45.8\n\n可以看到不同组别间混杂因素明显是不同的，还可以分别对3个连续型变量做t检验，结果也显示这些混杂因素在一开始就是存在差异的。\n\necls %&gt;% \n  pivot_longer(cols = c(p5hmage,w3momscr,w3dadscr),\n               names_to = \"covs\",\n               values_to = \"values\"\n               ) %&gt;% \n  group_split(covs) %&gt;% \n  map(~t.test(values ~ catholic, data = .x)) %&gt;% \n  map_dbl(\"p.value\")\n## [1] 1.062659e-28 3.722314e-16 2.208513e-18\n\n对于两个分类变量，我们可以看看分别在两组间的数量构成比有没有差异。\n\ntab &lt;- xtabs(~race_white+catholic,data = ecls)\ntab\n##           catholic\n## race_white    0    1\n##          0 1610  222\n##          1 2987  729\nchisq.test(tab,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  tab\n## X-squared = 48.596, df = 1, p-value = 3.145e-12\n\n\ntab &lt;- xtabs(~w3momed_hsb+catholic,data = ecls)\ntab\n##            catholic\n## w3momed_hsb    0    1\n##           0 2777  751\n##           1 1820  200\nchisq.test(tab,correct = F)\n## \n##  Pearson's Chi-squared test\n## \n## data:  tab\n## X-squared = 117.24, df = 1, p-value &lt; 2.2e-16\n\n可以看到两个分类变量在两组间的差异是非常明显的！\n所以我们现在要做的事就是控制混杂因素，让这些混杂因素变成可比的状态，不要影响我们的处理因素。\n开头也说过，控制混杂因素的方法其实是很多的，比如分层、协方差分析、多因素分析等，每种情况都要具体分析，选择一种最合适的。\n下面我们介绍倾向性评分回归和分层。"
  },
  {
    "objectID": "1036-pssc.html#计算倾向性评分",
    "href": "1036-pssc.html#计算倾向性评分",
    "title": "37  R语言倾向性评分：回归和分层",
    "section": "37.3 计算倾向性评分",
    "text": "37.3 计算倾向性评分\n倾向性评分就是倾向干预的概率，所以可以通过逻辑回归计算P值，这个P值就是倾向性评分，所以也不一定要用到专用的R包！\n首先以处理因素（这里是catholic）为因变量，混杂因素为自变量构建逻辑回归模型：\n\nm_ps &lt;- glm(catholic ~ race_white+w3momed_hsb+p5hmage+w3momscr+w3dadscr,\n            family = binomial(), data = ecls)\n\n提取P值，也就是倾向性评分：\n\nprs_df &lt;- data.frame(pr_score = predict(m_ps, type = \"response\"),\n                     catholic = m_ps$model$catholic)\nhead(prs_df)\n##    pr_score catholic\n## 1 0.3755223        0\n## 2 0.2340976        0\n## 4 0.2990706        0\n## 5 0.2394663        1\n## 6 0.3920115        0\n## 8 0.2391453        0\n\n可以看一下不同处理因素间的P值（倾向性评分）分布：\n\nlabs &lt;- paste(\"Actual school type attended:\", c(\"Catholic\", \"Public\"))\nprs_df %&gt;%\n  mutate(catholic = ifelse(catholic == 1, labs[1], labs[2])) %&gt;%\n  ggplot(aes(x = pr_score)) +\n  geom_histogram(color = \"white\") +\n  facet_wrap(~catholic) +\n  xlab(\"Probability of going to Catholic school\") +\n  theme_bw()\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n计算倾向性评分只是第一步，有了这个倾向性评分后，就可以进行下面的分析了，比如回归、匹配、加权、分层等。\n可以看出我们这个PS是偏态的，其实是可以对PS做一些变换的，比如log，然后使用变换后的PS继续进行后面的分析。这里就不做变换了。"
  },
  {
    "objectID": "1036-pssc.html#倾向性评分回归",
    "href": "1036-pssc.html#倾向性评分回归",
    "title": "37  R语言倾向性评分：回归和分层",
    "section": "37.4 倾向性评分回归",
    "text": "37.4 倾向性评分回归\n此时如果直接把这个评分和catholic作为自变量进行回归分析，就是倾向性评分回归了（也叫协变量调整/倾向性评分矫正等）！应该是倾向性评分4种方法里面最简单的一种了。\n\n# 计算倾向性评分\npr_score &lt;- predict(m_ps, type = \"response\")\n\n# 把倾向性评分加入到原数据中\necls_ps &lt;- ecls %&gt;% \n  mutate(ps = pr_score)\n\n# 把处理因素和倾向性评分作为自变量进行回归\npsl &lt;- lm(c5r2mtsc_std ~ catholic + ps, data = ecls_ps)\nsummary(psl)\n## \n## Call:\n## lm(formula = c5r2mtsc_std ~ catholic + ps, data = ecls_ps)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4.0525 -0.5741  0.0462  0.6106  3.1468 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.58249    0.02929 -19.885  &lt; 2e-16 ***\n## catholic    -0.10772    0.03241  -3.324 0.000893 ***\n## ps           4.48236    0.15873  28.239  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8934 on 5545 degrees of freedom\n## Multiple R-squared:  0.1263, Adjusted R-squared:  0.126 \n## F-statistic: 400.8 on 2 and 5545 DF,  p-value: &lt; 2.2e-16\n\n结果表明处理因素(分组变量)还是有意义的！"
  },
  {
    "objectID": "1036-pssc.html#倾向性评分分层",
    "href": "1036-pssc.html#倾向性评分分层",
    "title": "37  R语言倾向性评分：回归和分层",
    "section": "37.5 倾向性评分分层",
    "text": "37.5 倾向性评分分层\n顾名思义，根据PS值进行分层，然后在每层内进行分析。每一层的协变量分布可认为是同质或均衡的。先对每一层干预与结局之间的关联进行估算，然后对所有层的关联作加权平均，最后得出干预与结局之间的总的关联效应。\n一般来说最好保证干预组和对照组两组的PS范围在差不多的范围内，如果相差很大，那分层效果肯定不好。比如干预组PS范围是0.50.9，对照组PS范围是0.010.4，这样两组PS完全没有交集，按照PS进行分层没啥意义。\n首先看一下PS的范围：\n\necls_ps %&gt;% group_by(catholic) %&gt;% \n  summarise(range = range(ps))\n## Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\n## dplyr 1.1.0.\n## ℹ Please use `reframe()` instead.\n## ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n##   always returns an ungrouped data frame and adjust accordingly.\n## `summarise()` has grouped output by 'catholic'. You can override using the\n## `.groups` argument.\n## # A tibble: 4 × 2\n## # Groups:   catholic [2]\n##   catholic  range\n##      &lt;int&gt;  &lt;dbl&gt;\n## 1        0 0.0370\n## 2        0 0.477 \n## 3        1 0.0492\n## 4        1 0.404\n\n两组分别是0.0370.477和0.0490.404，范围基本一致，所以我们就直接按照总体PS的最大值和最小值进行分层，如果两组PS差很多，可以按照两组PS的交集进行分层。\n文献一般建议分5-10层，可以根据PS进行平分，也可以按照百分位数进行分层，具体方法很多，大家自己看文献即可。\n我们这里简单点，结合上面PS的分布图，分4层，切点就用0.1,0.2,0.3。\n\necls_pslevel &lt;- ecls_ps %&gt;% \n  mutate(ps_level = case_when(ps&lt;=0.1 ~ \"level_1\",\n                              ps&gt;0.1 & ps&lt;=0.2 ~ \"level_2\",\n                              ps&gt;0.2 & ps&lt;=0.3 ~ \"level_3\",\n                              TRUE ~ \"level_4\"\n                              ),\n         #ps_level = factor(ps_level),\n         p5hmage = as.double(p5hmage),\n         across(where(is.integer), as.factor)\n         )\n\nglimpse(ecls_pslevel)\n## Rows: 5,548\n## Columns: 9\n## $ c5r2mtsc_std &lt;dbl&gt; 0.98175332, 0.59437751, 0.49061062, 1.45127793, 2.5956991…\n## $ catholic     &lt;fct&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ race_white   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, …\n## $ w3momed_hsb  &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, …\n## $ p5hmage      &lt;dbl&gt; 47, 41, 43, 38, 47, 41, 31, 38, 26, 38, 27, 40, 33, 36, 4…\n## $ w3momscr     &lt;dbl&gt; 53.50, 34.95, 63.43, 53.50, 61.56, 38.18, 34.95, 63.43, 3…\n## $ w3dadscr     &lt;dbl&gt; 77.50, 53.50, 53.50, 53.50, 77.50, 53.50, 29.60, 33.42, 2…\n## $ ps           &lt;dbl&gt; 0.37552233, 0.23409764, 0.29907061, 0.23946627, 0.3920115…\n## $ ps_level     &lt;chr&gt; \"level_4\", \"level_3\", \"level_3\", \"level_3\", \"level_4\", \"l…"
  },
  {
    "objectID": "1036-pssc.html#分层后的数据",
    "href": "1036-pssc.html#分层后的数据",
    "title": "37  R语言倾向性评分：回归和分层",
    "section": "37.6 分层后的数据",
    "text": "37.6 分层后的数据\n下面我们对每一层内的3个连续型协变量和我们的因变量进行t检验，其实这里可以直接用rstatix包解决，非常好用，但其实rstatix包就是基于purrr的，所以直接用purrr也可以。\n\necls_pslevel %&gt;% \n  pivot_longer(cols = c(1,5:7),names_to = \"variates\",values_to = \"values\") %&gt;% \n  group_nest(ps_level,variates) %&gt;% \n  dplyr::mutate(tt = map(data, ~ t.test(values ~ catholic,data = .x)),\n                res = map_dfr(tt, broom::tidy)\n                ) %&gt;% \n  unnest(res)\n## # A tibble: 16 × 14\n##    ps_level variates         data tt      estimate estimate1 estimate2 statistic\n##    &lt;chr&gt;    &lt;chr&gt;     &lt;list&lt;tibb&gt; &lt;list&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n##  1 level_1  c5r2mtsc… [1,202 × 5] &lt;htest&gt; -0.00108    -0.347   -0.346   -0.00973\n##  2 level_1  p5hmage   [1,202 × 5] &lt;htest&gt; -1.00       32.9     33.9     -1.66   \n##  3 level_1  w3dadscr  [1,202 × 5] &lt;htest&gt; -0.639      36.8     37.4     -0.886  \n##  4 level_1  w3momscr  [1,202 × 5] &lt;htest&gt; -1.07       37.0     38.1     -1.40   \n##  5 level_2  c5r2mtsc… [2,388 × 5] &lt;htest&gt;  0.0685      0.142    0.0737   1.54   \n##  6 level_2  p5hmage   [2,388 × 5] &lt;htest&gt; -0.724      37.4     38.1     -2.92   \n##  7 level_2  w3dadscr  [2,388 × 5] &lt;htest&gt; -0.818      40.7     41.5     -1.73   \n##  8 level_2  w3momscr  [2,388 × 5] &lt;htest&gt; -1.13       41.3     42.5     -2.21   \n##  9 level_3  c5r2mtsc… [1,618 × 5] &lt;htest&gt;  0.171       0.533    0.361    3.46   \n## 10 level_3  p5hmage   [1,618 × 5] &lt;htest&gt;  0.00290    41.1     41.1      0.0141 \n## 11 level_3  w3dadscr  [1,618 × 5] &lt;htest&gt; -1.36       47.5     48.8     -2.17   \n## 12 level_3  w3momscr  [1,618 × 5] &lt;htest&gt; -0.371      50.9     51.3     -0.573  \n## 13 level_4  c5r2mtsc…   [340 × 5] &lt;htest&gt;  0.0580      0.728    0.670    0.548  \n## 14 level_4  p5hmage     [340 × 5] &lt;htest&gt;  0.820      46.2     45.4      1.84   \n## 15 level_4  w3dadscr    [340 × 5] &lt;htest&gt;  0.868      59.6     58.7      0.582  \n## 16 level_4  w3momscr    [340 × 5] &lt;htest&gt; -0.739      60.0     60.7     -0.637  \n## # ℹ 6 more variables: p.value &lt;dbl&gt;, parameter &lt;dbl&gt;, conf.low &lt;dbl&gt;,\n## #   conf.high &lt;dbl&gt;, method &lt;chr&gt;, alternative &lt;chr&gt;\n\n直接看p.value这一列，可以看到大部分都是大于0.05的，因变量c5r2mtsc_std只有在第3层是有差异的！\nlevel_2中的p5hmage和w3momscr变量的P值是小于0.05的，level_3中的w3dadscr变量P值也是小于0.05的。\n这说明我们的分层并没有很好的解决这几个混杂因素的影响，而且分层后每一层内（除了第3层）的因变量都没有差异了。。。理想的结果应该是分层后每一层内混杂因素在两组间都是没有差异的，而因变量都是有差异的！这样才能说明我们的分层很好地控制了混杂因素！\n但我们的这个结果很明显很差劲！大家可以考虑不同的分层方法再重新尝试几次，或者这个数据并不适合使用这种方法，可以用其他方法试试看，比如匹配、回归等。\n下面再看看分类变量，首先是race_white，在每一层内使用卡方检验，我们直接提取P值：\n\necls_pslevel %&gt;% \n  group_split(ps_level) %&gt;% \n  map(~chisq.test(.$race_white,.$catholic,correct=F)) %&gt;% \n  map_dbl(\"p.value\")\n## Warning in chisq.test(.$race_white, .$catholic, correct = F): Chi-squared\n## approximation may be incorrect\n## [1] 0.4755703 0.8423902 0.5696924 0.2667193\n\n结果还不错，每一层内都没有差异了。\n然后是w3momed_hsb这个变量，但是由于我们的分层有问题，导致level_4这一层中w3momed_hsb全都是0！\n\n# level_4有问题\necls_pslevel %&gt;% \n  group_by(ps_level,w3momed_hsb,catholic) %&gt;% \n  summarise(count=n())\n## `summarise()` has grouped output by 'ps_level', 'w3momed_hsb'. You can override\n## using the `.groups` argument.\n## # A tibble: 14 × 4\n## # Groups:   ps_level, w3momed_hsb [7]\n##    ps_level w3momed_hsb catholic count\n##    &lt;chr&gt;    &lt;fct&gt;       &lt;fct&gt;    &lt;int&gt;\n##  1 level_1  0           0           61\n##  2 level_1  0           1            5\n##  3 level_1  1           0         1082\n##  4 level_1  1           1           54\n##  5 level_2  0           0         1262\n##  6 level_2  0           1          261\n##  7 level_2  1           0          724\n##  8 level_2  1           1          141\n##  9 level_3  0           0         1192\n## 10 level_3  0           1          407\n## 11 level_3  1           0           14\n## 12 level_3  1           1            5\n## 13 level_4  0           0          262\n## 14 level_4  0           1           78\n\n所以我们就对前3层做一个统计检验吧。\n\necls_pslevel %&gt;% \n  filter(!ps_level == \"level_4\") %&gt;% \n  group_split(ps_level) %&gt;% \n  map(~chisq.test(.$w3momed_hsb,.$catholic,correct=F)) %&gt;% \n  map_dbl(\"p.value\")\n## Warning in chisq.test(.$w3momed_hsb, .$catholic, correct = F): Chi-squared\n## approximation may be incorrect\n\n## Warning in chisq.test(.$w3momed_hsb, .$catholic, correct = F): Chi-squared\n## approximation may be incorrect\n## [1] 0.3022080 0.5994507 0.9316443\n\n可以看到每一层内也是没有明显差别的。\n说明我们的分层对2个分类变量的平衡效果还是可以的，但是对连续型变量的效果真是一言难尽！"
  },
  {
    "objectID": "1036-pssc.html#总结",
    "href": "1036-pssc.html#总结",
    "title": "37  R语言倾向性评分：回归和分层",
    "section": "37.7 总结",
    "text": "37.7 总结\n倾向性评分回归和分层的大致过程就是这样的，但其实很多细节我都忽略了，比如到底分几层？依据是什么？用PS还是log(PS)？\n而且特地找了一个不是很成功的例子（可能不是很恰当），结果并不是很完美，还有很多可以调整测试的空间，大家可以适当修改其中的方法细节，最后得到一个笔记好的结果。\n实际使用时大家要根据自己的实际情况选择最合适的方法，多读文献，从文献中找灵感。"
  },
  {
    "objectID": "1036-pssc.html#参考资料",
    "href": "1036-pssc.html#参考资料",
    "title": "37  R语言倾向性评分：回归和分层",
    "section": "37.8 参考资料",
    "text": "37.8 参考资料\n\nhttps://sejdemyr.github.io/r-tutorials/statistics/tutorial8.html"
  },
  {
    "objectID": "1037-psw.html#演示数据",
    "href": "1037-psw.html#演示数据",
    "title": "38  R语言倾向性评分：加权",
    "section": "38.1 演示数据",
    "text": "38.1 演示数据\n\ndata(lindner, package = \"twang\")\n\nlindner[,c(3,4,6,7,8,10)] &lt;- lapply(lindner[,c(3,4,6,7,8,10)],factor)\n\nstr(lindner)\n## 'data.frame':    996 obs. of  11 variables:\n##  $ lifepres       : num  0 11.6 11.6 11.6 11.6 11.6 11.6 11.6 11.6 11.6 ...\n##  $ cardbill       : int  14301 3563 4694 7366 8247 8319 8410 8517 8763 8823 ...\n##  $ abcix          : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ stent          : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ height         : int  163 168 188 175 168 178 185 173 152 180 ...\n##  $ female         : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 1 1 2 2 1 ...\n##  $ diabetic       : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 1 1 1 1 1 1 ...\n##  $ acutemi        : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ejecfrac       : int  56 56 50 50 55 50 58 30 60 60 ...\n##  $ ves1proc       : Factor w/ 6 levels \"0\",\"1\",\"2\",\"3\",..: 2 2 2 2 2 2 2 2 2 2 ...\n##  $ sixMonthSurvive: logi  FALSE TRUE TRUE TRUE TRUE TRUE ...\n\n\n\n\n\n\n\n\n\n\n其中abcix是处理因素变量，sixMonthSurvive是二分类的结局变量，cardbill是连续型的结局变量，其余变量是协变量。\n首先可以通过tableone包看一下加权前的数据情况：\n\nlibrary(tableone)\n\ncovs &lt;- colnames(lindner)[c(1,4:10)]\n\ntab &lt;- CreateTableOne(vars = covs,\n                      strata = \"abcix\",\n                      data = lindner\n                      )\nprint(tab,showAllLevels = T,smd = T)\n##                       Stratified by abcix\n##                        level 0              1              p      test SMD   \n##   n                             298            698                           \n##   lifepres (mean (SD))        11.02 (2.54)   11.42 (1.45)   0.002       0.194\n##   stent (%)            0        124 (41.6)     206 (29.5)  &lt;0.001       0.255\n##                        1        174 (58.4)     492 (70.5)                    \n##   height (mean (SD))         171.45 (10.59) 171.44 (10.69)  0.996      &lt;0.001\n##   female (%)           0        183 (61.4)     467 (66.9)   0.111       0.115\n##                        1        115 (38.6)     231 (33.1)                    \n##   diabetic (%)         0        218 (73.2)     555 (79.5)   0.034       0.150\n##                        1         80 (26.8)     143 (20.5)                    \n##   acutemi (%)          0        280 (94.0)     573 (82.1)  &lt;0.001       0.372\n##                        1         18 ( 6.0)     125 (17.9)                    \n##   ejecfrac (mean (SD))        52.29 (10.30)  50.40 (10.42)  0.009       0.182\n##   ves1proc (%)         0          1 ( 0.3)       3 ( 0.4)  &lt;0.001       0.446\n##                        1        243 (81.5)     437 (62.6)                    \n##                        2         47 (15.8)     205 (29.4)                    \n##                        3          6 ( 2.0)      39 ( 5.6)                    \n##                        4          1 ( 0.3)      13 ( 1.9)                    \n##                        5          0 ( 0.0)       1 ( 0.1)\n\n如果只是看一下协变量是否在不同组间均衡，可以通过之前介绍过的cobalt实现：\n\nlibrary(cobalt)\n##  cobalt (Version 4.4.1, Build Date: 2022-11-03)\n\n# 选择只有协变量的数据框\ncovariates &lt;- subset(lindner, select = c(1,4:10))\n\nbal.tab(covariates,treat = lindner$abcix, s.d.denom = \"pooled\",\n        m.threshold = 0.1, un = TRUE,\n        v.threshold = 2\n        )\n## Balance Measures\n##               Type Diff.Un     M.Threshold.Un V.Ratio.Un   V.Threshold.Un\n## lifepres   Contin.  0.1941 Not Balanced, &gt;0.1     0.3239 Not Balanced, &gt;2\n## stent       Binary  0.1210 Not Balanced, &gt;0.1          .                 \n## height     Contin. -0.0003     Balanced, &lt;0.1     1.0201     Balanced, &lt;2\n## female      Binary -0.0550     Balanced, &lt;0.1          .                 \n## diabetic    Binary -0.0636     Balanced, &lt;0.1          .                 \n## acutemi     Binary  0.1187 Not Balanced, &gt;0.1          .                 \n## ejecfrac   Contin. -0.1821 Not Balanced, &gt;0.1     1.0238     Balanced, &lt;2\n## ves1proc_0  Binary  0.0009     Balanced, &lt;0.1          .                 \n## ves1proc_1  Binary -0.1894 Not Balanced, &gt;0.1          .                 \n## ves1proc_2  Binary  0.1360 Not Balanced, &gt;0.1          .                 \n## ves1proc_3  Binary  0.0357     Balanced, &lt;0.1          .                 \n## ves1proc_4  Binary  0.0153     Balanced, &lt;0.1          .                 \n## ves1proc_5  Binary  0.0014     Balanced, &lt;0.1          .                 \n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1         7\n## Not Balanced, &gt;0.1     6\n## \n## Variable with the greatest mean difference\n##  Variable Diff.Un     M.Threshold.Un\n##  lifepres  0.1941 Not Balanced, &gt;0.1\n## \n## Balance tally for variance ratios\n##                  count\n## Balanced, &lt;2         2\n## Not Balanced, &gt;2     1\n## \n## Variable with the greatest variance ratio\n##  Variable V.Ratio.Un   V.Threshold.Un\n##  lifepres     0.3239 Not Balanced, &gt;2\n## \n## Sample sizes\n##     Control Treated\n## All     298     698\n\nDiff.Adj就是SMD"
  },
  {
    "objectID": "1037-psw.html#iptw",
    "href": "1037-psw.html#iptw",
    "title": "38  R语言倾向性评分：加权",
    "section": "38.2 IPTW",
    "text": "38.2 IPTW\n倾向性评分只是一个概率（倾向干预组的概率），计算概率的算法是在是太多了，选择自己喜欢的就好，我这里就用最简单的逻辑回归，之前的推文中也演示过随机森林等其他估计PS的方法。\n\npsfit &lt;- glm(abcix ~ stent + height + female + diabetic + acutemi + \n               ejecfrac + ves1proc,\n             data = lindner, family = binomial())\nps &lt;- psfit$fitted.values\n\n逆概率加权以全部研究对象（ATE）为目标人群，通过加权是每一组研究对象的协变量分布于全部研究对象相似。\n该种加权方法下，研究对象的权重为该对象所在组的概率的倒数。\n\n干预组：1/ps\n对照组：1/(1-ps)\n\n下面根据计算出的PS计算每一个样本的权重：\n\niptw &lt;- ifelse(lindner$abcix == 1, 1/ps, 1/(1-ps))\n\nlindner$iptw &lt;- iptw\n\n加权后可以再次看看数据是否已经均衡：\n\nbal.tab(covariates,treat = lindner$abcix, s.d.denom = \"pooled\",\n        weights = lindner$iptw,\n        m.threshold = 0.1, un = TRUE,\n        v.threshold = 2\n        )\n## Balance Measures\n##               Type Diff.Un V.Ratio.Un Diff.Adj        M.Threshold V.Ratio.Adj\n## lifepres   Contin.  0.1941     0.3239   0.3310 Not Balanced, &gt;0.1      0.2185\n## stent       Binary  0.1210          .   0.0036     Balanced, &lt;0.1           .\n## height     Contin. -0.0003     1.0201  -0.0175     Balanced, &lt;0.1      0.8647\n## female      Binary -0.0550          .   0.0101     Balanced, &lt;0.1           .\n## diabetic    Binary -0.0636          .  -0.0175     Balanced, &lt;0.1           .\n## acutemi     Binary  0.1187          .  -0.0028     Balanced, &lt;0.1           .\n## ejecfrac   Contin. -0.1821     1.0238  -0.0119     Balanced, &lt;0.1      0.9784\n## ves1proc_0  Binary  0.0009          .   0.0009     Balanced, &lt;0.1           .\n## ves1proc_1  Binary -0.1894          .   0.0211     Balanced, &lt;0.1           .\n## ves1proc_2  Binary  0.1360          .  -0.0073     Balanced, &lt;0.1           .\n## ves1proc_3  Binary  0.0357          .  -0.0155     Balanced, &lt;0.1           .\n## ves1proc_4  Binary  0.0153          .  -0.0002     Balanced, &lt;0.1           .\n## ves1proc_5  Binary  0.0014          .   0.0010     Balanced, &lt;0.1           .\n##                 V.Threshold\n## lifepres   Not Balanced, &gt;2\n## stent                      \n## height         Balanced, &lt;2\n## female                     \n## diabetic                   \n## acutemi                    \n## ejecfrac       Balanced, &lt;2\n## ves1proc_0                 \n## ves1proc_1                 \n## ves1proc_2                 \n## ves1proc_3                 \n## ves1proc_4                 \n## ves1proc_5                 \n## \n## Balance tally for mean differences\n##                    count\n## Balanced, &lt;0.1        12\n## Not Balanced, &gt;0.1     1\n## \n## Variable with the greatest mean difference\n##  Variable Diff.Adj        M.Threshold\n##  lifepres    0.331 Not Balanced, &gt;0.1\n## \n## Balance tally for variance ratios\n##                  count\n## Balanced, &lt;2         2\n## Not Balanced, &gt;2     1\n## \n## Variable with the greatest variance ratio\n##  Variable V.Ratio.Adj      V.Threshold\n##  lifepres      0.2185 Not Balanced, &gt;2\n## \n## Effective sample sizes\n##            Control Treated\n## Unadjusted  298.    698.  \n## Adjusted    202.27  671.09\n\n可以看到除了lifepres之外，其他全都均衡了，效果还是挺不错的。加权后，干预组和对照组的样本量已经变了哦！\n如果想要画出加权后数据的基线资料表，可以借助survey包。\n\nlibrary(survey)\n## Loading required package: grid\n## Loading required package: Matrix\n## Loading required package: survival\n## \n## Attaching package: 'survey'\n## The following object is masked from 'package:graphics':\n## \n##     dotchart\n\n# 获取加权后的数据\ndf &lt;- svydesign(ids = ~1, data = lindner, weights = ~ iptw)\n\n# 使用tableone中的函数创建加权后的三线表\ntab_IPTW=svyCreateTableOne(vars=covs, strata=\"abcix\",data=df ,test=T) \nprint(tab_IPTW,showAllLevels=TRUE,smd=TRUE)\n##                       Stratified by abcix\n##                        level 0               1              p      test SMD   \n##   n                          1004.38         994.47                           \n##   lifepres (mean (SD))         10.74 (3.05)   11.42 (1.43)   0.024       0.288\n##   stent (%)            0       333.7 (33.2)   326.8 (32.9)   0.921       0.008\n##                        1       670.7 (66.8)   667.7 (67.1)                    \n##   height (mean (SD))          171.60 (11.39) 171.41 (10.60)  0.849       0.017\n##   female (%)           0       668.5 (66.6)   651.8 (65.5)   0.782       0.021\n##                        1       335.9 (33.4)   342.7 (34.5)                    \n##   diabetic (%)         0       762.7 (75.9)   772.6 (77.7)   0.610       0.042\n##                        1       241.7 (24.1)   221.8 (22.3)                    \n##   acutemi (%)          0       857.6 (85.4)   851.8 (85.7)   0.942       0.008\n##                        1       146.8 (14.6)   142.6 (14.3)                    \n##   ejecfrac (mean (SD))         51.07 (10.23)  50.95 (10.12)  0.879       0.012\n##   ves1proc (%)         0         3.0 ( 0.3)     3.9 ( 0.4)   0.937       0.088\n##                        1       664.1 (66.1)   678.6 (68.2)                    \n##                        2       261.6 (26.0)   251.7 (25.3)                    \n##                        3        61.3 ( 6.1)    45.3 ( 4.6)                    \n##                        4        14.4 ( 1.4)    14.0 ( 1.4)                    \n##                        5         0.0 ( 0.0)     1.0 ( 0.1)\n\n加权之后，就可以做各种分析了，比如回归分析等，分析时把权重因素也考虑进去即可。\n这里演示逻辑回归，根据因变量的类型，可选择不同的回归方法。\n\nf &lt;- glm(sixMonthSurvive~abcix+stent+height+female+diabetic+acutemi+\n           ejecfrac+ves1proc,\n             data = lindner, family = binomial(),\n         weights = iptw # 把权重加进去\n         )\n## Warning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nsummary(f)\n## \n## Call:\n## glm(formula = sixMonthSurvive ~ abcix + stent + height + female + \n##     diabetic + acutemi + ejecfrac + ves1proc, family = binomial(), \n##     data = lindner, weights = iptw)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -4.9894   0.0958   0.1640   0.3071   2.8009  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  8.598e+00  1.585e+03   0.005  0.99567    \n## abcix1       1.785e+00  3.215e-01   5.551 2.84e-08 ***\n## stent1      -6.401e-01  3.024e-01  -2.117  0.03426 *  \n## height       3.491e-02  1.154e-02   3.025  0.00248 ** \n## female1      8.123e-03  3.143e-01   0.026  0.97938    \n## diabetic1   -7.314e-01  2.813e-01  -2.599  0.00934 ** \n## acutemi1    -1.540e+00  3.011e-01  -5.116 3.11e-07 ***\n## ejecfrac     5.923e-02  1.057e-02   5.604 2.10e-08 ***\n## ves1proc1   -1.378e+01  1.585e+03  -0.009  0.99306    \n## ves1proc2   -1.184e+01  1.585e+03  -0.007  0.99404    \n## ves1proc3   -1.569e+01  1.585e+03  -0.010  0.99210    \n## ves1proc4   -7.180e-02  1.787e+03   0.000  0.99997    \n## ves1proc5   -1.817e+00  4.262e+03   0.000  0.99966    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 732.98  on 995  degrees of freedom\n## Residual deviance: 469.49  on 983  degrees of freedom\n## AIC: 485.36\n## \n## Number of Fisher Scoring iterations: 16\n\n\n但是这种方法存在问题，我在stackoverflow中的帖子中看到有人指出，R自带的lm和glm中的weights参数并不是样本的权重，这点可以查看帮助文档确定，所以如果想要使用加权后的数据进行线性回归和逻辑回归，需要使用其他的R包，比如survey包。\n\n除了上面介绍的手动计算权重的方法，也可以通过多个R包实现，比如PSW/PSweight/twang等，大家感兴趣的可以自己查看相关说明。"
  },
  {
    "objectID": "1037-psw.html#重叠加权",
    "href": "1037-psw.html#重叠加权",
    "title": "38  R语言倾向性评分：加权",
    "section": "38.3 重叠加权",
    "text": "38.3 重叠加权\n重叠加权的目标人群是两组协变量相似的人，即PS值分布重叠的人，其估计的效应为重叠人群平均处理效应（ATO）。\n\n干预组：1-ps\n对照组：ps\n\n重叠加权的优缺点可以看这篇文章：最强的倾向性评分方法—重叠加权\n使用PSweight包演示重叠加权，这个包不仅可以用于二分类，还可以用于多分类。\n还是使用lindner这个数据集。\n\n## 数据准备\nrm(list = ls())\ndata(lindner, package = \"twang\") \n\n# 构建估计PS的formula\nformula.ps &lt;- abcix ~ stent + height + female + diabetic + acutemi + ejecfrac + ves1proc\n\n进行重叠加权：\n\nlibrary(PSweight)\n\nPSweight &lt;- PSweight(ps.formula = formula.ps, data = lindner, \n                     weight = \"overlap\", # 重叠加权\n                     yname = \"cardbill\", # 因变量\n                     family = \"gaussian\", \n                     ps.method = \"glm\", \n                     out.method = \"glm\"\n                     )\n\n#返回结果，效应估计及其标准误、置信区间、P值\nsummary(PSweight)\n## \n## Closed-form inference: \n## \n## Original group value:  0, 1 \n## \n## Contrast: \n##             0 1\n## Contrast 1 -1 1\n## \n##            Estimate Std.Error     lwr    upr Pr(&gt;|z|)\n## Contrast 1  1134.91    879.51 -588.90 2858.7   0.1969\n\n计算数据均衡性：\n\nSumStat&lt;-SumStat(ps.formula = formula.ps, data = lindner, weight = \"overlap\")\nSumStat[[\"ess\"]] #有效样本量\n##   unweighted  overlap\n## 0        298 287.4367\n## 1        698 569.5826\nplot(SumStat) #均衡性检验图形\n\n\n\nsummary(SumStat) #均衡性检验\n## unweighted result\n##           Mean 0  Mean 1   SMD\n## stent      0.584   0.705 0.254\n## height   171.446 171.443 0.000\n## female     0.386   0.331 0.115\n## diabetic   0.268   0.205 0.150\n## acutemi    0.060   0.179 0.371\n## ejecfrac  52.289  50.403 0.182\n## ves1proc   1.205   1.463 0.427\n## \n## overlap result\n##           Mean 0  Mean 1 SMD\n## stent      0.633   0.633   0\n## height   171.464 171.464   0\n## female     0.359   0.359   0\n## diabetic   0.242   0.242   0\n## acutemi    0.078   0.078   0\n## ejecfrac  51.830  51.830   0\n## ves1proc   1.257   1.257   0\n\n加权后，可以进行后续的各种分析，这里就不演示了。"
  },
  {
    "objectID": "1037-psw.html#参考资料",
    "href": "1037-psw.html#参考资料",
    "title": "38  R语言倾向性评分：加权",
    "section": "38.4 参考资料",
    "text": "38.4 参考资料\n涂博祥, 秦婴逸, 吴骋, 等. 倾向性评分加权方法介绍及R软件实现[J]. 中国循证医学杂志, 2022, 22(3): 365–372."
  },
  {
    "objectID": "1038-p4trend.html#p-for-trend",
    "href": "1038-p4trend.html#p-for-trend",
    "title": "39  p-for-trend/ p-for-interaction/ per-1-sd R语言实现",
    "section": "39.1 P for trend",
    "text": "39.1 P for trend\nP for trend是线性趋势检验的P值，用于反映自变量和因变量是否存在线性趋势关系。线性趋势检验，之前介绍过Cochran Armitage检验，不过是针对分类变量的。\n今天要介绍的P for trend主要是针对连续型变量的。\n关于p for trend具体含义和数值型变量分箱的方法，大家可以参考医咖会的文章：p for trend是个啥\n把连续性变量转换为分类变量(在R里转变为因子)，设置哑变量，进行回归分析，即可得到OR值及95%的可信区间；把转换好的分类变量当做数值型，进行回归分析，即可得到P for trend\n使用之前逻辑回归的例子演示，来自孙振球版医学统计学第4版，电子版和配套数据均放在QQ群文件中，需要的加群下载即可。\n\ndf16_2 &lt;- foreign::read.spss(\"datasets/例16-02.sav\", \n                             to.data.frame = T,\n                             use.value.labels = F,\n                             reencode  = \"utf-8\")\n## re-encoding from utf-8\n\nstr(df16_2)\n## 'data.frame':    54 obs. of  11 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : num  3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : num  1 0 1 0 0 0 0 0 0 0 ...\n##  $ x3   : num  0 1 0 0 0 1 1 1 0 0 ...\n##  $ x4   : num  1 1 1 1 1 1 0 1 0 1 ...\n##  $ x5   : num  0 0 0 0 0 0 0 1 0 0 ...\n##  $ x6   : num  0 0 0 0 1 0 0 0 0 0 ...\n##  $ x7   : num  1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : num  1 0 0 0 1 1 0 0 1 0 ...\n##  $ y    : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n\n数据一共11列，第1列是编号，第2-9列是自变量，第10列是因变量。\n具体说明： - x1：年龄，小于45岁是1,45-55是2,55-65是3,65以上是4； - x2：高血压病史，1代表有，0代表无； - x3：高血压家族史，1代表有，0代表无； - x4：吸烟，1代表吸烟，0代表不吸烟； - x5：高血脂病史，1代表有，0代表无； - x6：动物脂肪摄入，0表示低，1表示高 - x7：BMI，小于24是1,24-26是2，大于26是3； - x8：A型性格，1代表是，0代表否； - y：是否是冠心病，1代表是，0代表否\n这里的x1~y虽然是数值型，但并不是真的代表数字大小，只是为了方便标识，\n年龄x1应该是数值型的，但是为了方便解释逻辑回归的意义，我们对它进行了分箱处理，也就是把它转换为了分类变量。数值型变量进行分箱，是回归分析中计算p for trend的第一步\n此时x1是数值型，我们直接进行逻辑回归，得到的P值就是 p for trend\n\nf &lt;- glm(y ~ x1 + x2, \n         data = df16_2, \n         family = binomial())\n\nbroom::tidy(f)\n## # A tibble: 3 × 5\n##   term        estimate std.error statistic p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 (Intercept)   -2.22      1.03      -2.15  0.0313\n## 2 x1             0.712     0.423      1.68  0.0928\n## 3 x2             1.08      0.625      1.73  0.0840\n\n0.09279918就是x1的p for trend，而且还是校正了x2这个变量之后的p for trend，是不是很简单？\n此时如果我们把x1变成因子型，那在进行回归分析时会自动进行哑变量编码，就可以得到几个组的OR值和95%的可信区间，关于R语言中分类变量进行回归分析时常用的一些编码方法，强烈你看一下这篇推文：R语言分类变量进行回归分析的编码方案。\n\n# 变为因子型\ndf16_2$x1.f &lt;- factor(df16_2$x1)\n\n# 把因子放入自变量\nf &lt;- glm(y ~ x1.f + x2, \n         data = df16_2, \n         family = binomial())\nbroom::tidy(f,conf.int=T,exponentiate=T)\n## # A tibble: 5 × 7\n##   term        estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)    0.200     1.10     -1.47   0.142    0.0104      1.24\n## 2 x1.f2          2.32      1.19      0.704  0.481    0.289      49.3 \n## 3 x1.f3          4.48      1.26      1.19   0.233    0.485     102.  \n## 4 x1.f4          9.42      1.63      1.38   0.169    0.508     438.  \n## 5 x2             2.94      0.639     1.69   0.0918   0.854      10.7\n\n这样就得到了x1.f中4/3/2分别和1进行比较的OR值和95%的可信区间。当然你写函数提取也行：\n\n# OR值\nexp(coef(f))\n## (Intercept)       x1.f2       x1.f3       x1.f4          x2 \n##    0.200000    2.319343    4.476753    9.415697    2.936212\n\n# OR值的95%的可信区间\nexp(confint(f))\n## Waiting for profiling to be done...\n##                  2.5 %     97.5 %\n## (Intercept) 0.01043892   1.240092\n## x1.f2       0.28932733  49.284803\n## x1.f3       0.48497992 102.335996\n## x1.f4       0.50766137 437.541812\n## x2          0.85353009  10.723068\n\n这样就得到了每个组的OR值和95%的可信区间，可以看到没有第1组的，因为第一组是参考，所有组都是和第一组进行比较。"
  },
  {
    "objectID": "1038-p4trend.html#p-for-interaction",
    "href": "1038-p4trend.html#p-for-interaction",
    "title": "39  p-for-trend/ p-for-interaction/ per-1-sd R语言实现",
    "section": "39.2 p for interaction",
    "text": "39.2 p for interaction\np for interaction是交互作用的P值，关于其含义可以参考松哥统计的这篇文章：p for interaction是什么\n\n目前计算P for interaction两种方法： 1. 对于数值与等级或二分类，可以直接模型中增加相乘项【如x1×X2】，然后看交互项有无意义。 2. 而对于多项分类【如血型】，产生哑变量后，相乘则会产生多个交互项，此时不能整体判断交互作用是否有意义。我们可以先构建一个无交互作用项的模型，再构建一个有交互作用项的模型。然后采用似然比检验（likelihood ratio test）进行比较有个模型差异，则可以判定交互项整体是否有意义。\n\n\n39.2.1 方法1\n假如探索年龄(x1)和BMI(x7)之间对因变量y有没有交互作用，我们首先新建一列相乘列，然后进行回归分析。\n\n# 新建1列\ndf16_2$x17 &lt;- df16_2$x1 * df16_2$x7\nstr(df16_2)\n## 'data.frame':    54 obs. of  13 variables:\n##  $ .... : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ x1   : num  3 2 2 2 3 3 2 3 2 1 ...\n##  $ x2   : num  1 0 1 0 0 0 0 0 0 0 ...\n##  $ x3   : num  0 1 0 0 0 1 1 1 0 0 ...\n##  $ x4   : num  1 1 1 1 1 1 0 1 0 1 ...\n##  $ x5   : num  0 0 0 0 0 0 0 1 0 0 ...\n##  $ x6   : num  0 0 0 0 1 0 0 0 0 0 ...\n##  $ x7   : num  1 1 1 1 1 2 1 1 1 1 ...\n##  $ x8   : num  1 0 0 0 1 1 0 0 1 0 ...\n##  $ y    : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ PGR_1: num  1 0 0 0 1 1 0 0 0 0 ...\n##  $ x1.f : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 3 2 2 2 3 3 2 3 2 1 ...\n##  $ x17  : num  3 2 2 2 3 6 2 3 2 1 ...\n##  - attr(*, \"variable.labels\")= Named chr [1:11] \"\" \"\" \"\" \"\" ...\n##   ..- attr(*, \"names\")= chr [1:11] \"....\" \"x1\" \"x2\" \"x3\" ...\n\n\n# 进行逻辑回归\nf &lt;- glm(y ~ x1 + x7 + x17, \n         family = binomial(),\n         data = df16_2\n         )\nsummary(f)\n## \n## Call:\n## glm(formula = y ~ x1 + x7 + x17, family = binomial(), data = df16_2)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.7967  -0.9092  -0.6507   1.1651   1.7708  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)\n## (Intercept)  -0.3953     2.6477  -0.149    0.881\n## x1           -0.4867     1.1142  -0.437    0.662\n## x7           -1.1509     1.7095  -0.673    0.501\n## x17           0.9249     0.7489   1.235    0.217\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 74.786  on 53  degrees of freedom\n## Residual deviance: 62.508  on 50  degrees of freedom\n## AIC: 70.508\n## \n## Number of Fisher Scoring iterations: 5\n\n结果中显示x17的P值(p for interaction)是：0.217，交互作用项是没有统计学意义的。\n\n\n39.2.2 方法2\n\n# 先构建一个没有交互项的逻辑回归模型\nf1 &lt;- glm(y ~ x1 + x7, \n         family = binomial(),\n         data = df16_2)\n\n# 再构建一个有交互作用的逻辑回归模型\nf2 &lt;- glm(y ~ x1 + x7 + x17, \n         family = binomial(),\n         data = df16_2)\n\n# 似然比检验\nlmtest::lrtest(f1,f2)\n## Likelihood ratio test\n## \n## Model 1: y ~ x1 + x7\n## Model 2: y ~ x1 + x7 + x17\n##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n## 1   3 -32.216                     \n## 2   4 -31.254  1 1.9238     0.1654\n\n结果显示P(p for interaction)=0.1654，也就是交互作用项没有统计学意义。"
  },
  {
    "objectID": "1038-p4trend.html#per-1-sd",
    "href": "1038-p4trend.html#per-1-sd",
    "title": "39  p-for-trend/ p-for-interaction/ per-1-sd R语言实现",
    "section": "39.3 per 1 sd",
    "text": "39.3 per 1 sd\n关于什么是per 1 sd，可以参考松哥统计的这篇文章：per 1 sd\n\nPer 1 sd的实现，其实就是把原始数据进行标准化，另存为一个新的变量X，新变量X因为是被标准化后的数据，因此其均数和标准差为0和1。然后让x进入模型进行分析。请问大家此时x每增加1个单位，效应量增加的风险为HR。因为标准差为1，此时x增加1个单位，就是Per 1 sd。1=Per 1 sd。就是自变量每增加1个标准差。\n\n为了方便演示，我们新建一列数据weight，然后进行标准化，再进行逻辑回归。\n\n# 新建一列weight\ndf16_2$weight &lt;- rnorm(54, 70,11)\n\n# 进行标准化\ndf16_2$weight.scaled &lt;- scale(df16_2$weight)\n\n# 进行逻辑回归\nf &lt;- glm(y ~ weight.scaled, data = df16_2)\nbroom::tidy(f,conf.int=T,exponentiate=T)\n## # A tibble: 2 × 7\n##   term          estimate std.error statistic       p.value conf.low conf.high\n##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)      1.62     0.0693     6.95  0.00000000592    1.41       1.85\n## 2 weight.scaled    0.987    0.0699    -0.188 0.852            0.861      1.13\n\n结果给出了P值，OR值以及95%的可信区间。"
  },
  {
    "objectID": "1039-nonlinear.html#多项式拟合",
    "href": "1039-nonlinear.html#多项式拟合",
    "title": "40  R语言多项式拟合",
    "section": "40.1 多项式拟合",
    "text": "40.1 多项式拟合\n我们用car包里面的USPop数据集进行演示。这个数据集一共两列，一列是年份，另一列是美国每一年的人口数量，数据一共22行。\n\n# 加载数据\nlibrary(car)\n## Warning: package 'car' was built under R version 4.2.3\n## Loading required package: carData\ndata(\"USPop\")\npsych::headTail(USPop)\n##     year population\n## 1   1790       3.93\n## 2   1800       5.31\n## 3   1810       7.24\n## 4   1820       9.64\n## ...  ...        ...\n## 19  1970      203.3\n## 20  1980     226.54\n## 21  1990     248.71\n## 22  2000     281.42\n\n我们首先画图看看两列数据的情况：\n\nplot(population ~ year, data = USPop)\n\n\n\n\n这个数据很明显是曲线的形状，并不是一条直线，所以此时我们直接用线性回归（直线）拟合这样的数据是不合适的。不信我们可以画图看看。\n\n# 拟合线性回归\nf &lt;- lm(population ~ year, data = USPop)\n\n# 画出原来的数据\nplot(population ~ year, data = USPop)\n\n# 添加拟合线\nlines(USPop$year, fitted(f), col = \"blue\")\n\n\n\n\n图中这条蓝色的线就是线性拟合的线，很明显，对数据的拟合很差。\n那我们应该用什么方法拟合这个关系呢？\n根据之前的两篇推文，拟合非线性关系有非常多的方法，至少有3种：\n\n多项式回归\n分段回归\n样条回归\n\n我们这里先介绍多项式回归。\n多项式回归非常简单，就是个高中学过的高次方程的曲线。\n现在我们先拟合一个二次项的多项式回归：\n\n# 2次项，注意用法\nf1 &lt;- lm(population ~ year + I(year^2), data = USPop)\n\n# 画出拟合线\nplot(population ~ year, data = USPop)\nlines(USPop$year, fitted(f1))\n\n\n\n\n结果拟合很好，二次项就已经拟合效果非常好了，如果你还想看一下更高次项拟合，可以继续试试，比如3次项：\n\n# 3次项，注意用法\nf2 &lt;- lm(population ~ year + I(year^2) + I(year^3), data = USPop)\n\n# 画出拟合线\nplot(population ~ year, data = USPop)\nlines(USPop$year, fitted(f2))\n\n\n\n\n结果可见增加了一个3次项，结果并没有好很多。所以我们可以就选2次项即可。\n当然也有一些统计方法可以检验，加了2次项、3次项之后是不是有统计学意义，可以用似然比检验，比如anova：\n\n# 线性回归和2次项比较\nanova(f, f1)\n## Analysis of Variance Table\n## \n## Model 1: population ~ year\n## Model 2: population ~ year + I(year^2)\n##   Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n## 1     20 12819.0                                  \n## 2     19   170.7  1     12648 1408.1 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 2次项和3次项比较\nanova(f1, f2)\n## Analysis of Variance Table\n## \n## Model 1: population ~ year + I(year^2)\n## Model 2: population ~ year + I(year^2) + I(year^3)\n##   Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n## 1     19 170.66                              \n## 2     18 143.64  1    27.027 3.3868 0.08227 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n结果很明显，加入2次项之后，P值是小于0.05的，说明是有统计学意义的，但是2次项和3次项比较，就没有统计学意义了，说明我们只要用到2次项即可。\n在写论文的时候应该如何描述这些方法呢？请多看文献，这不在本文的讨论范围。\n为了加深理解，下面再给大家举一个例子。\n首先是构造一个数据，构造数据的过程不需要看。\n\nx &lt;- 1:100         \nk &lt;- c(25, 50, 75) \nu &lt;- function(x)ifelse(x &gt; 0, x, 0)\nx2 &lt;- u(x - k[1])\nx3 &lt;- u(x - k[2])\nx4 &lt;- u(x - k[3])\nset.seed(1)\ny &lt;- 0.8 + 1*x + -1.2*x2 + 1.4*x3 + -1.6*x4 + rnorm(100,sd = 2.2)\nplot(x, y)\n\n\n\n\n这样的一个数据，很明显也不是线性的，所以此时线性回归肯定不合适。我们尝试用多项式回归来拟合这个数据。\n这个数据，我已经帮大家试好了，需要拟合6次项才会比较完美。\n\n# 拟合6次项\nf.6 &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))\n\n# 画出拟合线\nplot(x,y)\nlines(x, fitted(f.6))\n\n\n\n\n可以看到拟合线比较贴合数据。但是在拟合线的开头和末尾可以发现有点上翘的趋势，这也是多项式拟合的缺点，如果此时在两头多点数据，可能拟合效果就不是很好了。解决方法也很简单，就是我们下次要介绍的样条回归。\n多项式回归的公式写法像上面这样略显复杂，如果是更高次的项，岂不是更复杂？当然是有简便写法的。可以使用poly()函数。\n\n# 多项式拟合的简便写法，拟合6次项，和上面结果完全一样\nf.6 &lt;- lm(y ~ poly(x, 6))\n\n# 画出拟合线\nplot(x,y)\nlines(x, fitted(f.6))\n\n\n\n\n可以看到使用poly()函数极大的简化了公式写法，而且很好理解，后面的数字就代表了次方。看到这里，不知道你有没有想起重复测量数据的多重比较中用过的正交多项式呢？没有印象的赶紧去复习下：重复测量数据的多重比较\n这样的拟合线，当然也是可以用ggplot2画的。\n\nlibrary(ggplot2)\n## Warning: package 'ggplot2' was built under R version 4.2.3\n\nggplot()+\n  geom_point(aes(x,y),size=2)+\n  geom_line(aes(x, fitted(f.6)), color=\"red\",size=2)+\n  theme_bw()\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead.\n\n\n\n\n或者下面这样，好理解，还可以添加可信区间：\n\ndf.tmp &lt;- data.frame(x = x, y= y)\n\nggplot(df.tmp, aes(x,y))+\n  geom_point(size=2)+\n  geom_smooth(method = \"lm\",\n              formula = y ~ poly(x,6),\n              color=\"red\",\n              size=2,\n              se = T, # 可信区间\n              )+\n  theme_bw()\n\n\n\n\n最后一个问题：多项式能用于逻辑回归吗？Cox回归呢？\n当然可以了，只是把自变量变成多次项而已，和lm用法一模一样，函数使用glm()/coxph()等即可！"
  },
  {
    "objectID": "1040-rcs.html#推荐阅读",
    "href": "1040-rcs.html#推荐阅读",
    "title": "41  R语言样条回归",
    "section": "41.1 推荐阅读",
    "text": "41.1 推荐阅读\n聂博士的10+篇高质量RCS合集：RCS系列合集"
  },
  {
    "objectID": "1041-subgroupanalysis.html#准备数据",
    "href": "1041-subgroupanalysis.html#准备数据",
    "title": "42  R语言亚组分析及森林图绘制",
    "section": "42.1 准备数据",
    "text": "42.1 准备数据\n使用survival包中的colon数据集用于演示，这是一份关于结肠癌患者的生存数据，共有1858行，16列，共分为3个组，1个观察组+2个治疗组，观察他们发生终点事件的差异。\n各变量的解释如下：\n\nid：患者id\nstudy：没啥用，所有患者都是1\nrx：治疗方法，共3种，Obs(观察组), Lev(左旋咪唑), Lev+5FU(左旋咪唑+5-FU)\nsex：性别，1是男性\nage：年龄\nobstruct：肠梗阻，1是有\nperfor：肠穿孔，1是有\nadhere：和附近器官粘连，1是有\nnodes：转移的淋巴结数量\nstatus：生存状态，0代表删失，1代表发生终点事件\ndiffer：肿瘤分化程度，1-well,2-moderate,3-poor\nextent：局部扩散情况，1-submucosa，2-muscle，3-serosa，4-contiguous structures\nsurg：手术后多久了，1-long,2-short\nnode4：是否有超过4个阳性淋巴结，1代表是\ntime：生存时间\netype：终点事件类型，1-复发，2-死亡\n\n\nrm(list = ls())\nlibrary(survival)\n\nstr(colon)\n## 'data.frame':    1858 obs. of  16 variables:\n##  $ id      : num  1 1 2 2 3 3 4 4 5 5 ...\n##  $ study   : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ rx      : Factor w/ 3 levels \"Obs\",\"Lev\",\"Lev+5FU\": 3 3 3 3 1 1 3 3 1 1 ...\n##  $ sex     : num  1 1 1 1 0 0 0 0 1 1 ...\n##  $ age     : num  43 43 63 63 71 71 66 66 69 69 ...\n##  $ obstruct: num  0 0 0 0 0 0 1 1 0 0 ...\n##  $ perfor  : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ adhere  : num  0 0 0 0 1 1 0 0 0 0 ...\n##  $ nodes   : num  5 5 1 1 7 7 6 6 22 22 ...\n##  $ status  : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ differ  : num  2 2 2 2 2 2 2 2 2 2 ...\n##  $ extent  : num  3 3 3 3 2 2 3 3 3 3 ...\n##  $ surg    : num  0 0 0 0 0 0 1 1 1 1 ...\n##  $ node4   : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ time    : num  1521 968 3087 3087 963 ...\n##  $ etype   : num  2 1 2 1 2 1 2 1 2 1 ...\n\n可以使用cox回归探索危险因素。分类变量需要变为因子型，这样在进行回归时会自动进行哑变量设置。\n为了演示，我们只选择Obs组和Lev+5FU组的患者，所有的分类变量都变为factor，把年龄也变为分类变量并变成factor。\n\nsuppressMessages(library(tidyverse))\n## Warning: package 'ggplot2' was built under R version 4.2.3\n## Warning: package 'tibble' was built under R version 4.2.3\n## Warning: package 'dplyr' was built under R version 4.2.3\n\ndf &lt;- colon %&gt;% \n  mutate(rx=as.numeric(rx)) %&gt;% \n  filter(etype == 1, !rx == 2) %&gt;%  #rx %in% c(\"Obs\",\"Lev+5FU\"), \n  select(time, status,rx, sex, age,obstruct,perfor,adhere,differ,extent,surg,node4) %&gt;% \n  mutate(sex=factor(sex, levels=c(0,1),labels=c(\"female\",\"male\")),\n         age=ifelse(age &gt;65,\"&gt;65\",\"&lt;=65\"),\n         age=factor(age, levels=c(\"&gt;65\",\"&lt;=65\")),\n         obstruct=factor(obstruct, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         perfor=factor(perfor, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         adhere=factor(adhere, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         differ=factor(differ, levels=c(1,2,3),labels=c(\"well\",\"moderate\",\"poor\")),\n         extent=factor(extent, levels=c(1,2,3,4),\n                       labels=c(\"submucosa\",\"muscle\",\"serosa\",\"contiguous\")),\n         surg=factor(surg, levels=c(0,1),labels=c(\"short\",\"long\")),\n         node4=factor(node4, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         rx=ifelse(rx==3,0,1),\n         rx=factor(rx,levels=c(0,1))\n         )\n\nstr(df)\n## 'data.frame':    619 obs. of  12 variables:\n##  $ time    : num  968 3087 542 245 523 ...\n##  $ status  : num  1 0 1 1 1 1 0 0 0 1 ...\n##  $ rx      : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 2 1 1 2 ...\n##  $ sex     : Factor w/ 2 levels \"female\",\"male\": 2 2 1 1 2 1 2 1 2 2 ...\n##  $ age     : Factor w/ 2 levels \"&gt;65\",\"&lt;=65\": 2 2 1 1 1 2 2 1 2 2 ...\n##  $ obstruct: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n##  $ perfor  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ adhere  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 1 1 1 1 1 ...\n##  $ differ  : Factor w/ 3 levels \"well\",\"moderate\",..: 2 2 2 2 2 2 2 2 3 2 ...\n##  $ extent  : Factor w/ 4 levels \"submucosa\",\"muscle\",..: 3 3 2 3 3 3 3 3 3 3 ...\n##  $ surg    : Factor w/ 2 levels \"short\",\"long\": 1 1 1 2 2 1 1 2 2 1 ...\n##  $ node4   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 1 1 1 1 ..."
  },
  {
    "objectID": "1041-subgroupanalysis.html#不分亚组的分析",
    "href": "1041-subgroupanalysis.html#不分亚组的分析",
    "title": "42  R语言亚组分析及森林图绘制",
    "section": "42.2 不分亚组的分析",
    "text": "42.2 不分亚组的分析\n直接使用所有数据，拟合多因素Cox回归模型：\n\nfit &lt;- coxph(Surv(time, status) ~ rx, data = df)\nbroom::tidy(fit,exponentiate = T,conf.int = T)\n## # A tibble: 1 × 7\n##   term  estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 rx1       1.67     0.119      4.32 0.0000156     1.32      2.11\n\n这个结果我们在之前说过无数遍了，各项意义就不做具体解释了。\n通过这个结果可以看出，Lev+5FU组是明显好于Obs组的，那么问题来了。\n有一个著名的东西叫辛普森悖论，这个方法对所有人有效，但是把这个方法单独对男人或女人使用，就没效了！\n这就是由于性别这个混杂因素导致的，控制混杂因素的方法，我们在医学统计系列推文中说过至少3种，今天就给大家演示最好理解的亚组分析。\n思路其实很简单，单独在男性患者中拟合模型看看结果是不是和所有患者的结果一样；然后单独在女性患者中也拟合模型。\n对于其他的分类变量，都是一样的操作。\n所以我说这个方法最简单，没有什么高深的数学理论，只是操作起来比较复杂，因为需要在每个分类变量的每个亚组中分别拟合模型。\n刚开始我是想通过嵌套for循环实现的，但是有点费脑子，所以我给大家演示下tidyverse的做法，后期会考虑再写个R包，实现这个功能。\n其实我已经找到了一个R包Publish可以实现回归分析的亚组分析，但是它的方法是错误的。。。\n通常最笨的方法也是最靠谱的方法，如果你实在不会，也可以手动实现这个过程，就以sex为例，先在male中拟合模型：\n\nfit0 &lt;- coxph(Surv(time, status) ~ rx, data = df[df$sex == \"male\",])\nbroom::tidy(fit0,exponentiate = T,conf.int = T)\n## # A tibble: 1 × 7\n##   term  estimate std.error statistic    p.value conf.low conf.high\n##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 rx1       2.29     0.181      4.57 0.00000495     1.60      3.26\n\n然后在female中拟合模型：\n\nfit0 &lt;- coxph(Surv(time, status) ~ rx, data = df[df$sex == \"female\",])\nbroom::tidy(fit0,exponentiate = T,conf.int = T)\n## # A tibble: 1 × 7\n##   term  estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 rx1       1.32     0.161      1.71  0.0878    0.960      1.80\n\n就这样不断的重复即可，然后把数据手动摘录一下。"
  },
  {
    "objectID": "1041-subgroupanalysis.html#亚组分析",
    "href": "1041-subgroupanalysis.html#亚组分析",
    "title": "42  R语言亚组分析及森林图绘制",
    "section": "42.3 亚组分析",
    "text": "42.3 亚组分析\n对于先分组，再做某事这种分析思路，tidyverse天生就比较擅长。\n以下是tidyverse实现方法，借助purrr。\n首先把数据变为长数据，经典的长宽转换：\n\ndfl &lt;- df %&gt;% \n  pivot_longer(cols = 4:ncol(.),names_to = \"var\",values_to = \"value\") %&gt;% \n  arrange(var)\n\nhead(dfl)\n## # A tibble: 6 × 5\n##    time status rx    var    value\n##   &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;  &lt;fct&gt;\n## 1   968      1 0     adhere No   \n## 2  3087      0 0     adhere No   \n## 3   542      1 1     adhere Yes  \n## 4   245      1 0     adhere No   \n## 5   523      1 1     adhere No   \n## 6   904      1 0     adhere No\n\n根据rx（治疗方式）和var（需要分亚组的变量）分组，分别在每个组内拟合cox回归，并提取结果，一气呵成，这个操作我们在之前的倾向性评分分层中也演示过：倾向性评分回归和分层\n\nress &lt;- dfl %&gt;% \n  #group_by(var,value) %&gt;% \n  group_nest(var,value) %&gt;% \n  drop_na(value) %&gt;% \n  mutate(#sample_size=map(data, ~ nrow(.x)),\n         model=map(data, ~ coxph(Surv(time, status) ~ rx,data = .x)),\n         res = map(model, broom::tidy,conf.int = T, exponentiate = T)\n         ) %&gt;% \n  dplyr::select(var,value,res)\n## Warning: There was 1 warning in `mutate()`.\n## ℹ In argument: `model = map(data, ~coxph(Surv(time, status) ~ rx, data = .x))`.\n## Caused by warning in `coxph.fit()`:\n## ! Loglik converged before variable  1 ; coefficient may be infinite.\n\nglimpse(ress)\n## Rows: 21\n## Columns: 3\n## $ var   &lt;chr&gt; \"adhere\", \"adhere\", \"age\", \"age\", \"differ\", \"differ\", \"differ\", …\n## $ value &lt;fct&gt; No, Yes, &gt;65, &lt;=65, well, moderate, poor, submucosa, muscle, ser…\n## $ res   &lt;list&gt; [&lt;tbl_df[1 x 7]&gt;], [&lt;tbl_df[1 x 7]&gt;], [&lt;tbl_df[1 x 7]&gt;], [&lt;tbl_…\n\nres是列表列，其中每个元素就是我们的结果。\n顺便把每个亚组中每种治疗方式的人数也一起计算出来：\n\nss &lt;- dfl %&gt;% \n  group_by(var,value,rx) %&gt;% \n  drop_na(value) %&gt;% \n  summarise(sample_size=n()) %&gt;% \n  dplyr::select(var,value,rx,sample_size)\n## `summarise()` has grouped output by 'var', 'value'. You can override using the\n## `.groups` argument.\n\n然后把两个结果合并到一起：\n\nresss &lt;- ress %&gt;% \n  left_join(ss,b=c(\"var\",\"value\")) %&gt;% \n  unnest(res,rx,sample_size) %&gt;% \n  pivot_wider(names_from = \"rx\",values_from = \"sample_size\",names_prefix = \"rx_\") %&gt;% \n  select(-c(term,std.error,statistic)) %&gt;% \n  mutate(across(where(is.numeric), round,digits=2)) %&gt;% \n  mutate(`HR(95%CI)`=paste0(estimate,\"(\",conf.low,\"-\",conf.high,\")\"))\n## Warning: `unnest()` has a new interface. See `?unnest` for details.\n## ℹ Try `df %&gt;% unnest(c(res, rx, sample_size))`, with `mutate()` if needed.\n## Warning: There was 1 warning in `mutate()`.\n## ℹ In argument: `across(where(is.numeric), round, digits = 2)`.\n## Caused by warning:\n## ! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\n## Supply arguments directly to `.fns` through an anonymous function instead.\n## \n##   # Previously\n##   across(a:b, mean, na.rm = TRUE)\n## \n##   # Now\n##   across(a:b, \\(x) mean(x, na.rm = TRUE))\n\nstr(resss)\n## tibble [21 × 9] (S3: tbl_df/tbl/data.frame)\n##  $ var      : chr [1:21] \"adhere\" \"adhere\" \"age\" \"age\" ...\n##  $ value    : Factor w/ 15 levels \"female\",\"male\",..: 5 6 3 4 7 8 9 10 11 12 ...\n##  $ estimate : num [1:21] 1.69 1.5 1.97 1.52 2.68 1.67 1.32 0 2.41 1.68 ...\n##  $ p.value  : num [1:21] 0 0.17 0 0 0.02 0 0.28 1 0.07 0 ...\n##  $ conf.low : num [1:21] 1.31 0.84 1.33 1.14 1.19 1.26 0.8 0 0.93 1.31 ...\n##  $ conf.high: num [1:21] 2.18 2.67 2.93 2.03 6.02 ...\n##  $ rx_0     : num [1:21] 265 39 114 190 29 215 54 10 32 251 ...\n##  $ rx_1     : num [1:21] 268 47 110 205 27 229 52 8 38 249 ...\n##  $ HR(95%CI): chr [1:21] \"1.69(1.31-2.18)\" \"1.5(0.84-2.67)\" \"1.97(1.33-2.93)\" \"1.52(1.14-2.03)\" ...\n\nhead(resss)\n## # A tibble: 6 × 9\n##   var    value    estimate p.value conf.low conf.high  rx_0  rx_1 `HR(95%CI)`   \n##   &lt;chr&gt;  &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         \n## 1 adhere No           1.69    0        1.31      2.18   265   268 1.69(1.31-2.1…\n## 2 adhere Yes          1.5     0.17     0.84      2.67    39    47 1.5(0.84-2.67)\n## 3 age    &gt;65          1.97    0        1.33      2.93   114   110 1.97(1.33-2.9…\n## 4 age    &lt;=65         1.52    0        1.14      2.03   190   205 1.52(1.14-2.0…\n## 5 differ well         2.68    0.02     1.19      6.02    29    27 2.68(1.19-6.0…\n## 6 differ moderate     1.67    0        1.26      2.21   215   229 1.67(1.26-2.2…\n\n这样亚组分析就做好了，HR，可信区间，P值，每个组的人数都有了，还记得前面做的整体的结果吗，我们把它合并进来，方便后面画森林图用。\n\nfit &lt;- coxph(Surv(time, status) ~ rx, data = df)\nres_all &lt;- broom::tidy(fit,exponentiate = T,conf.int = T)\n\n#看下不同治疗组的人数\ndf %&gt;% count(rx)\n##   rx   n\n## 1  0 304\n## 2  1 315\n\nres_all &lt;- res_all %&gt;% \n  mutate(var=\"All people\",\n         value=\" \",\n         rx_0=304,\n         rx_1=305,\n         across(where(is.numeric), round,digits=2)\n         ) %&gt;% \n  mutate(`HR(95%CI)`=paste0(estimate,\"(\",conf.low,\"-\",conf.high,\")\")\n         ) %&gt;% \n  select(var,value,estimate,p.value,conf.low,conf.high,rx_0,rx_1,`HR(95%CI)`)\n\nres_all\n## # A tibble: 1 × 9\n##   var        value estimate p.value conf.low conf.high  rx_0  rx_1 `HR(95%CI)`  \n##   &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n## 1 All people \" \"       1.67       0     1.32      2.11   304   305 1.67(1.32-2.…\n\n合并到一起：\n\nresss &lt;- bind_rows(res_all,resss)\nhead(resss)\n## # A tibble: 6 × 9\n##   var        value  estimate p.value conf.low conf.high  rx_0  rx_1 `HR(95%CI)` \n##   &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n## 1 All people \" \"        1.67    0        1.32      2.11   304   305 1.67(1.32-2…\n## 2 adhere     \"No\"       1.69    0        1.31      2.18   265   268 1.69(1.31-2…\n## 3 adhere     \"Yes\"      1.5     0.17     0.84      2.67    39    47 1.5(0.84-2.…\n## 4 age        \"&gt;65\"      1.97    0        1.33      2.93   114   110 1.97(1.33-2…\n## 5 age        \"&lt;=65\"     1.52    0        1.14      2.03   190   205 1.52(1.14-2…\n## 6 differ     \"well\"     2.68    0.02     1.19      6.02    29    27 2.68(1.19-6…\n\n到这里所有数据就都准备好了！下面只要整理下格式，画图即可。\n但是forestploter包画森林图的格式还是蛮复杂的，所以我们直接另存为csv，用excel修改好，再读进来。\n\nwrite.csv(resss, file = \"resss.csv\",quote = F,row.names = T)"
  },
  {
    "objectID": "1041-subgroupanalysis.html#画森林图",
    "href": "1041-subgroupanalysis.html#画森林图",
    "title": "42  R语言亚组分析及森林图绘制",
    "section": "42.4 画森林图",
    "text": "42.4 画森林图\n把数据整理成这样：\n\n\n\n\n\n\n\n\n\n还有一些细节你可以自己修改下，比如各个亚组的顺序，首字母大写，各个变体的大小写，分组变量的名字，把P值为0的改成&lt;0.0001，等。我就不改了\n然后读取进来：\n\nplot_df &lt;- read.csv(file = \"datasets/resss.csv\",check.names = F)\nplot_df\n##         Subgroup estimate p.value conf.low conf.high rx_0 rx_1        HR(95%CI)\n## 1     All people     1.67    0.00     1.32      2.11  304  305  1.67(1.32-2.11)\n## 2         adhere       NA      NA       NA        NA   NA   NA                 \n## 3             No     1.69    0.00     1.31      2.18  265  268  1.69(1.31-2.18)\n## 4            Yes     1.50    0.17     0.84      2.67   39   47   1.5(0.84-2.67)\n## 5            age       NA      NA       NA        NA   NA   NA                 \n## 6            &gt;65     1.97    0.00     1.33      2.93  114  110  1.97(1.33-2.93)\n## 7           &lt;=65     1.52    0.00     1.14      2.03  190  205  1.52(1.14-2.03)\n## 8         differ       NA      NA       NA        NA   NA   NA                 \n## 9           well     2.68    0.02     1.19      6.02   29   27  2.68(1.19-6.02)\n## 10      moderate     1.67    0.00     1.26      2.21  215  229  1.67(1.26-2.21)\n## 11          poor     1.32    0.28     0.80      2.19   54   52   1.32(0.8-2.19)\n## 12        extent       NA      NA       NA        NA   NA   NA                 \n## 13     submucosa     0.00    1.00     0.00       Inf   10    8         0(0-Inf)\n## 14        muscle     2.41    0.07     0.93      6.22   32   38  2.41(0.93-6.22)\n## 15        serosa     1.68    0.00     1.31      2.16  251  249  1.68(1.31-2.16)\n## 16    contiguous     1.44    0.46     0.55      3.75   11   20  1.44(0.55-3.75)\n## 17         node4       NA      NA       NA        NA   NA   NA                 \n## 18            No     1.85    0.00     1.37      2.49  225  228  1.85(1.37-2.49)\n## 19           Yes     1.41    0.07     0.97      2.05   79   87  1.41(0.97-2.05)\n## 20      obstruct       NA      NA       NA        NA   NA   NA                 \n## 21            No     1.65    0.00     1.27      2.13  250  252  1.65(1.27-2.13)\n## 22           Yes     1.73    0.05     1.01      2.95   54   63  1.73(1.01-2.95)\n## 23        perfor       NA      NA       NA        NA   NA   NA                 \n## 24            No     1.64    0.00     1.30      2.08  296  306   1.64(1.3-2.08)\n## 25           Yes     2.87    0.13     0.74     11.21    8    9 2.87(0.74-11.21)\n## 26           sex       NA      NA       NA        NA   NA   NA                 \n## 27        female     1.32    0.09     0.96      1.80  163  149   1.32(0.96-1.8)\n## 28          male     2.29    0.00     1.60      3.26  141  166   2.29(1.6-3.26)\n## 29          surg       NA      NA       NA        NA   NA   NA                 \n## 30         short     1.82    0.00     1.37      2.40  228  224   1.82(1.37-2.4)\n## 31          long     1.31    0.21     0.86      1.99   76   91  1.31(0.86-1.99)\n\n把数据中的说明部分的NA变成空格，这样画森林图时就不会显示了，然后增加1列空值用于展示可信区间：\n\nplot_df[,c(3,6,7)][is.na(plot_df[,c(3,6,7)])] &lt;- \" \"\nplot_df$` ` &lt;- paste(rep(\" \", nrow(plot_df)), collapse = \" \")\nplot_df\n##         Subgroup estimate p.value conf.low conf.high rx_0 rx_1        HR(95%CI)\n## 1     All people     1.67       0     1.32      2.11  304  305  1.67(1.32-2.11)\n## 2         adhere       NA               NA        NA                           \n## 3             No     1.69       0     1.31      2.18  265  268  1.69(1.31-2.18)\n## 4            Yes     1.50    0.17     0.84      2.67   39   47   1.5(0.84-2.67)\n## 5            age       NA               NA        NA                           \n## 6            &gt;65     1.97       0     1.33      2.93  114  110  1.97(1.33-2.93)\n## 7           &lt;=65     1.52       0     1.14      2.03  190  205  1.52(1.14-2.03)\n## 8         differ       NA               NA        NA                           \n## 9           well     2.68    0.02     1.19      6.02   29   27  2.68(1.19-6.02)\n## 10      moderate     1.67       0     1.26      2.21  215  229  1.67(1.26-2.21)\n## 11          poor     1.32    0.28     0.80      2.19   54   52   1.32(0.8-2.19)\n## 12        extent       NA               NA        NA                           \n## 13     submucosa     0.00       1     0.00       Inf   10    8         0(0-Inf)\n## 14        muscle     2.41    0.07     0.93      6.22   32   38  2.41(0.93-6.22)\n## 15        serosa     1.68       0     1.31      2.16  251  249  1.68(1.31-2.16)\n## 16    contiguous     1.44    0.46     0.55      3.75   11   20  1.44(0.55-3.75)\n## 17         node4       NA               NA        NA                           \n## 18            No     1.85       0     1.37      2.49  225  228  1.85(1.37-2.49)\n## 19           Yes     1.41    0.07     0.97      2.05   79   87  1.41(0.97-2.05)\n## 20      obstruct       NA               NA        NA                           \n## 21            No     1.65       0     1.27      2.13  250  252  1.65(1.27-2.13)\n## 22           Yes     1.73    0.05     1.01      2.95   54   63  1.73(1.01-2.95)\n## 23        perfor       NA               NA        NA                           \n## 24            No     1.64       0     1.30      2.08  296  306   1.64(1.3-2.08)\n## 25           Yes     2.87    0.13     0.74     11.21    8    9 2.87(0.74-11.21)\n## 26           sex       NA               NA        NA                           \n## 27        female     1.32    0.09     0.96      1.80  163  149   1.32(0.96-1.8)\n## 28          male     2.29       0     1.60      3.26  141  166   2.29(1.6-3.26)\n## 29          surg       NA               NA        NA                           \n## 30         short     1.82       0     1.37      2.40  228  224   1.82(1.37-2.4)\n## 31          long     1.31    0.21     0.86      1.99   76   91  1.31(0.86-1.99)\n##                                                                 \n## 1                                                               \n## 2                                                               \n## 3                                                               \n## 4                                                               \n## 5                                                               \n## 6                                                               \n## 7                                                               \n## 8                                                               \n## 9                                                               \n## 10                                                              \n## 11                                                              \n## 12                                                              \n## 13                                                              \n## 14                                                              \n## 15                                                              \n## 16                                                              \n## 17                                                              \n## 18                                                              \n## 19                                                              \n## 20                                                              \n## 21                                                              \n## 22                                                              \n## 23                                                              \n## 24                                                              \n## 25                                                              \n## 26                                                              \n## 27                                                              \n## 28                                                              \n## 29                                                              \n## 30                                                              \n## 31\n\n然后画图即可，默认的出图就已经很美观了，但是大家要注意，这里每个组的人数和开头那张图的每个组的人数稍有不同哦~\n\nlibrary(forestploter)\n## Warning: package 'forestploter' was built under R version 4.2.3\nlibrary(grid)\n\np &lt;- forest(\n  data = plot_df[,c(1,6,7,9,8,3)],\n  lower = plot_df$conf.low,\n  upper = plot_df$conf.high,\n  est = plot_df$estimate,\n  ci_column = 4,\n  sizes = (plot_df$estimate+0.001)*0.3, \n  ref_line = 1, \n  xlim = c(0.1,4)\n  )\nprint(p)\n\n\n\n\n如果你还需要美化，我们在之前也详细介绍过这个包的使用细节了：\n\n画一个好看的森林图\n用更简单的方式画森林图\n森林图展示回归系数\nggforestplot绘制森林图\nR语言画误差线的5种方法\nggplot2绘制森林图(有亚组和没亚组)\n\n下面是我们美化后的森林图，其实变化不是非常大，只要数据好，默认的图形也很好看：\n\n\n\n\n\n\n\n\n\n和开头那张NEJM的风格一模一样！\n\npdf(\"aaa.pdf\",width = 8,height = 10)\ntm &lt;- forest_theme(base_size = 12, # 基础大小\n                   # 可信区间点的形状，线型、颜色、宽度\n                   #ci_lty = 1,\n                   ci_lwd = 1.5,\n                   #ci_Theight = 0.2, # 可信区间两端加短竖线\n                   # 参考线宽度、形状、颜色\n                   refline_lwd = 1.5,\n                   refline_lty = \"dashed\",\n                   refline_col = \"grey20\",\n                   # 汇总菱形的填充色和边框色\n                   #summary_fill = \"#4575b4\",\n                   #summary_col = \"#4575b4\",\n                   # 脚注大小、字体、颜色\n                   footnote_cex = 0.8,\n                   footnote_fontface = \"italic\",\n                   footnote_col = \"grey30\",\n                   # 自定义背景色、前景色。fontface:1常规，2粗体，3斜体，4粗斜体 \n                   core = list(bg_params = list(fill = c(\"#FFFFFF\",\"#f5f7f6\"), col=NA))\n                   )\n\np &lt;- forest(\n  data = plot_df[,c(1,6,7,9,8,3)],\n  lower = plot_df$conf.low,\n  upper = plot_df$conf.high,\n  est = plot_df$estimate,\n  ci_column = 4,\n  sizes = (plot_df$estimate+0.001)*0.3, # 不能是负值或NA，而且不能太大\n  ref_line = 1, # 把竖线放到1的位置\n  xlim = c(0.1,4), # x轴范围,如果有的可信区间超过这个范围会显示为箭头\n  arrow_lab = c(\"Obs better\",\"Lev+5-FU better\"), # x轴下面的文字\n  theme = tm\n  )\nprint(p)\ndev.off()"
  },
  {
    "objectID": "1042-subgroup1code.html#安装",
    "href": "1042-subgroup1code.html#安装",
    "title": "43  亚组分析1行代码实现",
    "section": "43.1 安装",
    "text": "43.1 安装\n\ninstall.packages(\"jstable\")\n\n## From github: latest version\nremotes::install_github('jinseob2kim/jstable')\nlibrary(jstable)"
  },
  {
    "objectID": "1042-subgroup1code.html#准备数据",
    "href": "1042-subgroup1code.html#准备数据",
    "title": "43  亚组分析1行代码实现",
    "section": "43.2 准备数据",
    "text": "43.2 准备数据\n还是使用之前演示的数据。\n使用survival包中的colon数据集用于演示，这是一份关于结肠癌患者的生存数据，共有1858行，16列，共分为3个组，1个观察组+2个治疗组，观察他们发生终点事件的差异。\n各变量的解释如下：\n\nid：患者id\nstudy：没啥用，所有患者都是1\nrx：治疗方法，共3种，Obs(观察组), Lev(左旋咪唑), Lev+5FU(左旋咪唑+5-FU)\nsex：性别，1是男性\nage：年龄\nobstruct：肠梗阻，1是有\nperfor：肠穿孔，1是有\nadhere：和附近器官粘连，1是有\nnodes：转移的淋巴结数量\nstatus：生存状态，0代表删失，1代表发生终点事件\ndiffer：肿瘤分化程度，1-well,2-moderate,3-poor\nextent：局部扩散情况，1-submucosa，2-muscle，3-serosa，4-contiguous structures\nsurg：手术后多久了，1-long,2-short\nnode4：是否有超过4个阳性淋巴结，1代表是\ntime：生存时间\netype：终点事件类型，1-复发，2-死亡\n\n\nrm(list = ls())\nlibrary(survival)\n\nstr(colon)\n## 'data.frame':    1858 obs. of  16 variables:\n##  $ id      : num  1 1 2 2 3 3 4 4 5 5 ...\n##  $ study   : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ rx      : Factor w/ 3 levels \"Obs\",\"Lev\",\"Lev+5FU\": 3 3 3 3 1 1 3 3 1 1 ...\n##  $ sex     : num  1 1 1 1 0 0 0 0 1 1 ...\n##  $ age     : num  43 43 63 63 71 71 66 66 69 69 ...\n##  $ obstruct: num  0 0 0 0 0 0 1 1 0 0 ...\n##  $ perfor  : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ adhere  : num  0 0 0 0 1 1 0 0 0 0 ...\n##  $ nodes   : num  5 5 1 1 7 7 6 6 22 22 ...\n##  $ status  : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ differ  : num  2 2 2 2 2 2 2 2 2 2 ...\n##  $ extent  : num  3 3 3 3 2 2 3 3 3 3 ...\n##  $ surg    : num  0 0 0 0 0 0 1 1 1 1 ...\n##  $ node4   : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ time    : num  1521 968 3087 3087 963 ...\n##  $ etype   : num  2 1 2 1 2 1 2 1 2 1 ...\n\n可以使用cox回归探索危险因素。分类变量需要变为因子型，这样在进行回归时会自动进行哑变量设置。\n为了演示，我们只选择Obs组和Lev+5FU组的患者，所有的分类变量都变为factor，把年龄也变为分类变量并变成factor。\n\nsuppressMessages(library(tidyverse))\n## Warning: package 'ggplot2' was built under R version 4.2.3\n## Warning: package 'tibble' was built under R version 4.2.3\n## Warning: package 'dplyr' was built under R version 4.2.3\n\ndf &lt;- colon %&gt;% \n  mutate(rx=as.numeric(rx)) %&gt;% \n  filter(etype == 1, !rx == 2) %&gt;%  #rx %in% c(\"Obs\",\"Lev+5FU\"), \n  select(time, status,rx, sex, age,obstruct,perfor,adhere,differ,extent,surg,node4) %&gt;% \n  mutate(sex=factor(sex, levels=c(0,1),labels=c(\"female\",\"male\")),\n         age=ifelse(age &gt;65,\"&gt;65\",\"&lt;=65\"),\n         age=factor(age, levels=c(\"&gt;65\",\"&lt;=65\")),\n         obstruct=factor(obstruct, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         perfor=factor(perfor, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         adhere=factor(adhere, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         differ=factor(differ, levels=c(1,2,3),labels=c(\"well\",\"moderate\",\"poor\")),\n         extent=factor(extent, levels=c(1,2,3,4),\n                       labels=c(\"submucosa\",\"muscle\",\"serosa\",\"contiguous\")),\n         surg=factor(surg, levels=c(0,1),labels=c(\"short\",\"long\")),\n         node4=factor(node4, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         rx=ifelse(rx==3,0,1),\n         rx=factor(rx,levels=c(0,1))\n         )\n\nstr(df)\n## 'data.frame':    619 obs. of  12 variables:\n##  $ time    : num  968 3087 542 245 523 ...\n##  $ status  : num  1 0 1 1 1 1 0 0 0 1 ...\n##  $ rx      : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 2 1 1 2 ...\n##  $ sex     : Factor w/ 2 levels \"female\",\"male\": 2 2 1 1 2 1 2 1 2 2 ...\n##  $ age     : Factor w/ 2 levels \"&gt;65\",\"&lt;=65\": 2 2 1 1 1 2 2 1 2 2 ...\n##  $ obstruct: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n##  $ perfor  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ adhere  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 1 1 1 1 1 ...\n##  $ differ  : Factor w/ 3 levels \"well\",\"moderate\",..: 2 2 2 2 2 2 2 2 3 2 ...\n##  $ extent  : Factor w/ 4 levels \"submucosa\",\"muscle\",..: 3 3 2 3 3 3 3 3 3 3 ...\n##  $ surg    : Factor w/ 2 levels \"short\",\"long\": 1 1 1 2 2 1 1 2 2 1 ...\n##  $ node4   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 1 1 1 1 ..."
  },
  {
    "objectID": "1042-subgroup1code.html#亚组分析",
    "href": "1042-subgroup1code.html#亚组分析",
    "title": "43  亚组分析1行代码实现",
    "section": "43.3 亚组分析",
    "text": "43.3 亚组分析\n使用jstable，只要1行代码即可！！！\n\nlibrary(jstable)\n## Warning: package 'jstable' was built under R version 4.2.3\n\nres &lt;- TableSubgroupMultiCox(\n  \n  # 指定公式\n  formula = Surv(time, status) ~ rx, \n  \n  # 指定哪些变量有亚组\n  var_subgroups = c(\"sex\",\"age\",\"obstruct\",\"perfor\",\"adhere\",\n                    \"differ\",\"extent\",\"surg\",\"node4\"), \n  data = df #指定你的数据\n  )\n## Warning in coxph.fit(X, Y, istrat, offset, init, control, weights = weights, :\n## Loglik converged before variable 1 ; coefficient may be infinite.\n## Warning in coxph.fit(X, Y, istrat, offset, init, control, weights = weights, :\n## Loglik converged before variable 1,5,6,7 ; coefficient may be infinite.\nres\n##                 Variable Count Percent Point Estimate Lower Upper    0    1\n## rx1              Overall   619     100           1.67  1.32  2.11 34.4 48.9\n## 1...1                sex  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n## female            female   312    50.4           1.32  0.96   1.8 41.1 47.8\n## male                male   307    49.6           2.29   1.6  3.26 26.6 50.1\n## 1...4                age  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n## &gt;65                  &gt;65   224    36.2           1.97  1.33  2.93 29.8 50.6\n## &lt;=65                &lt;=65   395    63.8           1.52  1.14  2.03   37 48.1\n## 1...7           obstruct  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n## No...8                No   502    81.1           1.65  1.27  2.13 34.4 46.8\n## Yes...9              Yes   117    18.9           1.73  1.01  2.95 34.2 57.6\n## 1...10            perfor  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n## No...11               No   602    97.3           1.64   1.3  2.08 34.3 48.1\n## Yes...12             Yes    17     2.7           2.87  0.74 11.21 37.5 77.8\n## 1...13            adhere  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n## No...14               No   533    86.1           1.69  1.31  2.18 32.9 47.4\n## Yes...15             Yes    86    13.9            1.5  0.84  2.67 44.4 57.9\n## 1...16            differ  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n## well                well    56     9.2           2.68  1.19  6.02 &lt;NA&gt; &lt;NA&gt;\n## moderate        moderate   444    73.3           1.67  1.26  2.21 &lt;NA&gt; &lt;NA&gt;\n## poor                poor   106    17.5           1.32   0.8  2.19 &lt;NA&gt; &lt;NA&gt;\n## 1...20            extent  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n## submucosa      submucosa    18     2.9              0     0   Inf &lt;NA&gt; &lt;NA&gt;\n## muscle            muscle    70    11.3           2.41  0.93  6.22 &lt;NA&gt; &lt;NA&gt;\n## serosa            serosa   500    80.8           1.68  1.31  2.16 &lt;NA&gt; &lt;NA&gt;\n## contiguous    contiguous    31       5           1.44  0.55  3.75 &lt;NA&gt; &lt;NA&gt;\n## 1...25              surg  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n## short              short   452      73           1.82  1.37   2.4 31.5 48.9\n## long                long   167      27           1.31  0.86  1.99 43.2 49.1\n## 1...28             node4  &lt;NA&gt;    &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n## No...29               No   453    73.2           1.85  1.37  2.49 27.7 41.5\n## Yes...30             Yes   166    26.8           1.41  0.97  2.05 53.2 68.7\n##            P value P for interaction\n## rx1         &lt;0.001              &lt;NA&gt;\n## 1...1         &lt;NA&gt;             0.029\n## female       0.088              &lt;NA&gt;\n## male        &lt;0.001              &lt;NA&gt;\n## 1...4         &lt;NA&gt;             0.316\n## &gt;65          0.001              &lt;NA&gt;\n## &lt;=65         0.004              &lt;NA&gt;\n## 1...7         &lt;NA&gt;             0.752\n## No...8      &lt;0.001              &lt;NA&gt;\n## Yes...9      0.046              &lt;NA&gt;\n## 1...10        &lt;NA&gt;             0.442\n## No...11     &lt;0.001              &lt;NA&gt;\n## Yes...12     0.129              &lt;NA&gt;\n## 1...13        &lt;NA&gt;             0.756\n## No...14     &lt;0.001              &lt;NA&gt;\n## Yes...15     0.173              &lt;NA&gt;\n## 1...16        &lt;NA&gt;             0.402\n## well         0.017              &lt;NA&gt;\n## moderate    &lt;0.001              &lt;NA&gt;\n## poor         0.277              &lt;NA&gt;\n## 1...20        &lt;NA&gt;               0.1\n## submucosa    0.999              &lt;NA&gt;\n## muscle       0.069              &lt;NA&gt;\n## serosa      &lt;0.001              &lt;NA&gt;\n## contiguous   0.459              &lt;NA&gt;\n## 1...25        &lt;NA&gt;             0.183\n## short       &lt;0.001              &lt;NA&gt;\n## long         0.208              &lt;NA&gt;\n## 1...28        &lt;NA&gt;             0.338\n## No...29     &lt;0.001              &lt;NA&gt;\n## Yes...30     0.074              &lt;NA&gt;\n\n直接就得出了结果！除了亚组分析的各种结果，还给出了交互作用的P值！"
  },
  {
    "objectID": "1042-subgroup1code.html#画森林图",
    "href": "1042-subgroup1code.html#画森林图",
    "title": "43  亚组分析1行代码实现",
    "section": "43.4 画森林图",
    "text": "43.4 画森林图\n这个结果不需要另存为csv也能直接使用（除非你是细节控，需要修改各种大小写等信息），当然如果你需要HR(95%CI)这种信息，还是需要自己添加一下的。\n我们添加个空列用于显示可信区间，并把不想显示的NA去掉即可，还需要把P值，可信区间这些列变为数值型。\n\nplot_df &lt;- res\nplot_df[,c(2,3,9)][is.na(plot_df[,c(2,3,9)])] &lt;- \" \"\nplot_df$` ` &lt;- paste(rep(\" \", nrow(plot_df)), collapse = \" \")\nplot_df[,4:6] &lt;- apply(plot_df[,4:6],2,as.numeric)\nplot_df\n##                 Variable Count Percent Point Estimate Lower Upper    0    1\n## rx1              Overall   619     100           1.67  1.32  2.11 34.4 48.9\n## 1...1                sex                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;\n## female            female   312    50.4           1.32  0.96  1.80 41.1 47.8\n## male                male   307    49.6           2.29  1.60  3.26 26.6 50.1\n## 1...4                age                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;\n## &gt;65                  &gt;65   224    36.2           1.97  1.33  2.93 29.8 50.6\n## &lt;=65                &lt;=65   395    63.8           1.52  1.14  2.03   37 48.1\n## 1...7           obstruct                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;\n## No...8                No   502    81.1           1.65  1.27  2.13 34.4 46.8\n## Yes...9              Yes   117    18.9           1.73  1.01  2.95 34.2 57.6\n## 1...10            perfor                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;\n## No...11               No   602    97.3           1.64  1.30  2.08 34.3 48.1\n## Yes...12             Yes    17     2.7           2.87  0.74 11.21 37.5 77.8\n## 1...13            adhere                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;\n## No...14               No   533    86.1           1.69  1.31  2.18 32.9 47.4\n## Yes...15             Yes    86    13.9           1.50  0.84  2.67 44.4 57.9\n## 1...16            differ                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;\n## well                well    56     9.2           2.68  1.19  6.02 &lt;NA&gt; &lt;NA&gt;\n## moderate        moderate   444    73.3           1.67  1.26  2.21 &lt;NA&gt; &lt;NA&gt;\n## poor                poor   106    17.5           1.32  0.80  2.19 &lt;NA&gt; &lt;NA&gt;\n## 1...20            extent                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;\n## submucosa      submucosa    18     2.9           0.00  0.00   Inf &lt;NA&gt; &lt;NA&gt;\n## muscle            muscle    70    11.3           2.41  0.93  6.22 &lt;NA&gt; &lt;NA&gt;\n## serosa            serosa   500    80.8           1.68  1.31  2.16 &lt;NA&gt; &lt;NA&gt;\n## contiguous    contiguous    31       5           1.44  0.55  3.75 &lt;NA&gt; &lt;NA&gt;\n## 1...25              surg                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;\n## short              short   452      73           1.82  1.37  2.40 31.5 48.9\n## long                long   167      27           1.31  0.86  1.99 43.2 49.1\n## 1...28             node4                           NA    NA    NA &lt;NA&gt; &lt;NA&gt;\n## No...29               No   453    73.2           1.85  1.37  2.49 27.7 41.5\n## Yes...30             Yes   166    26.8           1.41  0.97  2.05 53.2 68.7\n##            P value P for interaction\n## rx1         &lt;0.001              &lt;NA&gt;\n## 1...1                          0.029\n## female       0.088              &lt;NA&gt;\n## male        &lt;0.001              &lt;NA&gt;\n## 1...4                          0.316\n## &gt;65          0.001              &lt;NA&gt;\n## &lt;=65         0.004              &lt;NA&gt;\n## 1...7                          0.752\n## No...8      &lt;0.001              &lt;NA&gt;\n## Yes...9      0.046              &lt;NA&gt;\n## 1...10                         0.442\n## No...11     &lt;0.001              &lt;NA&gt;\n## Yes...12     0.129              &lt;NA&gt;\n## 1...13                         0.756\n## No...14     &lt;0.001              &lt;NA&gt;\n## Yes...15     0.173              &lt;NA&gt;\n## 1...16                         0.402\n## well         0.017              &lt;NA&gt;\n## moderate    &lt;0.001              &lt;NA&gt;\n## poor         0.277              &lt;NA&gt;\n## 1...20                           0.1\n## submucosa    0.999              &lt;NA&gt;\n## muscle       0.069              &lt;NA&gt;\n## serosa      &lt;0.001              &lt;NA&gt;\n## contiguous   0.459              &lt;NA&gt;\n## 1...25                         0.183\n## short       &lt;0.001              &lt;NA&gt;\n## long         0.208              &lt;NA&gt;\n## 1...28                         0.338\n## No...29     &lt;0.001              &lt;NA&gt;\n## Yes...30     0.074              &lt;NA&gt;\n##                                                                         \n## rx1                                                                     \n## 1...1                                                                   \n## female                                                                  \n## male                                                                    \n## 1...4                                                                   \n## &gt;65                                                                     \n## &lt;=65                                                                    \n## 1...7                                                                   \n## No...8                                                                  \n## Yes...9                                                                 \n## 1...10                                                                  \n## No...11                                                                 \n## Yes...12                                                                \n## 1...13                                                                  \n## No...14                                                                 \n## Yes...15                                                                \n## 1...16                                                                  \n## well                                                                    \n## moderate                                                                \n## poor                                                                    \n## 1...20                                                                  \n## submucosa                                                               \n## muscle                                                                  \n## serosa                                                                  \n## contiguous                                                              \n## 1...25                                                                  \n## short                                                                   \n## long                                                                    \n## 1...28                                                                  \n## No...29                                                                 \n## Yes...30\n\n画图就非常简单！\n\nlibrary(forestploter)\nlibrary(grid)\n\np &lt;- forest(\n  data = plot_df[,c(1,2,3,11,9)],\n  lower = plot_df$Lower,\n  upper = plot_df$Upper,\n  est = plot_df$`Point Estimate`,\n  ci_column = 4,\n  #sizes = (plot_df$estimate+0.001)*0.3, \n  ref_line = 1, \n  xlim = c(0.1,4)\n  )\nprint(p)\n\n\n\n\n\n\n\n\n\n\n这样就搞定了，真的是非常简单了，省去了大量的步骤。"
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#准备数据",
    "href": "亚组分析和多因素回归的森林图.html#准备数据",
    "title": "44  亚组分析和多因素回归的森林图",
    "section": "44.1 准备数据",
    "text": "44.1 准备数据\n使用survival包中的colon数据集用于演示，这是一份关于结肠癌患者的生存数据，共有1858行，16列，共分为3个组，1个观察组+2个治疗组，观察他们发生终点事件的差异。\n各变量的解释如下：\n\nid：患者id\nstudy：没啥用，所有患者都是1\nrx：治疗方法，共3种，Obs(观察组), Lev(左旋咪唑), Lev+5FU(左旋咪唑+5-FU)\nsex：性别，1是男性\nage：年龄\nobstruct：肠梗阻，1是有\nperfor：肠穿孔，1是有\nadhere：和附近器官粘连，1是有\nnodes：转移的淋巴结数量\nstatus：生存状态，0代表删失，1代表发生终点事件\ndiffer：肿瘤分化程度，1-well,2-moderate,3-poor\nextent：局部扩散情况，1-submucosa，2-muscle，3-serosa，4-contiguous_structures\nsurg：手术后多久了，1-long,2-short\nnode4：是否有超过4个阳性淋巴结，1代表是\ntime：生存时间\netype：终点事件类型，1-复发，2-死亡\n\n\nrm(list = ls())\nlibrary(survival)\n\nstr(colon)\n## 'data.frame':    1858 obs. of  16 variables:\n##  $ id      : num  1 1 2 2 3 3 4 4 5 5 ...\n##  $ study   : num  1 1 1 1 1 1 1 1 1 1 ...\n##  $ rx      : Factor w/ 3 levels \"Obs\",\"Lev\",\"Lev+5FU\": 3 3 3 3 1 1 3 3 1 1 ...\n##  $ sex     : num  1 1 1 1 0 0 0 0 1 1 ...\n##  $ age     : num  43 43 63 63 71 71 66 66 69 69 ...\n##  $ obstruct: num  0 0 0 0 0 0 1 1 0 0 ...\n##  $ perfor  : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ adhere  : num  0 0 0 0 1 1 0 0 0 0 ...\n##  $ nodes   : num  5 5 1 1 7 7 6 6 22 22 ...\n##  $ status  : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ differ  : num  2 2 2 2 2 2 2 2 2 2 ...\n##  $ extent  : num  3 3 3 3 2 2 3 3 3 3 ...\n##  $ surg    : num  0 0 0 0 0 0 1 1 1 1 ...\n##  $ node4   : num  1 1 0 0 1 1 1 1 1 1 ...\n##  $ time    : num  1521 968 3087 3087 963 ...\n##  $ etype   : num  2 1 2 1 2 1 2 1 2 1 ...\n\n为了演示，我们只选择Obs组和Lev+5FU组的患者，所有的分类变量都变为factor，把年龄也变为分类变量并变成factor。\n\nlibrary(tidyverse)\n\ndf &lt;- colon %&gt;% \n  mutate(rx=as.numeric(rx)) %&gt;% \n  filter(etype == 1, !rx == 2) %&gt;%  #rx %in% c(\"Obs\",\"Lev+5FU\"), \n  select(time, status,rx, sex, age,obstruct,perfor,adhere,differ,extent,surg,node4) %&gt;% \n  mutate(sex=factor(sex, levels=c(0,1),labels=c(\"female\",\"male\")),\n         age=ifelse(age &gt;65,\"&gt;65\",\"&lt;=65\"),\n         age=factor(age, levels=c(\"&gt;65\",\"&lt;=65\")),\n         obstruct=factor(obstruct, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         perfor=factor(perfor, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         adhere=factor(adhere, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         differ=factor(differ, levels=c(1,2,3),labels=c(\"well\",\"moderate\",\"poor\")),\n         extent=factor(extent, levels=c(1,2,3,4),\n                       labels=c(\"submucosa\",\"muscle\",\"serosa\",\"contiguous\")),\n         surg=factor(surg, levels=c(0,1),labels=c(\"short\",\"long\")),\n         node4=factor(node4, levels=c(0,1),labels=c(\"No\",\"Yes\")),\n         rx=ifelse(rx==3,0,1),\n         rx=factor(rx,levels=c(0,1))\n         )\n\nstr(df)\n## 'data.frame':    619 obs. of  12 variables:\n##  $ time    : num  968 3087 542 245 523 ...\n##  $ status  : num  1 0 1 1 1 1 0 0 0 1 ...\n##  $ rx      : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 2 1 1 2 ...\n##  $ sex     : Factor w/ 2 levels \"female\",\"male\": 2 2 1 1 2 1 2 1 2 2 ...\n##  $ age     : Factor w/ 2 levels \"&gt;65\",\"&lt;=65\": 2 2 1 1 1 2 2 1 2 2 ...\n##  $ obstruct: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n##  $ perfor  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ adhere  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 1 1 1 1 1 1 ...\n##  $ differ  : Factor w/ 3 levels \"well\",\"moderate\",..: 2 2 2 2 2 2 2 2 3 2 ...\n##  $ extent  : Factor w/ 4 levels \"submucosa\",\"muscle\",..: 3 3 2 3 3 3 3 3 3 3 ...\n##  $ surg    : Factor w/ 2 levels \"short\",\"long\": 1 1 1 2 2 1 1 2 2 1 ...\n##  $ node4   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 1 1 1 1 ...\n\n多因素cox回归现在的很多文章中都是用来筛选变量的，但其实它是一种识别危险因素的方法，通常是根据P值和可信区间判断某个变量对终点事件是不是有影响。\n如果某个变量是分类变量，那么它在进入回归分析后会自动被执行哑变量编码，以其中第一个水平作为参考，其他水平都和参考组进行比较。具体的编码细节我在很久之前就详细介绍过了：分类数据回归分析时的编码方案"
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#多因素回归",
    "href": "亚组分析和多因素回归的森林图.html#多因素回归",
    "title": "44  亚组分析和多因素回归的森林图",
    "section": "44.2 多因素回归",
    "text": "44.2 多因素回归\n\nlibrary(survival)\n\nfit_multi &lt;- coxph(Surv(time, status) ~ ., data = df)\nsummary(fit_multi)\n## Call:\n## coxph(formula = Surv(time, status) ~ ., data = df)\n## \n##   n= 606, number of events= 292 \n##    (13 observations deleted due to missingness)\n## \n##                       coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \n## rx1               0.521198  1.684043  0.120261  4.334 1.47e-05 ***\n## sexmale          -0.125724  0.881858  0.118615 -1.060   0.2892    \n## age&lt;=65           0.022860  1.023123  0.124973  0.183   0.8549    \n## obstructYes       0.001102  1.001103  0.150254  0.007   0.9941    \n## perforYes         0.219640  1.245629  0.335564  0.655   0.5128    \n## adhereYes         0.121203  1.128854  0.172725  0.702   0.4829    \n## differmoderate   -0.214304  0.807103  0.211843 -1.012   0.3117    \n## differpoor        0.196139  1.216696  0.240222  0.816   0.4142    \n## extentmuscle      0.413055  1.511429  0.620625  0.666   0.5057    \n## extentserosa      1.043101  2.838005  0.584977  1.783   0.0746 .  \n## extentcontiguous  1.336959  3.807447  0.637908  2.096   0.0361 *  \n## surglong          0.198218  1.219229  0.127288  1.557   0.1194    \n## node4Yes          0.811284  2.250796  0.123699  6.559 5.43e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##                  exp(coef) exp(-coef) lower .95 upper .95\n## rx1                 1.6840     0.5938    1.3304     2.132\n## sexmale             0.8819     1.1340    0.6989     1.113\n## age&lt;=65             1.0231     0.9774    0.8008     1.307\n## obstructYes         1.0011     0.9989    0.7457     1.344\n## perforYes           1.2456     0.8028    0.6453     2.404\n## adhereYes           1.1289     0.8859    0.8047     1.584\n## differmoderate      0.8071     1.2390    0.5329     1.223\n## differpoor          1.2167     0.8219    0.7598     1.948\n## extentmuscle        1.5114     0.6616    0.4478     5.101\n## extentserosa        2.8380     0.3524    0.9017     8.932\n## extentcontiguous    3.8074     0.2626    1.0906    13.293\n## surglong            1.2192     0.8202    0.9500     1.565\n## node4Yes            2.2508     0.4443    1.7662     2.868\n## \n## Concordance= 0.672  (se = 0.016 )\n## Likelihood ratio test= 93.76  on 13 df,   p=3e-14\n## Wald test            = 93.41  on 13 df,   p=3e-14\n## Score (logrank) test = 98.95  on 13 df,   p=3e-15\n\n可以看到这个多因素回归的结果，对于每一个分类变量，都会进行哑变量编码（参考上面的推文），所有结果中会有rx1，sexmale这样的结果，rx这个变量是有2个类别的，分别是类别0和类别1，结果只有rx1，因为列别0是参考，对于sex也是，其中的female时参考，所以只有sexmale的结果。\n此时的森林图是这样的，也是表达的一模一样的意思，你可以看到结果中都有一个reference，这个就是参考了，参考类别是没有P值的，也没有可信区间，HR都是1。\n\nlibrary(survminer)\n\nggforest(fit_multi,fontsize = 1)\n## Warning in .get_data(model, data = data): The `data` argument is not provided.\n## Data will be extracted from model fit.\n\n\n\n\n为了和亚组分析的森林图比较一下，我们重新提取一下数据，使用forestploter包再画一遍。\n\nmultidf &lt;- broom::tidy(fit_multi,exponentiate = T,conf.int = T) %&gt;% \n  mutate(across(where(is.numeric), round,digits=2),\n         `HR(95%CI)`=paste0(estimate,\"(\",conf.low,\"-\",conf.high,\")\")\n         ) %&gt;% \n  select(term,estimate,p.value,conf.low,conf.high,`HR(95%CI)`)\n## Warning: There was 1 warning in `mutate()`.\n## ℹ In argument: `across(where(is.numeric), round, digits = 2)`.\n## Caused by warning:\n## ! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\n## Supply arguments directly to `.fns` through an anonymous function instead.\n## \n##   # Previously\n##   across(a:b, mean, na.rm = TRUE)\n## \n##   # Now\n##   across(a:b, \\(x) mean(x, na.rm = TRUE))\n\nmultidf\n## # A tibble: 13 × 6\n##    term             estimate p.value conf.low conf.high `HR(95%CI)`     \n##    &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;           \n##  1 rx1                  1.68    0        1.33      2.13 1.68(1.33-2.13) \n##  2 sexmale              0.88    0.29     0.7       1.11 0.88(0.7-1.11)  \n##  3 age&lt;=65              1.02    0.85     0.8       1.31 1.02(0.8-1.31)  \n##  4 obstructYes          1       0.99     0.75      1.34 1(0.75-1.34)    \n##  5 perforYes            1.25    0.51     0.65      2.4  1.25(0.65-2.4)  \n##  6 adhereYes            1.13    0.48     0.8       1.58 1.13(0.8-1.58)  \n##  7 differmoderate       0.81    0.31     0.53      1.22 0.81(0.53-1.22) \n##  8 differpoor           1.22    0.41     0.76      1.95 1.22(0.76-1.95) \n##  9 extentmuscle         1.51    0.51     0.45      5.1  1.51(0.45-5.1)  \n## 10 extentserosa         2.84    0.07     0.9       8.93 2.84(0.9-8.93)  \n## 11 extentcontiguous     3.81    0.04     1.09     13.3  3.81(1.09-13.29)\n## 12 surglong             1.22    0.12     0.95      1.56 1.22(0.95-1.56) \n## 13 node4Yes             2.25    0        1.77      2.87 2.25(1.77-2.87)\n\n#write.csv(multidf,file = \"multidf.csv\",quote = F)\n\n保存后重新整理下格式再读取进来：\n\nplot_df &lt;- read.csv(file=\"./datasets/multidf.csv\",check.names = F)\nplot_df\n##         subgroup estimate p.value conf.low conf.high        HR(95%CI)\n## 1             rx       NA      NA       NA        NA                 \n## 2            rx0     1.00      NA     1.00      1.00                 \n## 3            rx1     1.68    0.00     1.33      2.13  1.68(1.33-2.13)\n## 4            sex       NA      NA       NA        NA                 \n## 5         female     1.00      NA     1.00      1.00                 \n## 6           male     0.88    0.29     0.70      1.11   0.88(0.7-1.11)\n## 7            age       NA      NA       NA        NA                 \n## 8            &gt;65     1.00      NA     1.00      1.00                 \n## 9           &lt;=65     1.02    0.85     0.80      1.31   1.02(0.8-1.31)\n## 10      obstruct       NA      NA       NA        NA                 \n## 11            No     1.00      NA     1.00      1.00                 \n## 12           Yes     1.00    0.99     0.75      1.34     1(0.75-1.34)\n## 13        perfor       NA      NA       NA        NA                 \n## 14            No     1.00      NA     1.00      1.00                 \n## 15           Yes     1.25    0.51     0.65      2.40   1.25(0.65-2.4)\n## 16        adhere       NA      NA       NA        NA                 \n## 17            No     1.00      NA     1.00      1.00                 \n## 18           Yes     1.13    0.48     0.80      1.58   1.13(0.8-1.58)\n## 19        differ       NA      NA       NA        NA                 \n## 20          well     1.00      NA     1.00      1.00                 \n## 21      moderate     0.81    0.31     0.53      1.22  0.81(0.53-1.22)\n## 22          poor     1.22    0.41     0.76      1.95  1.22(0.76-1.95)\n## 23        extent       NA      NA       NA        NA                 \n## 24     submucosa     1.00      NA     1.00      1.00                 \n## 25        muscle     1.51    0.51     0.45      5.10   1.51(0.45-5.1)\n## 26        serosa     2.84    0.07     0.90      8.93   2.84(0.9-8.93)\n## 27    contiguous     3.81    0.04     1.09     13.29 3.81(1.09-13.29)\n## 28          surg       NA      NA       NA        NA                 \n## 29         short     1.00      NA     1.00      1.00                 \n## 30          long     1.22    0.12     0.95      1.56  1.22(0.95-1.56)\n## 31         node4       NA      NA       NA        NA                 \n## 32            No     1.00      NA     1.00      1.00                 \n## 33           Yes     2.25    0.00     1.77      2.87  2.25(1.77-2.87)\n\n把数据中的P值部分的NA变成空格，这样画森林图时就不会显示了，然后增加1列空值用于展示可信区间：\n\nplot_df[,c(3)][is.na(plot_df[,c(3)])] &lt;- \" \"\nplot_df$` ` &lt;- paste(rep(\" \", nrow(plot_df)), collapse = \" \")\nplot_df\n##         subgroup estimate p.value conf.low conf.high        HR(95%CI)\n## 1             rx       NA               NA        NA                 \n## 2            rx0     1.00             1.00      1.00                 \n## 3            rx1     1.68       0     1.33      2.13  1.68(1.33-2.13)\n## 4            sex       NA               NA        NA                 \n## 5         female     1.00             1.00      1.00                 \n## 6           male     0.88    0.29     0.70      1.11   0.88(0.7-1.11)\n## 7            age       NA               NA        NA                 \n## 8            &gt;65     1.00             1.00      1.00                 \n## 9           &lt;=65     1.02    0.85     0.80      1.31   1.02(0.8-1.31)\n## 10      obstruct       NA               NA        NA                 \n## 11            No     1.00             1.00      1.00                 \n## 12           Yes     1.00    0.99     0.75      1.34     1(0.75-1.34)\n## 13        perfor       NA               NA        NA                 \n## 14            No     1.00             1.00      1.00                 \n## 15           Yes     1.25    0.51     0.65      2.40   1.25(0.65-2.4)\n## 16        adhere       NA               NA        NA                 \n## 17            No     1.00             1.00      1.00                 \n## 18           Yes     1.13    0.48     0.80      1.58   1.13(0.8-1.58)\n## 19        differ       NA               NA        NA                 \n## 20          well     1.00             1.00      1.00                 \n## 21      moderate     0.81    0.31     0.53      1.22  0.81(0.53-1.22)\n## 22          poor     1.22    0.41     0.76      1.95  1.22(0.76-1.95)\n## 23        extent       NA               NA        NA                 \n## 24     submucosa     1.00             1.00      1.00                 \n## 25        muscle     1.51    0.51     0.45      5.10   1.51(0.45-5.1)\n## 26        serosa     2.84    0.07     0.90      8.93   2.84(0.9-8.93)\n## 27    contiguous     3.81    0.04     1.09     13.29 3.81(1.09-13.29)\n## 28          surg       NA               NA        NA                 \n## 29         short     1.00             1.00      1.00                 \n## 30          long     1.22    0.12     0.95      1.56  1.22(0.95-1.56)\n## 31         node4       NA               NA        NA                 \n## 32            No     1.00             1.00      1.00                 \n## 33           Yes     2.25       0     1.77      2.87  2.25(1.77-2.87)\n##                                                                     \n## 1                                                                   \n## 2                                                                   \n## 3                                                                   \n## 4                                                                   \n## 5                                                                   \n## 6                                                                   \n## 7                                                                   \n## 8                                                                   \n## 9                                                                   \n## 10                                                                  \n## 11                                                                  \n## 12                                                                  \n## 13                                                                  \n## 14                                                                  \n## 15                                                                  \n## 16                                                                  \n## 17                                                                  \n## 18                                                                  \n## 19                                                                  \n## 20                                                                  \n## 21                                                                  \n## 22                                                                  \n## 23                                                                  \n## 24                                                                  \n## 25                                                                  \n## 26                                                                  \n## 27                                                                  \n## 28                                                                  \n## 29                                                                  \n## 30                                                                  \n## 31                                                                  \n## 32                                                                  \n## 33\n\n然后画图即可，默认的出图就已经很美观了:\n\nlibrary(forestploter)\nlibrary(grid)\n\np &lt;- forest(\n  data = plot_df[,c(1,6,7,3)],\n  lower = plot_df$conf.low,\n  upper = plot_df$conf.high,\n  est = plot_df$estimate,\n  ci_column = 3,\n  sizes = 1, \n  ref_line = 1, \n  xlim = c(0.1,4)\n  )\nprint(p)"
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#亚组分析",
    "href": "亚组分析和多因素回归的森林图.html#亚组分析",
    "title": "44  亚组分析和多因素回归的森林图",
    "section": "44.3 亚组分析",
    "text": "44.3 亚组分析\n亚组分析的思路非常简单，就是在每一个亚组中进行分析，详细过程我们就不介绍了，大家可以参考之前的推文（不理解亚组分析怎么做的一定要看）：\n\nR语言亚组分析及森林图绘制\nR语言亚组分析1行代码实现！\n\n对于我们这个演示数据，它画出来的亚组分析的森林图是这样的（绘制代码参考上面两篇推文）："
  },
  {
    "objectID": "亚组分析和多因素回归的森林图.html#比较",
    "href": "亚组分析和多因素回归的森林图.html#比较",
    "title": "44  亚组分析和多因素回归的森林图",
    "section": "44.4 比较",
    "text": "44.4 比较\n不知道看到这里你明白了没有，亚组分析是在所有数据的子集中做分析，在每一个亚组中都进行一次分析，每次分析都能得到一个HR值和可信区间，把所有结果放在一起，就得到森林图了。\n而多因素回归其实只是把分类变量进行哑变量编码而已，其中一个是参考，其余都和参考比，这样也能得到不同类别的HR值和可信区间。如果是数值型变量而不是分类变量不用进行哑变量编码了，自然也不会出现“亚组”的形式。\n虽有都有HR值、可信区间、P值等信息，但是表达的意思和实现方法确实去安全不同的！\n还有一个我没见过的形式：多因素分析+亚组分析的森林图，但是粉丝群里有群友问到过，意思是在每一个亚组内都做多因素分析，这样的森林图就要在每个亚组内展示多个HR和可信区间了，这样做是可行的吗？欢迎大位大佬把DOI发在评论区，让我开开眼。"
  },
  {
    "objectID": "comparegroups.html#导出",
    "href": "comparegroups.html#导出",
    "title": "45  compareGroups绘制三线表",
    "section": "45.1 导出",
    "text": "45.1 导出\n支持的格式非常丰富：\n\nexport2csv(restab, file='table1.csv'), 导出为CSV\nexport2html(restab, file='table1.html'), 导出为HTML\nexport2latex(restab, file='table1.tex'), 导出为LaTeX\nexport2pdf(restab, file='table1.pdf'), 导出为PDF\nexport2md(restab, file='table1.md'), 导出为Markdown\nexport2word(restab, file='table1.docx'), 导出为Word\nexport2xls(restab, file='table1.xlsx'), 导出为Excel\n\n导出时还支持各种格式调整，比如添加阴影等。"
  },
  {
    "objectID": "tableone.html#安装",
    "href": "tableone.html#安装",
    "title": "46  tableone绘制三线表",
    "section": "46.1 安装",
    "text": "46.1 安装\n两种安装方式任选一种即可：\n\ninstall.packages(\"tableone\")\n# install.packages(\"devtools\")\ndevtools::install_github(repo = \"kaz-yos/tableone\", ref = \"develop\")\n\nR包安装有问题的小伙伴可以加我微信或者评论区留言~"
  },
  {
    "objectID": "tableone.html#使用",
    "href": "tableone.html#使用",
    "title": "46  tableone绘制三线表",
    "section": "46.2 使用",
    "text": "46.2 使用\n为了方便比较，还是使用compareGroups包自带的regicor数据集。\n\n为了说明这个软件包是如何工作的，我们从REGICOR研究中取了一部分数据。REGICOR是一个 对来自西班牙东北部的参与者进行的横断面研究，包括：人口统计学信息（年龄、性别、身高、体重、腰围等）、血脂特征（总胆固醇和胆固醇、甘油三酯等）、问卷调查信息（体育活动，生活质量，…）等。此外，心血管事件和 死亡信息来自医院和官方登记处。\n\n\nlibrary(compareGroups)\nlibrary(tableone)\n\ndata(\"regicor\")\ndim(regicor)\n## [1] 2294   25\nstr(regicor)\n## 'data.frame':    2294 obs. of  25 variables:\n##  $ id      : num  2.26e+03 1.88e+03 3.00e+09 3.00e+09 3.00e+09 ...\n##   ..- attr(*, \"label\")= Named chr \"Individual id\"\n##   .. ..- attr(*, \"names\")= chr \"id\"\n##  $ year    : Factor w/ 3 levels \"1995\",\"2000\",..: 3 3 2 2 2 2 2 1 3 1 ...\n##   ..- attr(*, \"label\")= Named chr \"Recruitment year\"\n##   .. ..- attr(*, \"names\")= chr \"year\"\n##  $ age     : int  70 56 37 69 70 40 66 53 43 70 ...\n##   ..- attr(*, \"label\")= Named chr \"Age\"\n##   .. ..- attr(*, \"names\")= chr \"age\"\n##  $ sex     : Factor w/ 2 levels \"Male\",\"Female\": 2 2 1 2 2 2 1 2 2 1 ...\n##   ..- attr(*, \"label\")= chr \"Sex\"\n##  $ smoker  : Factor w/ 3 levels \"Never smoker\",..: 1 1 2 1 NA 2 1 1 3 3 ...\n##   ..- attr(*, \"label\")= Named chr \"Smoking status\"\n##   .. ..- attr(*, \"names\")= chr \"smoker\"\n##  $ sbp     : int  138 139 132 168 NA 108 120 132 95 142 ...\n##   ..- attr(*, \"label\")= Named chr \"Systolic blood pressure\"\n##   .. ..- attr(*, \"names\")= chr \"sbp\"\n##  $ dbp     : int  75 89 82 97 NA 70 72 78 65 78 ...\n##   ..- attr(*, \"label\")= Named chr \"Diastolic blood pressure\"\n##   .. ..- attr(*, \"names\")= chr \"dbp\"\n##  $ histhtn : Factor w/ 2 levels \"Yes\",\"No\": 2 2 2 2 2 2 1 2 2 2 ...\n##   ..- attr(*, \"label\")= Named chr \"History of hypertension\"\n##   .. ..- attr(*, \"names\")= chr \"histbp\"\n##  $ txhtn   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n##   ..- attr(*, \"label\")= chr \"Hypertension treatment\"\n##  $ chol    : num  294 220 245 168 NA NA 298 254 194 188 ...\n##   ..- attr(*, \"label\")= Named chr \"Total cholesterol\"\n##   .. ..- attr(*, \"names\")= chr \"chol\"\n##  $ hdl     : num  57 50 59.8 53.2 NA ...\n##   ..- attr(*, \"label\")= Named chr \"HDL cholesterol\"\n##   .. ..- attr(*, \"names\")= chr \"hdl\"\n##  $ triglyc : num  93 160 89 116 NA 94 71 NA 68 137 ...\n##   ..- attr(*, \"label\")= Named chr \"Triglycerides\"\n##   .. ..- attr(*, \"names\")= chr \"triglyc\"\n##  $ ldl     : num  218.4 138 167.4 91.6 NA ...\n##   ..- attr(*, \"label\")= Named chr \"LDL cholesterol\"\n##   .. ..- attr(*, \"names\")= chr \"ldl\"\n##  $ histchol: Factor w/ 2 levels \"Yes\",\"No\": 2 2 2 2 NA 2 1 2 2 2 ...\n##   ..- attr(*, \"label\")= chr \"History of hyperchol.\"\n##  $ txchol  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 NA 1 1 1 1 1 ...\n##   ..- attr(*, \"label\")= Named chr \"Cholesterol treatment\"\n##   .. ..- attr(*, \"names\")= chr \"txchol\"\n##  $ height  : num  160 163 170 147 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Height (cm)\"\n##   .. ..- attr(*, \"names\")= chr \"height\"\n##  $ weight  : num  64 67 70 68 NA 43.5 79.2 45.8 53 62 ...\n##   ..- attr(*, \"label\")= Named chr \"Weight (Kg)\"\n##   .. ..- attr(*, \"names\")= chr \"weight\"\n##  $ bmi     : num  25 25.2 24.2 31.5 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Body mass index\"\n##   .. ..- attr(*, \"names\")= chr \"bmi\"\n##  $ phyact  : num  304 160 553 522 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Physical activity (Kcal/week)\"\n##   .. ..- attr(*, \"names\")= chr \"phyact\"\n##  $ pcs     : num  54.5 58.2 43.4 54.3 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Physical component\"\n##   .. ..- attr(*, \"names\")= chr \"pcs\"\n##  $ mcs     : num  58.9 48 62.6 57.9 NA ...\n##   ..- attr(*, \"label\")= chr \"Mental component\"\n##  $ cv      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 NA 1 1 1 1 1 ...\n##   ..- attr(*, \"label\")= chr \"Cardiovascular event\"\n##  $ tocv    : num  1025 2757 1906 1055 NA ...\n##   ..- attr(*, \"label\")= chr \"Days to cardiovascular event or end of follow-up\"\n##  $ death   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 1 NA 1 2 1 1 1 ...\n##   ..- attr(*, \"label\")= chr \"Overall death\"\n##  $ todeath : num  1299.2 39.3 858.4 1833.1 NA ...\n##   ..- attr(*, \"label\")= chr \"Days to overall death or end of follow-up\"\n\n各个变量的信息如下：\n\n\n\n\nName\nLabel\nCodes\n\n\n\n\nid\nIndividual id\n\n\n\nyear\nRecruitment year\n1995; 2000; 2005\n\n\nage\nAge\n\n\n\nsex\nSex\nMale; Female\n\n\nsmoker\nSmoking status\nNever smoker; Current or former &lt; 1y; Former $\\geq$ 1y\n\n\nsbp\nSystolic blood pressure\n\n\n\ndbp\nDiastolic blood pressure\n\n\n\nhisthtn\nHistory of hypertension\nYes; No\n\n\ntxhtn\nHypertension treatment\nNo; Yes\n\n\nchol\nTotal cholesterol\n\n\n\nhdl\nHDL cholesterol\n\n\n\ntriglyc\nTriglycerides\n\n\n\nldl\nLDL cholesterol\n\n\n\nhistchol\nHistory of hyperchol.\nYes; No\n\n\ntxchol\nCholesterol treatment\nNo; Yes\n\n\nheight\nHeight (cm)\n\n\n\nweight\nWeight (Kg)\n\n\n\nbmi\nBody mass index\n\n\n\nphyact\nPhysical activity (Kcal/week)\n\n\n\npcs\nPhysical component\n\n\n\nmcs\nMental component\n\n\n\ncv\nCardiovascular event\nNo; Yes\n\n\ntocv\nDays to cardiovascular event or end of follow-up\n\n\n\ndeath\nOverall death\nNo; Yes\n\n\ntodeath\nDays to overall death or end of follow-up\n\n\n\n\n\n\n\n为了防止这个数据集再次消失，把它保存到本地。。。\n\nsave(regicor,file = \"./datasets/regicor.rdata\")\n\n\n46.2.1 基本描述功能\n首先是基本的统计描述功能，使用CreateTableOne()函数可以给出数据的基本情况：\n\ntab &lt;- CreateTableOne(data = regicor)\ntab\n##                            \n##                             Overall                      \n##   n                                  2294                \n##   id (mean (SD))            1215817623.64 (1339538685.65)\n##   year (%)                                               \n##      1995                             431 (18.8)         \n##      2000                             786 (34.3)         \n##      2005                            1077 (46.9)         \n##   age (mean (SD))                   54.74 (11.05)        \n##   sex = Female (%)                   1193 (52.0)         \n##   smoker (%)                                             \n##      Never smoker                    1201 (53.8)         \n##      Current or former &lt; 1y           593 (26.6)         \n##      Former &gt;= 1y                     439 (19.7)         \n##   sbp (mean (SD))                  131.17 (20.31)        \n##   dbp (mean (SD))                   79.66 (10.55)        \n##   histhtn = No (%)                   1563 (68.4)         \n##   txhtn = Yes (%)                     428 (19.0)         \n##   chol (mean (SD))                 218.76 (45.25)        \n##   hdl (mean (SD))                   52.69 (14.75)        \n##   triglyc (mean (SD))              115.58 (73.94)        \n##   ldl (mean (SD))                  143.25 (39.69)        \n##   histchol = No (%)                  1564 (68.8)         \n##   txchol = Yes (%)                    228 (10.2)         \n##   height (mean (SD))               162.92 (9.22)         \n##   weight (mean (SD))                73.44 (13.68)        \n##   bmi (mean (SD))                   27.64 (4.56)         \n##   phyact (mean (SD))               398.83 (388.16)       \n##   pcs (mean (SD))                   49.62 (9.01)         \n##   mcs (mean (SD))                   47.98 (10.98)        \n##   cv = Yes (%)                         92 ( 4.3)         \n##   tocv (mean (SD))                1754.67 (1080.62)      \n##   death = Yes (%)                     173 ( 8.1)         \n##   todeath (mean (SD))             1721.31 (1051.05)\n\n\n\n46.2.2 选择变量&指定变量类型\n通过vars()函数指定保留哪些变量，通过factorVars()函数指定哪些是分类变量。\n但其实regicor这个数据集已经把分类变量因子化了，因此这里不用factorVars()也是可以的。\n\nCreateTableOne(data = regicor,\n               vars = c(\"age\",\"sex\",\"bmi\",\"smoker\",\"height\",\"weight\"),\n               factorVars = c(\"smoker\",\"sex\")\n               )\n##                            \n##                             Overall       \n##   n                           2294        \n##   age (mean (SD))            54.74 (11.05)\n##   sex = Female (%)            1193 (52.0) \n##   bmi (mean (SD))            27.64 (4.56) \n##   smoker (%)                              \n##      Never smoker             1201 (53.8) \n##      Current or former &lt; 1y    593 (26.6) \n##      Former &gt;= 1y              439 (19.7) \n##   height (mean (SD))        162.92 (9.22) \n##   weight (mean (SD))         73.44 (13.68)\n\n\n\n46.2.3 显示所有水平\n在第一个表中我们可以发现很多分类变量被折叠了，都只显示了yes或者no的一部分，比如sex/txhtn/cv等，我们在print()中添加showAllLevels = T显示所有分类！\n\nprint(tab, showAllLevels = T)\n##                      \n##                       level                  Overall                      \n##   n                                                   2294                \n##   id (mean (SD))                             1215817623.64 (1339538685.65)\n##   year (%)            1995                             431 (18.8)         \n##                       2000                             786 (34.3)         \n##                       2005                            1077 (46.9)         \n##   age (mean (SD))                                    54.74 (11.05)        \n##   sex (%)             Male                            1101 (48.0)         \n##                       Female                          1193 (52.0)         \n##   smoker (%)          Never smoker                    1201 (53.8)         \n##                       Current or former &lt; 1y           593 (26.6)         \n##                       Former &gt;= 1y                     439 (19.7)         \n##   sbp (mean (SD))                                   131.17 (20.31)        \n##   dbp (mean (SD))                                    79.66 (10.55)        \n##   histhtn (%)         Yes                              723 (31.6)         \n##                       No                              1563 (68.4)         \n##   txhtn (%)           No                              1823 (81.0)         \n##                       Yes                              428 (19.0)         \n##   chol (mean (SD))                                  218.76 (45.25)        \n##   hdl (mean (SD))                                    52.69 (14.75)        \n##   triglyc (mean (SD))                               115.58 (73.94)        \n##   ldl (mean (SD))                                   143.25 (39.69)        \n##   histchol (%)        Yes                              709 (31.2)         \n##                       No                              1564 (68.8)         \n##   txchol (%)          No                              2011 (89.8)         \n##                       Yes                              228 (10.2)         \n##   height (mean (SD))                                162.92 (9.22)         \n##   weight (mean (SD))                                 73.44 (13.68)        \n##   bmi (mean (SD))                                    27.64 (4.56)         \n##   phyact (mean (SD))                                398.83 (388.16)       \n##   pcs (mean (SD))                                    49.62 (9.01)         \n##   mcs (mean (SD))                                    47.98 (10.98)        \n##   cv (%)              No                              2071 (95.7)         \n##                       Yes                               92 ( 4.3)         \n##   tocv (mean (SD))                                 1754.67 (1080.62)      \n##   death (%)           No                              1975 (91.9)         \n##                       Yes                              173 ( 8.1)         \n##   todeath (mean (SD))                              1721.31 (1051.05)\n\n对于二分类变量，还可以使用cramVars参数达到类似的效果，但要注意，此时不同类别是显示在一行中的：\n\nprint(tab, cramVars = c(\"sex\",\"histchol\",\"txchol\",\"cv\",\"death\"))\n##                            \n##                             Overall                      \n##   n                                  2294                \n##   id (mean (SD))            1215817623.64 (1339538685.65)\n##   year (%)                                               \n##      1995                             431 (18.8)         \n##      2000                             786 (34.3)         \n##      2005                            1077 (46.9)         \n##   age (mean (SD))                   54.74 (11.05)        \n##   sex = Male/Female (%)         1101/1193 (48.0/52.0)    \n##   smoker (%)                                             \n##      Never smoker                    1201 (53.8)         \n##      Current or former &lt; 1y           593 (26.6)         \n##      Former &gt;= 1y                     439 (19.7)         \n##   sbp (mean (SD))                  131.17 (20.31)        \n##   dbp (mean (SD))                   79.66 (10.55)        \n##   histhtn = No (%)                   1563 (68.4)         \n##   txhtn = Yes (%)                     428 (19.0)         \n##   chol (mean (SD))                 218.76 (45.25)        \n##   hdl (mean (SD))                   52.69 (14.75)        \n##   triglyc (mean (SD))              115.58 (73.94)        \n##   ldl (mean (SD))                  143.25 (39.69)        \n##   histchol = Yes/No (%)          709/1564 (31.2/68.8)    \n##   txchol = No/Yes (%)            2011/228 (89.8/10.2)    \n##   height (mean (SD))               162.92 (9.22)         \n##   weight (mean (SD))                73.44 (13.68)        \n##   bmi (mean (SD))                   27.64 (4.56)         \n##   phyact (mean (SD))               398.83 (388.16)       \n##   pcs (mean (SD))                   49.62 (9.01)         \n##   mcs (mean (SD))                   47.98 (10.98)        \n##   cv = No/Yes (%)                 2071/92 (95.7/4.3)     \n##   tocv (mean (SD))                1754.67 (1080.62)      \n##   death = No/Yes (%)             1975/173 (91.9/8.1)     \n##   todeath (mean (SD))             1721.31 (1051.05)\n\n\n\n46.2.4 非正态分布变量\n对于正态分布的变量使用的是均值±标准差的方式进行展示，对于非正态变量则通过中位数（四分位距）表示。可以通过nonnormal参数指定。\n\nprint(tab, \n      showAllLevels = T,\n      nonnormal = c(\"todeath\")\n      )\n##                         \n##                          level                  Overall                        \n##   n                                                      2294                  \n##   id (mean (SD))                                1215817623.64 (1339538685.65)  \n##   year (%)               1995                             431 (18.8)           \n##                          2000                             786 (34.3)           \n##                          2005                            1077 (46.9)           \n##   age (mean (SD))                                       54.74 (11.05)          \n##   sex (%)                Male                            1101 (48.0)           \n##                          Female                          1193 (52.0)           \n##   smoker (%)             Never smoker                    1201 (53.8)           \n##                          Current or former &lt; 1y           593 (26.6)           \n##                          Former &gt;= 1y                     439 (19.7)           \n##   sbp (mean (SD))                                      131.17 (20.31)          \n##   dbp (mean (SD))                                       79.66 (10.55)          \n##   histhtn (%)            Yes                              723 (31.6)           \n##                          No                              1563 (68.4)           \n##   txhtn (%)              No                              1823 (81.0)           \n##                          Yes                              428 (19.0)           \n##   chol (mean (SD))                                     218.76 (45.25)          \n##   hdl (mean (SD))                                       52.69 (14.75)          \n##   triglyc (mean (SD))                                  115.58 (73.94)          \n##   ldl (mean (SD))                                      143.25 (39.69)          \n##   histchol (%)           Yes                              709 (31.2)           \n##                          No                              1564 (68.8)           \n##   txchol (%)             No                              2011 (89.8)           \n##                          Yes                              228 (10.2)           \n##   height (mean (SD))                                   162.92 (9.22)           \n##   weight (mean (SD))                                    73.44 (13.68)          \n##   bmi (mean (SD))                                       27.64 (4.56)           \n##   phyact (mean (SD))                                   398.83 (388.16)         \n##   pcs (mean (SD))                                       49.62 (9.01)           \n##   mcs (mean (SD))                                       47.98 (10.98)          \n##   cv (%)                 No                              2071 (95.7)           \n##                          Yes                               92 ( 4.3)           \n##   tocv (mean (SD))                                    1754.67 (1080.62)        \n##   death (%)              No                              1975 (91.9)           \n##                          Yes                              173 ( 8.1)           \n##   todeath (median [IQR])                              1668.40 [787.63, 2662.54]\n\n\n\n46.2.5 分层显示\n但是在实际写论文的时候，经常需要分组显示，分别展示不同组间的统计资料，然后计算组间有没有差别！\n可以通过strata参数实现，自动给出P值。\n\ntab_s &lt;- CreateTableOne(data = regicor, \n                        vars = colnames(regicor)[-1], # id就不要了 \n                        strata = \"year\")\n# 全部展开展示:\nprint(tab_s, showAllLevels = T)\n##                      Stratified by year\n##                       level                  1995             \n##   n                                              431          \n##   year (%)            1995                       431 (100.0)  \n##                       2000                         0 (  0.0)  \n##                       2005                         0 (  0.0)  \n##   age (mean (SD))                              54.10 (11.72)  \n##   sex (%)             Male                       206 ( 47.8)  \n##                       Female                     225 ( 52.2)  \n##   smoker (%)          Never smoker               234 ( 56.4)  \n##                       Current or former &lt; 1y     109 ( 26.3)  \n##                       Former &gt;= 1y                72 ( 17.3)  \n##   sbp (mean (SD))                             132.61 (19.17)  \n##   dbp (mean (SD))                              77.04 (10.54)  \n##   histhtn (%)         Yes                        111 ( 25.8)  \n##                       No                         320 ( 74.2)  \n##   txhtn (%)           No                         360 ( 83.5)  \n##                       Yes                         71 ( 16.5)  \n##   chol (mean (SD))                            225.32 (43.13)  \n##   hdl (mean (SD))                              51.87 (14.46)  \n##   triglyc (mean (SD))                         114.15 (74.37)  \n##   ldl (mean (SD))                             151.73 (38.41)  \n##   histchol (%)        Yes                         97 ( 22.5)  \n##                       No                         334 ( 77.5)  \n##   txchol (%)          No                         403 ( 93.5)  \n##                       Yes                         28 (  6.5)  \n##   height (mean (SD))                          163.50 (9.21)   \n##   weight (mean (SD))                           72.29 (12.61)  \n##   bmi (mean (SD))                              27.02 (4.15)   \n##   phyact (mean (SD))                          490.78 (419.04) \n##   pcs (mean (SD))                              49.33 (8.08)   \n##   mcs (mean (SD))                              49.25 (11.35)  \n##   cv (%)              No                         388 ( 97.5)  \n##                       Yes                         10 (  2.5)  \n##   tocv (mean (SD))                           1783.62 (1101.17)\n##   death (%)           No                         369 ( 95.3)  \n##                       Yes                         18 (  4.7)  \n##   todeath (mean (SD))                        1713.31 (1042.18)\n##                      Stratified by year\n##                       2000              2005              p      test\n##   n                       786              1077                      \n##   year (%)                  0 (  0.0)         0 (  0.0)   &lt;0.001     \n##                           786 (100.0)         0 (  0.0)              \n##                             0 (  0.0)      1077 (100.0)              \n##   age (mean (SD))       54.34 (11.22)     55.28 (10.63)    0.078     \n##   sex (%)                 390 ( 49.6)       505 ( 46.9)    0.506     \n##                           396 ( 50.4)       572 ( 53.1)              \n##   smoker (%)              414 ( 54.6)       553 ( 52.2)   &lt;0.001     \n##                           267 ( 35.2)       217 ( 20.5)              \n##                            77 ( 10.2)       290 ( 27.4)              \n##   sbp (mean (SD))      133.04 (21.31)    129.26 (19.85)   &lt;0.001     \n##   dbp (mean (SD))       80.80 (10.31)     79.88 (10.55)   &lt;0.001     \n##   histhtn (%)             233 ( 29.6)       379 ( 35.5)   &lt;0.001     \n##                           553 ( 70.4)       690 ( 64.5)              \n##   txhtn (%)               659 ( 83.8)       804 ( 77.8)    0.002     \n##                           127 ( 16.2)       230 ( 22.2)              \n##   chol (mean (SD))     223.67 (44.37)    213.03 (45.92)   &lt;0.001     \n##   hdl (mean (SD))       52.34 (15.60)     53.24 (14.23)    0.208     \n##   triglyc (mean (SD))  113.94 (70.69)    117.27 (76.01)    0.582     \n##   ldl (mean (SD))      149.03 (38.61)    136.32 (39.68)   &lt;0.001     \n##   histchol (%)            256 ( 33.2)       356 ( 33.2)   &lt;0.001     \n##                           515 ( 66.8)       715 ( 66.8)              \n##   txchol (%)              705 ( 91.2)       903 ( 87.2)   &lt;0.001     \n##                            68 (  8.8)       132 ( 12.8)              \n##   height (mean (SD))   162.01 (9.39)     163.34 (9.05)     0.003     \n##   weight (mean (SD))    73.84 (13.95)     73.60 (13.87)    0.150     \n##   bmi (mean (SD))       28.10 (4.62)      27.56 (4.63)    &lt;0.001     \n##   phyact (mean (SD))   421.74 (377.13)   351.16 (378.05)  &lt;0.001     \n##   pcs (mean (SD))       49.01 (9.63)      50.14 (8.91)     0.032     \n##   mcs (mean (SD))       48.90 (10.95)     46.87 (10.75)   &lt;0.001     \n##   cv (%)                  706 ( 95.3)       977 ( 95.4)    0.161     \n##                            35 (  4.7)        47 (  4.6)              \n##   tocv (mean (SD))    1685.62 (1079.72) 1793.38 (1071.81)  0.099     \n##   death (%)               657 ( 89.0)       949 ( 92.8)   &lt;0.001     \n##                            81 ( 11.0)        74 (  7.2)              \n##   todeath (mean (SD)) 1674.37 (1050.09) 1758.19 (1054.68)  0.252\n\n但是tableone并没有提供直接导出到Word的途径，只能导入到csv文件中，这是有点差劲的地方。\n\n\n46.2.6 导出\n\ntab_sv &lt;- print(tab_s,showAllLevels = T,printToggle = F)\n\nwrite.csv(tab_sv,file = \"tab_sv.csv\")"
  },
  {
    "objectID": "tableone.html#不同包的比较",
    "href": "tableone.html#不同包的比较",
    "title": "46  tableone绘制三线表",
    "section": "46.3 不同包的比较",
    "text": "46.3 不同包的比较\n可以看到tableone做出一张表需要2行代码：\n\ntab_s &lt;- CreateTableOne(data = regicor,vars = colnames(regicor)[-1], \n                        strata = \"year\")\nprint(tab_s,showAllLevels = T)\n##                      Stratified by year\n##                       level                  1995             \n##   n                                              431          \n##   year (%)            1995                       431 (100.0)  \n##                       2000                         0 (  0.0)  \n##                       2005                         0 (  0.0)  \n##   age (mean (SD))                              54.10 (11.72)  \n##   sex (%)             Male                       206 ( 47.8)  \n##                       Female                     225 ( 52.2)  \n##   smoker (%)          Never smoker               234 ( 56.4)  \n##                       Current or former &lt; 1y     109 ( 26.3)  \n##                       Former &gt;= 1y                72 ( 17.3)  \n##   sbp (mean (SD))                             132.61 (19.17)  \n##   dbp (mean (SD))                              77.04 (10.54)  \n##   histhtn (%)         Yes                        111 ( 25.8)  \n##                       No                         320 ( 74.2)  \n##   txhtn (%)           No                         360 ( 83.5)  \n##                       Yes                         71 ( 16.5)  \n##   chol (mean (SD))                            225.32 (43.13)  \n##   hdl (mean (SD))                              51.87 (14.46)  \n##   triglyc (mean (SD))                         114.15 (74.37)  \n##   ldl (mean (SD))                             151.73 (38.41)  \n##   histchol (%)        Yes                         97 ( 22.5)  \n##                       No                         334 ( 77.5)  \n##   txchol (%)          No                         403 ( 93.5)  \n##                       Yes                         28 (  6.5)  \n##   height (mean (SD))                          163.50 (9.21)   \n##   weight (mean (SD))                           72.29 (12.61)  \n##   bmi (mean (SD))                              27.02 (4.15)   \n##   phyact (mean (SD))                          490.78 (419.04) \n##   pcs (mean (SD))                              49.33 (8.08)   \n##   mcs (mean (SD))                              49.25 (11.35)  \n##   cv (%)              No                         388 ( 97.5)  \n##                       Yes                         10 (  2.5)  \n##   tocv (mean (SD))                           1783.62 (1101.17)\n##   death (%)           No                         369 ( 95.3)  \n##                       Yes                         18 (  4.7)  \n##   todeath (mean (SD))                        1713.31 (1042.18)\n##                      Stratified by year\n##                       2000              2005              p      test\n##   n                       786              1077                      \n##   year (%)                  0 (  0.0)         0 (  0.0)   &lt;0.001     \n##                           786 (100.0)         0 (  0.0)              \n##                             0 (  0.0)      1077 (100.0)              \n##   age (mean (SD))       54.34 (11.22)     55.28 (10.63)    0.078     \n##   sex (%)                 390 ( 49.6)       505 ( 46.9)    0.506     \n##                           396 ( 50.4)       572 ( 53.1)              \n##   smoker (%)              414 ( 54.6)       553 ( 52.2)   &lt;0.001     \n##                           267 ( 35.2)       217 ( 20.5)              \n##                            77 ( 10.2)       290 ( 27.4)              \n##   sbp (mean (SD))      133.04 (21.31)    129.26 (19.85)   &lt;0.001     \n##   dbp (mean (SD))       80.80 (10.31)     79.88 (10.55)   &lt;0.001     \n##   histhtn (%)             233 ( 29.6)       379 ( 35.5)   &lt;0.001     \n##                           553 ( 70.4)       690 ( 64.5)              \n##   txhtn (%)               659 ( 83.8)       804 ( 77.8)    0.002     \n##                           127 ( 16.2)       230 ( 22.2)              \n##   chol (mean (SD))     223.67 (44.37)    213.03 (45.92)   &lt;0.001     \n##   hdl (mean (SD))       52.34 (15.60)     53.24 (14.23)    0.208     \n##   triglyc (mean (SD))  113.94 (70.69)    117.27 (76.01)    0.582     \n##   ldl (mean (SD))      149.03 (38.61)    136.32 (39.68)   &lt;0.001     \n##   histchol (%)            256 ( 33.2)       356 ( 33.2)   &lt;0.001     \n##                           515 ( 66.8)       715 ( 66.8)              \n##   txchol (%)              705 ( 91.2)       903 ( 87.2)   &lt;0.001     \n##                            68 (  8.8)       132 ( 12.8)              \n##   height (mean (SD))   162.01 (9.39)     163.34 (9.05)     0.003     \n##   weight (mean (SD))    73.84 (13.95)     73.60 (13.87)    0.150     \n##   bmi (mean (SD))       28.10 (4.62)      27.56 (4.63)    &lt;0.001     \n##   phyact (mean (SD))   421.74 (377.13)   351.16 (378.05)  &lt;0.001     \n##   pcs (mean (SD))       49.01 (9.63)      50.14 (8.91)     0.032     \n##   mcs (mean (SD))       48.90 (10.95)     46.87 (10.75)   &lt;0.001     \n##   cv (%)                  706 ( 95.3)       977 ( 95.4)    0.161     \n##                            35 (  4.7)        47 (  4.6)              \n##   tocv (mean (SD))    1685.62 (1079.72) 1793.38 (1071.81)  0.099     \n##   death (%)               657 ( 89.0)       949 ( 92.8)   &lt;0.001     \n##                            81 ( 11.0)        74 (  7.2)              \n##   todeath (mean (SD)) 1674.37 (1050.09) 1758.19 (1054.68)  0.252\n\n而comparegroups只需要1行：\n\ndescrTable(year ~ . - id, data = regicor)\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ______________________________________________________________________________________________ \n##                                                     1995        2000        2005     p.overall \n##                                                     N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                                              54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex:                                                                                   0.506   \n##     Male                                         206 (47.8%) 390 (49.6%) 505 (46.9%)           \n##     Female                                       225 (52.2%) 396 (50.4%) 572 (53.1%)           \n## Smoking status:                                                                       &lt;0.001   \n##     Never smoker                                 234 (56.4%) 414 (54.6%) 553 (52.2%)           \n##     Current or former &lt; 1y                       109 (26.3%) 267 (35.2%) 217 (20.5%)           \n##     Former &gt;= 1y                                 72 (17.3%)  77 (10.2%)  290 (27.4%)           \n## Systolic blood pressure                          133 (19.2)  133 (21.3)  129 (19.8)   &lt;0.001   \n## Diastolic blood pressure                         77.0 (10.5) 80.8 (10.3) 79.9 (10.6)  &lt;0.001   \n## History of hypertension:                                                              &lt;0.001   \n##     Yes                                          111 (25.8%) 233 (29.6%) 379 (35.5%)           \n##     No                                           320 (74.2%) 553 (70.4%) 690 (64.5%)           \n## Hypertension treatment:                                                                0.002   \n##     No                                           360 (83.5%) 659 (83.8%) 804 (77.8%)           \n##     Yes                                          71 (16.5%)  127 (16.2%) 230 (22.2%)           \n## Total cholesterol                                225 (43.1)  224 (44.4)  213 (45.9)   &lt;0.001   \n## HDL cholesterol                                  51.9 (14.5) 52.3 (15.6) 53.2 (14.2)   0.208   \n## Triglycerides                                    114 (74.4)  114 (70.7)  117 (76.0)    0.582   \n## LDL cholesterol                                  152 (38.4)  149 (38.6)  136 (39.7)   &lt;0.001   \n## History of hyperchol.:                                                                &lt;0.001   \n##     Yes                                          97 (22.5%)  256 (33.2%) 356 (33.2%)           \n##     No                                           334 (77.5%) 515 (66.8%) 715 (66.8%)           \n## Cholesterol treatment:                                                                &lt;0.001   \n##     No                                           403 (93.5%) 705 (91.2%) 903 (87.2%)           \n##     Yes                                          28 (6.50%)  68 (8.80%)  132 (12.8%)           \n## Height (cm)                                      163 (9.21)  162 (9.39)  163 (9.05)    0.003   \n## Weight (Kg)                                      72.3 (12.6) 73.8 (14.0) 73.6 (13.9)   0.150   \n## Body mass index                                  27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  &lt;0.001   \n## Physical activity (Kcal/week)                     491 (419)   422 (377)   351 (378)   &lt;0.001   \n## Physical component                               49.3 (8.08) 49.0 (9.63) 50.1 (8.91)   0.032   \n## Mental component                                 49.2 (11.3) 48.9 (11.0) 46.9 (10.8)  &lt;0.001   \n## Cardiovascular event:                                                                  0.161   \n##     No                                           388 (97.5%) 706 (95.3%) 977 (95.4%)           \n##     Yes                                          10 (2.51%)  35 (4.72%)  47 (4.59%)            \n## Days to cardiovascular event or end of follow-up 1784 (1101) 1686 (1080) 1793 (1072)   0.099   \n## Overall death:                                                                        &lt;0.001   \n##     No                                           369 (95.3%) 657 (89.0%) 949 (92.8%)           \n##     Yes                                          18 (4.65%)  81 (11.0%)  74 (7.23%)            \n## Days to overall death or end of follow-up        1713 (1042) 1674 (1050) 1758 (1055)   0.252   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n而且还能直接导出到Word，直接变成三线表！\n如果是简单的做个三线表的话还是compareGroups更简单，但是tableone的功能貌似更多一点，比如除了这个三线表，它还可以用于加权数据中，还能用于倾向性评分中。"
  },
  {
    "objectID": "table1.html#安装",
    "href": "table1.html#安装",
    "title": "47  table1绘制三线表",
    "section": "47.1 安装",
    "text": "47.1 安装\n\n# 2种方式选择1个\ninstall.packages(\"table1\")\n\nrequire(devtools)\ndevtools::install_github(\"benjaminrich/table1\")"
  },
  {
    "objectID": "table1.html#使用",
    "href": "table1.html#使用",
    "title": "47  table1绘制三线表",
    "section": "47.2 使用",
    "text": "47.2 使用\n为了方便比较，还是使用compareGroups包自带的regicor数据集。\n\n为了说明这个软件包是如何工作的，我们从REGICOR研究中取了一部分数据。REGICOR是一个 对来自西班牙东北部的参与者进行的横断面研究，包括：人口统计学信息（年龄、性别、身高、体重、腰围等）、血脂特征（总胆固醇和胆固醇、甘油三酯等）、问卷调查信息（体育活动，生活质量，…）等。此外，心血管事件和 死亡信息来自医院和官方登记处。\n\n\nlibrary(compareGroups)\nlibrary(table1)\n## \n## Attaching package: 'table1'\n## The following objects are masked from 'package:base':\n## \n##     units, units&lt;-\n\ndata(\"regicor\")\ndim(regicor)\n## [1] 2294   25\nstr(regicor)\n## 'data.frame':    2294 obs. of  25 variables:\n##  $ id      : num  2.26e+03 1.88e+03 3.00e+09 3.00e+09 3.00e+09 ...\n##   ..- attr(*, \"label\")= Named chr \"Individual id\"\n##   .. ..- attr(*, \"names\")= chr \"id\"\n##  $ year    : Factor w/ 3 levels \"1995\",\"2000\",..: 3 3 2 2 2 2 2 1 3 1 ...\n##   ..- attr(*, \"label\")= Named chr \"Recruitment year\"\n##   .. ..- attr(*, \"names\")= chr \"year\"\n##  $ age     : int  70 56 37 69 70 40 66 53 43 70 ...\n##   ..- attr(*, \"label\")= Named chr \"Age\"\n##   .. ..- attr(*, \"names\")= chr \"age\"\n##  $ sex     : Factor w/ 2 levels \"Male\",\"Female\": 2 2 1 2 2 2 1 2 2 1 ...\n##   ..- attr(*, \"label\")= chr \"Sex\"\n##  $ smoker  : Factor w/ 3 levels \"Never smoker\",..: 1 1 2 1 NA 2 1 1 3 3 ...\n##   ..- attr(*, \"label\")= Named chr \"Smoking status\"\n##   .. ..- attr(*, \"names\")= chr \"smoker\"\n##  $ sbp     : int  138 139 132 168 NA 108 120 132 95 142 ...\n##   ..- attr(*, \"label\")= Named chr \"Systolic blood pressure\"\n##   .. ..- attr(*, \"names\")= chr \"sbp\"\n##  $ dbp     : int  75 89 82 97 NA 70 72 78 65 78 ...\n##   ..- attr(*, \"label\")= Named chr \"Diastolic blood pressure\"\n##   .. ..- attr(*, \"names\")= chr \"dbp\"\n##  $ histhtn : Factor w/ 2 levels \"Yes\",\"No\": 2 2 2 2 2 2 1 2 2 2 ...\n##   ..- attr(*, \"label\")= Named chr \"History of hypertension\"\n##   .. ..- attr(*, \"names\")= chr \"histbp\"\n##  $ txhtn   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 2 1 1 1 ...\n##   ..- attr(*, \"label\")= chr \"Hypertension treatment\"\n##  $ chol    : num  294 220 245 168 NA NA 298 254 194 188 ...\n##   ..- attr(*, \"label\")= Named chr \"Total cholesterol\"\n##   .. ..- attr(*, \"names\")= chr \"chol\"\n##  $ hdl     : num  57 50 59.8 53.2 NA ...\n##   ..- attr(*, \"label\")= Named chr \"HDL cholesterol\"\n##   .. ..- attr(*, \"names\")= chr \"hdl\"\n##  $ triglyc : num  93 160 89 116 NA 94 71 NA 68 137 ...\n##   ..- attr(*, \"label\")= Named chr \"Triglycerides\"\n##   .. ..- attr(*, \"names\")= chr \"triglyc\"\n##  $ ldl     : num  218.4 138 167.4 91.6 NA ...\n##   ..- attr(*, \"label\")= Named chr \"LDL cholesterol\"\n##   .. ..- attr(*, \"names\")= chr \"ldl\"\n##  $ histchol: Factor w/ 2 levels \"Yes\",\"No\": 2 2 2 2 NA 2 1 2 2 2 ...\n##   ..- attr(*, \"label\")= chr \"History of hyperchol.\"\n##  $ txchol  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 NA 1 1 1 1 1 ...\n##   ..- attr(*, \"label\")= Named chr \"Cholesterol treatment\"\n##   .. ..- attr(*, \"names\")= chr \"txchol\"\n##  $ height  : num  160 163 170 147 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Height (cm)\"\n##   .. ..- attr(*, \"names\")= chr \"height\"\n##  $ weight  : num  64 67 70 68 NA 43.5 79.2 45.8 53 62 ...\n##   ..- attr(*, \"label\")= Named chr \"Weight (Kg)\"\n##   .. ..- attr(*, \"names\")= chr \"weight\"\n##  $ bmi     : num  25 25.2 24.2 31.5 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Body mass index\"\n##   .. ..- attr(*, \"names\")= chr \"bmi\"\n##  $ phyact  : num  304 160 553 522 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Physical activity (Kcal/week)\"\n##   .. ..- attr(*, \"names\")= chr \"phyact\"\n##  $ pcs     : num  54.5 58.2 43.4 54.3 NA ...\n##   ..- attr(*, \"label\")= Named chr \"Physical component\"\n##   .. ..- attr(*, \"names\")= chr \"pcs\"\n##  $ mcs     : num  58.9 48 62.6 57.9 NA ...\n##   ..- attr(*, \"label\")= chr \"Mental component\"\n##  $ cv      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 NA 1 1 1 1 1 ...\n##   ..- attr(*, \"label\")= chr \"Cardiovascular event\"\n##  $ tocv    : num  1025 2757 1906 1055 NA ...\n##   ..- attr(*, \"label\")= chr \"Days to cardiovascular event or end of follow-up\"\n##  $ death   : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 1 NA 1 2 1 1 1 ...\n##   ..- attr(*, \"label\")= chr \"Overall death\"\n##  $ todeath : num  1299.2 39.3 858.4 1833.1 NA ...\n##   ..- attr(*, \"label\")= chr \"Days to overall death or end of follow-up\"\n\n各个变量的信息如下：\n\n\n\n\nName\nLabel\nCodes\n\n\n\n\nid\nIndividual id\n\n\n\nyear\nRecruitment year\n1995; 2000; 2005\n\n\nage\nAge\n\n\n\nsex\nSex\nMale; Female\n\n\nsmoker\nSmoking status\nNever smoker; Current or former &lt; 1y; Former $\\geq$ 1y\n\n\nsbp\nSystolic blood pressure\n\n\n\ndbp\nDiastolic blood pressure\n\n\n\nhisthtn\nHistory of hypertension\nYes; No\n\n\ntxhtn\nHypertension treatment\nNo; Yes\n\n\nchol\nTotal cholesterol\n\n\n\nhdl\nHDL cholesterol\n\n\n\ntriglyc\nTriglycerides\n\n\n\nldl\nLDL cholesterol\n\n\n\nhistchol\nHistory of hyperchol.\nYes; No\n\n\ntxchol\nCholesterol treatment\nNo; Yes\n\n\nheight\nHeight (cm)\n\n\n\nweight\nWeight (Kg)\n\n\n\nbmi\nBody mass index\n\n\n\nphyact\nPhysical activity (Kcal/week)\n\n\n\npcs\nPhysical component\n\n\n\nmcs\nMental component\n\n\n\ncv\nCardiovascular event\nNo; Yes\n\n\ntocv\nDays to cardiovascular event or end of follow-up\n\n\n\ndeath\nOverall death\nNo; Yes\n\n\ntodeath\nDays to overall death or end of follow-up\n\n\n\n\n\n\n\n大家在使用的时候还是要注意，要先把分类变量因子化！使用起来也是1行代码就可以出图了：\n\ntable1(~ . | year, data = regicor[,-1])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1995\n(N=431)\n2000\n(N=786)\n2005\n(N=1077)\nOverall\n(N=2294)\n\n\n\n\nAge\n\n\n\n\n\n\nMean (SD)\n54.1 (11.7)\n54.3 (11.2)\n55.3 (10.6)\n54.7 (11.0)\n\n\nMedian [Min, Max]\n54.0 [35.0, 74.0]\n54.0 [35.0, 74.0]\n56.0 [35.0, 74.0]\n55.0 [35.0, 74.0]\n\n\nSex\n\n\n\n\n\n\nMale\n206 (47.8%)\n390 (49.6%)\n505 (46.9%)\n1101 (48.0%)\n\n\nFemale\n225 (52.2%)\n396 (50.4%)\n572 (53.1%)\n1193 (52.0%)\n\n\nSmoking status\n\n\n\n\n\n\nNever smoker\n234 (54.3%)\n414 (52.7%)\n553 (51.3%)\n1201 (52.4%)\n\n\nCurrent or former &lt; 1y\n109 (25.3%)\n267 (34.0%)\n217 (20.1%)\n593 (25.9%)\n\n\nFormer &gt;= 1y\n72 (16.7%)\n77 (9.8%)\n290 (26.9%)\n439 (19.1%)\n\n\nMissing\n16 (3.7%)\n28 (3.6%)\n17 (1.6%)\n61 (2.7%)\n\n\nSystolic blood pressure\n\n\n\n\n\n\nMean (SD)\n133 (19.2)\n133 (21.3)\n129 (19.8)\n131 (20.3)\n\n\nMedian [Min, Max]\n132 [92.0, 196]\n132 [80.0, 212]\n125 [88.0, 229]\n129 [80.0, 229]\n\n\nMissing\n3 (0.7%)\n11 (1.4%)\n0 (0%)\n14 (0.6%)\n\n\nDiastolic blood pressure\n\n\n\n\n\n\nMean (SD)\n77.0 (10.5)\n80.8 (10.3)\n79.9 (10.6)\n79.7 (10.5)\n\n\nMedian [Min, Max]\n78.0 [52.0, 108]\n80.0 [50.0, 123]\n79.0 [40.0, 121]\n80.0 [40.0, 123]\n\n\nMissing\n3 (0.7%)\n11 (1.4%)\n0 (0%)\n14 (0.6%)\n\n\nHistory of hypertension\n\n\n\n\n\n\nYes\n111 (25.8%)\n233 (29.6%)\n379 (35.2%)\n723 (31.5%)\n\n\nNo\n320 (74.2%)\n553 (70.4%)\n690 (64.1%)\n1563 (68.1%)\n\n\nMissing\n0 (0%)\n0 (0%)\n8 (0.7%)\n8 (0.3%)\n\n\nHypertension treatment\n\n\n\n\n\n\nNo\n360 (83.5%)\n659 (83.8%)\n804 (74.7%)\n1823 (79.5%)\n\n\nYes\n71 (16.5%)\n127 (16.2%)\n230 (21.4%)\n428 (18.7%)\n\n\nMissing\n0 (0%)\n0 (0%)\n43 (4.0%)\n43 (1.9%)\n\n\nTotal cholesterol\n\n\n\n\n\n\nMean (SD)\n225 (43.1)\n224 (44.4)\n213 (45.9)\n219 (45.2)\n\n\nMedian [Min, Max]\n225 [95.0, 361]\n222 [113, 404]\n209 [95.0, 488]\n215 [95.0, 488]\n\n\nMissing\n28 (6.5%)\n71 (9.0%)\n2 (0.2%)\n101 (4.4%)\n\n\nHDL cholesterol\n\n\n\n\n\n\nMean (SD)\n51.9 (14.5)\n52.3 (15.6)\n53.2 (14.2)\n52.7 (14.7)\n\n\nMedian [Min, Max]\n50.6 [24.5, 95.0]\n49.7 [19.6, 103]\n52.0 [20.0, 112]\n51.0 [19.6, 112]\n\n\nMissing\n30 (7.0%)\n38 (4.8%)\n1 (0.1%)\n69 (3.0%)\n\n\nTriglycerides\n\n\n\n\n\n\nMean (SD)\n114 (74.4)\n114 (70.7)\n117 (76.0)\n116 (73.9)\n\n\nMedian [Min, Max]\n94.0 [41.0, 882]\n98.0 [25.0, 960]\n98.0 [33.0, 744]\n97.0 [25.0, 960]\n\n\nMissing\n28 (6.5%)\n34 (4.3%)\n1 (0.1%)\n63 (2.7%)\n\n\nLDL cholesterol\n\n\n\n\n\n\nMean (SD)\n152 (38.4)\n149 (38.6)\n136 (39.7)\n143 (39.7)\n\n\nMedian [Min, Max]\n151 [36.3, 273]\n147 [51.7, 300]\n133 [36.8, 330]\n141 [36.3, 330]\n\n\nMissing\n43 (10.0%)\n98 (12.5%)\n27 (2.5%)\n168 (7.3%)\n\n\nHistory of hyperchol.\n\n\n\n\n\n\nYes\n97 (22.5%)\n256 (32.6%)\n356 (33.1%)\n709 (30.9%)\n\n\nNo\n334 (77.5%)\n515 (65.5%)\n715 (66.4%)\n1564 (68.2%)\n\n\nMissing\n0 (0%)\n15 (1.9%)\n6 (0.6%)\n21 (0.9%)\n\n\nCholesterol treatment\n\n\n\n\n\n\nNo\n403 (93.5%)\n705 (89.7%)\n903 (83.8%)\n2011 (87.7%)\n\n\nYes\n28 (6.5%)\n68 (8.7%)\n132 (12.3%)\n228 (9.9%)\n\n\nMissing\n0 (0%)\n13 (1.7%)\n42 (3.9%)\n55 (2.4%)\n\n\nHeight (cm)\n\n\n\n\n\n\nMean (SD)\n163 (9.21)\n162 (9.39)\n163 (9.05)\n163 (9.22)\n\n\nMedian [Min, Max]\n163 [143, 192]\n162 [139, 190]\n163 [137, 199]\n163 [137, 199]\n\n\nMissing\n8 (1.9%)\n15 (1.9%)\n12 (1.1%)\n35 (1.5%)\n\n\nWeight (Kg)\n\n\n\n\n\n\nMean (SD)\n72.3 (12.6)\n73.8 (14.0)\n73.6 (13.9)\n73.4 (13.7)\n\n\nMedian [Min, Max]\n72.0 [41.2, 127]\n73.0 [43.5, 118]\n73.0 [42.0, 125]\n73.0 [41.2, 127]\n\n\nMissing\n8 (1.9%)\n15 (1.9%)\n12 (1.1%)\n35 (1.5%)\n\n\nBody mass index\n\n\n\n\n\n\nMean (SD)\n27.0 (4.15)\n28.1 (4.62)\n27.6 (4.63)\n27.6 (4.56)\n\n\nMedian [Min, Max]\n26.5 [17.2, 43.7]\n27.6 [17.4, 48.2]\n27.2 [17.1, 48.2]\n27.2 [17.1, 48.2]\n\n\nMissing\n8 (1.9%)\n15 (1.9%)\n12 (1.1%)\n35 (1.5%)\n\n\nPhysical activity (Kcal/week)\n\n\n\n\n\n\nMean (SD)\n491 (419)\n422 (377)\n351 (378)\n399 (388)\n\n\nMedian [Min, Max]\n390 [1.00, 3430]\n347 [0, 5080]\n262 [0, 4230]\n304 [0, 5080]\n\n\nMissing\n64 (14.8%)\n22 (2.8%)\n2 (0.2%)\n88 (3.8%)\n\n\nPhysical component\n\n\n\n\n\n\nMean (SD)\n49.3 (8.08)\n49.0 (9.63)\n50.1 (8.91)\n49.6 (9.01)\n\n\nMedian [Min, Max]\n51.7 [18.6, 66.2]\n51.9 [13.9, 67.0]\n53.1 [17.9, 67.1]\n52.3 [13.9, 67.1]\n\n\nMissing\n34 (7.9%)\n123 (15.6%)\n83 (7.7%)\n240 (10.5%)\n\n\nMental component\n\n\n\n\n\n\nMean (SD)\n49.2 (11.3)\n48.9 (11.0)\n46.9 (10.8)\n48.0 (11.0)\n\n\nMedian [Min, Max]\n52.3 [3.42, 69.9]\n52.8 [10.2, 69.4]\n49.7 [7.62, 67.6]\n51.3 [3.42, 69.9]\n\n\nMissing\n34 (7.9%)\n123 (15.6%)\n83 (7.7%)\n240 (10.5%)\n\n\nCardiovascular event\n\n\n\n\n\n\nNo\n388 (90.0%)\n706 (89.8%)\n977 (90.7%)\n2071 (90.3%)\n\n\nYes\n10 (2.3%)\n35 (4.5%)\n47 (4.4%)\n92 (4.0%)\n\n\nMissing\n33 (7.7%)\n45 (5.7%)\n53 (4.9%)\n131 (5.7%)\n\n\nDays to cardiovascular event or end of follow-up\n\n\n\n\n\n\nMean (SD)\n1780 (1100)\n1690 (1080)\n1790 (1070)\n1750 (1080)\n\n\nMedian [Min, Max]\n1730 [0.260, 3650]\n1620 [0.115, 3640]\n1770 [2.65, 3650]\n1720 [0.115, 3650]\n\n\nMissing\n33 (7.7%)\n45 (5.7%)\n53 (4.9%)\n131 (5.7%)\n\n\nOverall death\n\n\n\n\n\n\nNo\n369 (85.6%)\n657 (83.6%)\n949 (88.1%)\n1975 (86.1%)\n\n\nYes\n18 (4.2%)\n81 (10.3%)\n74 (6.9%)\n173 (7.5%)\n\n\nMissing\n44 (10.2%)\n48 (6.1%)\n54 (5.0%)\n146 (6.4%)\n\n\nDays to overall death or end of follow-up\n\n\n\n\n\n\nMean (SD)\n1710 (1040)\n1670 (1050)\n1760 (1050)\n1720 (1050)\n\n\nMedian [Min, Max]\n1560 [0.220, 3640]\n1610 [0.897, 3650]\n1730 [0.889, 3650]\n1670 [0.220, 3650]\n\n\nMissing\n44 (10.2%)\n48 (6.1%)\n54 (5.0%)\n146 (6.4%)\n\n\n\n\n\n\n\n是不是很简单，把需要分组的变量放在|后面即可。\n结果是html形式的，可以直接复制粘贴到Word里面，连续性变量默认给出了两种表示方式！"
  },
  {
    "objectID": "table1.html#美化",
    "href": "table1.html#美化",
    "title": "47  table1绘制三线表",
    "section": "47.3 美化",
    "text": "47.3 美化\n先给大家展示下复杂表头的制作格式，其实非常简单，就是用*即可。\n\ntable1(~  . | year*sex, data = regicor[,-1],overall = F) # 不展示overall\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1995\n\n\n2000\n\n\n2005\n\n\n\n\nMale\n(N=206)\nFemale\n(N=225)\nMale\n(N=390)\nFemale\n(N=396)\nMale\n(N=505)\nFemale\n(N=572)\n\n\n\n\nAge\n\n\n\n\n\n\n\n\nMean (SD)\n54.1 (11.8)\n54.1 (11.7)\n54.3 (11.2)\n54.4 (11.2)\n55.4 (10.7)\n55.2 (10.6)\n\n\nMedian [Min, Max]\n53.0 [35.0, 74.0]\n54.0 [35.0, 74.0]\n54.0 [35.0, 74.0]\n54.0 [35.0, 74.0]\n55.0 [35.0, 74.0]\n56.0 [35.0, 74.0]\n\n\nSmoking status\n\n\n\n\n\n\n\n\nNever smoker\n52 (25.2%)\n182 (80.9%)\n112 (28.7%)\n302 (76.3%)\n137 (27.1%)\n416 (72.7%)\n\n\nCurrent or former &lt; 1y\n77 (37.4%)\n32 (14.2%)\n199 (51.0%)\n68 (17.2%)\n134 (26.5%)\n83 (14.5%)\n\n\nFormer &gt;= 1y\n67 (32.5%)\n5 (2.2%)\n66 (16.9%)\n11 (2.8%)\n227 (45.0%)\n63 (11.0%)\n\n\nMissing\n10 (4.9%)\n6 (2.7%)\n13 (3.3%)\n15 (3.8%)\n7 (1.4%)\n10 (1.7%)\n\n\nSystolic blood pressure\n\n\n\n\n\n\n\n\nMean (SD)\n134 (18.4)\n132 (19.8)\n137 (19.3)\n129 (22.6)\n132 (18.7)\n127 (20.5)\n\n\nMedian [Min, Max]\n132 [92.0, 184]\n132 [92.0, 196]\n134 [90.0, 212]\n124 [80.0, 206]\n129 [93.0, 200]\n122 [88.0, 229]\n\n\nMissing\n0 (0%)\n3 (1.3%)\n3 (0.8%)\n8 (2.0%)\n0 (0%)\n0 (0%)\n\n\nDiastolic blood pressure\n\n\n\n\n\n\n\n\nMean (SD)\n79.0 (9.27)\n75.2 (11.3)\n83.0 (9.54)\n78.6 (10.6)\n81.7 (10.8)\n78.3 (10.0)\n\n\nMedian [Min, Max]\n78.0 [54.0, 108]\n74.0 [52.0, 102]\n82.0 [60.0, 123]\n80.0 [50.0, 112]\n81.0 [40.0, 121]\n78.0 [47.0, 111]\n\n\nMissing\n0 (0%)\n3 (1.3%)\n3 (0.8%)\n8 (2.0%)\n0 (0%)\n0 (0%)\n\n\nHistory of hypertension\n\n\n\n\n\n\n\n\nYes\n50 (24.3%)\n61 (27.1%)\n110 (28.2%)\n123 (31.1%)\n181 (35.8%)\n198 (34.6%)\n\n\nNo\n156 (75.7%)\n164 (72.9%)\n280 (71.8%)\n273 (68.9%)\n319 (63.2%)\n371 (64.9%)\n\n\nMissing\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n5 (1.0%)\n3 (0.5%)\n\n\nHypertension treatment\n\n\n\n\n\n\n\n\nNo\n175 (85.0%)\n185 (82.2%)\n342 (87.7%)\n317 (80.1%)\n372 (73.7%)\n432 (75.5%)\n\n\nYes\n31 (15.0%)\n40 (17.8%)\n48 (12.3%)\n79 (19.9%)\n110 (21.8%)\n120 (21.0%)\n\n\nMissing\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n23 (4.6%)\n20 (3.5%)\n\n\nTotal cholesterol\n\n\n\n\n\n\n\n\nMean (SD)\n224 (43.9)\n226 (42.4)\n224 (43.9)\n224 (44.9)\n210 (40.3)\n216 (50.3)\n\n\nMedian [Min, Max]\n224 [101, 346]\n227 [95.0, 361]\n226 [122, 404]\n220 [113, 386]\n207 [95.0, 369]\n211 [109, 488]\n\n\nMissing\n10 (4.9%)\n18 (8.0%)\n37 (9.5%)\n34 (8.6%)\n0 (0%)\n2 (0.3%)\n\n\nHDL cholesterol\n\n\n\n\n\n\n\n\nMean (SD)\n46.5 (13.1)\n56.9 (13.9)\n47.3 (12.6)\n57.4 (16.7)\n48.1 (12.4)\n57.8 (14.2)\n\n\nMedian [Min, Max]\n45.3 [24.5, 95.0]\n56.6 [28.6, 92.6]\n45.7 [19.6, 94.2]\n56.8 [20.1, 103]\n47.0 [20.0, 97.0]\n57.0 [21.0, 112]\n\n\nMissing\n11 (5.3%)\n19 (8.4%)\n15 (3.8%)\n23 (5.8%)\n0 (0%)\n1 (0.2%)\n\n\nTriglycerides\n\n\n\n\n\n\n\n\nMean (SD)\n131 (91.5)\n97.8 (47.9)\n128 (81.1)\n99.7 (55.1)\n132 (90.3)\n104 (57.6)\n\n\nMedian [Min, Max]\n110 [43.0, 882]\n86.0 [41.0, 364]\n113 [25.0, 960]\n87.0 [31.0, 604]\n108 [33.0, 744]\n90.0 [33.0, 521]\n\n\nMissing\n9 (4.4%)\n19 (8.4%)\n17 (4.4%)\n17 (4.3%)\n0 (0%)\n1 (0.2%)\n\n\nLDL cholesterol\n\n\n\n\n\n\n\n\nMean (SD)\n153 (39.6)\n150 (37.3)\n152 (39.1)\n146 (38.0)\n137 (36.0)\n136 (42.6)\n\n\nMedian [Min, Max]\n153 [36.3, 266]\n148 [38.6, 273]\n150 [51.7, 300]\n142 [57.7, 290]\n132 [38.2, 289]\n133 [36.8, 330]\n\n\nMissing\n18 (8.7%)\n25 (11.1%)\n52 (13.3%)\n46 (11.6%)\n21 (4.2%)\n6 (1.0%)\n\n\nHistory of hyperchol.\n\n\n\n\n\n\n\n\nYes\n48 (23.3%)\n49 (21.8%)\n138 (35.4%)\n118 (29.8%)\n167 (33.1%)\n189 (33.0%)\n\n\nNo\n158 (76.7%)\n176 (78.2%)\n247 (63.3%)\n268 (67.7%)\n336 (66.5%)\n379 (66.3%)\n\n\nMissing\n0 (0%)\n0 (0%)\n5 (1.3%)\n10 (2.5%)\n2 (0.4%)\n4 (0.7%)\n\n\nCholesterol treatment\n\n\n\n\n\n\n\n\nNo\n189 (91.7%)\n214 (95.1%)\n348 (89.2%)\n357 (90.2%)\n425 (84.2%)\n478 (83.6%)\n\n\nYes\n17 (8.3%)\n11 (4.9%)\n38 (9.7%)\n30 (7.6%)\n59 (11.7%)\n73 (12.8%)\n\n\nMissing\n0 (0%)\n0 (0%)\n4 (1.0%)\n9 (2.3%)\n21 (4.2%)\n21 (3.7%)\n\n\nHeight (cm)\n\n\n\n\n\n\n\n\nMean (SD)\n170 (7.34)\n158 (6.31)\n168 (7.17)\n156 (6.50)\n170 (7.43)\n158 (6.24)\n\n\nMedian [Min, Max]\n170 [148, 192]\n158 [143, 178]\n168 [144, 190]\n155 [139, 176]\n170 [148, 199]\n157 [137, 180]\n\n\nMissing\n2 (1.0%)\n6 (2.7%)\n5 (1.3%)\n10 (2.5%)\n4 (0.8%)\n8 (1.4%)\n\n\nWeight (Kg)\n\n\n\n\n\n\n\n\nMean (SD)\n77.6 (11.7)\n67.3 (11.3)\n80.1 (12.3)\n67.6 (12.6)\n80.2 (11.6)\n67.7 (13.0)\n\n\nMedian [Min, Max]\n77.3 [41.2, 127]\n66.0 [43.2, 98.6]\n79.2 [50.0, 117]\n66.0 [43.5, 118]\n79.0 [43.0, 122]\n66.0 [42.0, 125]\n\n\nMissing\n2 (1.0%)\n6 (2.7%)\n5 (1.3%)\n10 (2.5%)\n4 (0.8%)\n8 (1.4%)\n\n\nBody mass index\n\n\n\n\n\n\n\n\nMean (SD)\n26.9 (3.64)\n27.2 (4.57)\n28.2 (3.89)\n28.0 (5.25)\n27.9 (3.58)\n27.3 (5.39)\n\n\nMedian [Min, Max]\n26.5 [18.5, 39.6]\n26.4 [17.2, 43.7]\n28.0 [18.4, 40.5]\n27.0 [17.4, 48.2]\n27.7 [18.1, 42.6]\n26.7 [17.1, 48.2]\n\n\nMissing\n2 (1.0%)\n6 (2.7%)\n5 (1.3%)\n10 (2.5%)\n4 (0.8%)\n8 (1.4%)\n\n\nPhysical activity (Kcal/week)\n\n\n\n\n\n\n\n\nMean (SD)\n422 (418)\n553 (412)\n356 (362)\n486 (382)\n439 (467)\n273 (253)\n\n\nMedian [Min, Max]\n291 [1.00, 3240]\n479 [18.0, 3430]\n264 [0, 4220]\n417 [0, 5080]\n317 [0, 4230]\n222 [0, 1960]\n\n\nMissing\n31 (15.0%)\n33 (14.7%)\n10 (2.6%)\n12 (3.0%)\n0 (0%)\n2 (0.3%)\n\n\nPhysical component\n\n\n\n\n\n\n\n\nMean (SD)\n50.1 (6.71)\n48.6 (9.16)\n50.9 (8.58)\n47.1 (10.2)\n51.5 (8.07)\n48.9 (9.45)\n\n\nMedian [Min, Max]\n51.9 [23.3, 65.9]\n51.2 [18.6, 66.2]\n54.3 [13.9, 67.0]\n49.7 [18.3, 65.1]\n54.5 [18.2, 67.1]\n51.6 [17.9, 64.6]\n\n\nMissing\n12 (5.8%)\n22 (9.8%)\n57 (14.6%)\n66 (16.7%)\n30 (5.9%)\n53 (9.3%)\n\n\nMental component\n\n\n\n\n\n\n\n\nMean (SD)\n52.1 (9.67)\n46.5 (12.2)\n50.9 (10.2)\n46.9 (11.3)\n49.2 (9.67)\n44.7 (11.2)\n\n\nMedian [Min, Max]\n54.4 [3.42, 66.4]\n49.4 [9.93, 69.9]\n54.5 [14.5, 65.4]\n49.6 [10.2, 69.4]\n51.9 [7.62, 67.6]\n47.3 [12.5, 65.1]\n\n\nMissing\n12 (5.8%)\n22 (9.8%)\n57 (14.6%)\n66 (16.7%)\n30 (5.9%)\n53 (9.3%)\n\n\nCardiovascular event\n\n\n\n\n\n\n\n\nNo\n190 (92.2%)\n198 (88.0%)\n345 (88.5%)\n361 (91.2%)\n461 (91.3%)\n516 (90.2%)\n\n\nYes\n6 (2.9%)\n4 (1.8%)\n21 (5.4%)\n14 (3.5%)\n19 (3.8%)\n28 (4.9%)\n\n\nMissing\n10 (4.9%)\n23 (10.2%)\n24 (6.2%)\n21 (5.3%)\n25 (5.0%)\n28 (4.9%)\n\n\nDays to cardiovascular event or end of follow-up\n\n\n\n\n\n\n\n\nMean (SD)\n1720 (1130)\n1850 (1070)\n1650 (1080)\n1720 (1080)\n1830 (1060)\n1760 (1080)\n\n\nMedian [Min, Max]\n1620 [0.260, 3640]\n1830 [26.4, 3650]\n1610 [0.115, 3640]\n1620 [0.279, 3630]\n1820 [2.65, 3640]\n1750 [7.22, 3650]\n\n\nMissing\n10 (4.9%)\n23 (10.2%)\n24 (6.2%)\n21 (5.3%)\n25 (5.0%)\n28 (4.9%)\n\n\nOverall death\n\n\n\n\n\n\n\n\nNo\n174 (84.5%)\n195 (86.7%)\n321 (82.3%)\n336 (84.8%)\n448 (88.7%)\n501 (87.6%)\n\n\nYes\n12 (5.8%)\n6 (2.7%)\n46 (11.8%)\n35 (8.8%)\n29 (5.7%)\n45 (7.9%)\n\n\nMissing\n20 (9.7%)\n24 (10.7%)\n23 (5.9%)\n25 (6.3%)\n28 (5.5%)\n26 (4.5%)\n\n\nDays to overall death or end of follow-up\n\n\n\n\n\n\n\n\nMean (SD)\n1690 (1030)\n1740 (1050)\n1660 (1030)\n1680 (1070)\n1680 (1030)\n1820 (1070)\n\n\nMedian [Min, Max]\n1560 [0.220, 3640]\n1590 [47.7, 3640]\n1610 [0.897, 3620]\n1620 [1.78, 3650]\n1690 [0.889, 3630]\n1810 [1.05, 3650]\n\n\nMissing\n20 (9.7%)\n24 (10.7%)\n23 (5.9%)\n25 (6.3%)\n28 (5.5%)\n26 (4.5%)\n\n\n\n\n\n\n\n还可以制作更加复杂的表头，比如把2000年和2005年放在同一个表头下，1995年单独放。\n这种情况，就需要先进行一点自定义设置了。\n\nlabels &lt;- list(\n  \n  # 先设置需要展示的变量,并设置下展示的名字\n  variables = list(sex=\"Sex\",age=\"Age(year)\",smoker=\"Smoking status\",\n                   sbp=\"Systolic blood pressure\",dbp=\"Diastolic blood pressure\",\n                   histhtn=\"History of hypertension\",\n                   txhtn=\"Hypertension treatment\",chol=\"Total cholesterol\",\n                   hdl=\"HDL cholesterol\",triglyc=\"Triglycerides\",\n                   ldl=\"LDL cholesterol\"\n               ),\n  # 设置分组\n  groups = list(\"\",\"\",\"2000年以后\")\n  )\n\n# 因子化分组变量，但regicor数据集已经因子化了，这里不用也可以\nlevels(regicor$year) &lt;- c(\"1995\",\"2000\",\"2005\")\n\n然后再设置一下分组变量：\n\nstrata &lt;- c(list(Total=regicor[,-1]),split(regicor,regicor$year))\n\n然后就能愉快的显示了，是不是很强？\n\ntable1(strata, labels, groupspan = c(1,1,2),\n       render.continuous=c(.=\"Mean ± SD\") # 设置连续性变量的显示方式\n       )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000年以后\n\n\n\n\nTotal\n(N=2294)\n1995\n(N=431)\n2000\n(N=786)\n2005\n(N=1077)\n\n\n\n\nSex\n\n\n\n\n\n\nMale\n1101 (48.0%)\n206 (47.8%)\n390 (49.6%)\n505 (46.9%)\n\n\nFemale\n1193 (52.0%)\n225 (52.2%)\n396 (50.4%)\n572 (53.1%)\n\n\nAge(year)\n\n\n\n\n\n\nMean ± SD\n54.7 ± 11.0\n54.1 ± 11.7\n54.3 ± 11.2\n55.3 ± 10.6\n\n\nSmoking status\n\n\n\n\n\n\nNever smoker\n1201 (52.4%)\n234 (54.3%)\n414 (52.7%)\n553 (51.3%)\n\n\nCurrent or former &lt; 1y\n593 (25.9%)\n109 (25.3%)\n267 (34.0%)\n217 (20.1%)\n\n\nFormer &gt;= 1y\n439 (19.1%)\n72 (16.7%)\n77 (9.8%)\n290 (26.9%)\n\n\nMissing\n61 (2.7%)\n16 (3.7%)\n28 (3.6%)\n17 (1.6%)\n\n\nSystolic blood pressure\n\n\n\n\n\n\nMean ± SD\n131 ± 20.3\n133 ± 19.2\n133 ± 21.3\n129 ± 19.8\n\n\nMissing\n14 (0.6%)\n3 (0.7%)\n11 (1.4%)\n0 (0%)\n\n\nDiastolic blood pressure\n\n\n\n\n\n\nMean ± SD\n79.7 ± 10.5\n77.0 ± 10.5\n80.8 ± 10.3\n79.9 ± 10.6\n\n\nMissing\n14 (0.6%)\n3 (0.7%)\n11 (1.4%)\n0 (0%)\n\n\nHistory of hypertension\n\n\n\n\n\n\nYes\n723 (31.5%)\n111 (25.8%)\n233 (29.6%)\n379 (35.2%)\n\n\nNo\n1563 (68.1%)\n320 (74.2%)\n553 (70.4%)\n690 (64.1%)\n\n\nMissing\n8 (0.3%)\n0 (0%)\n0 (0%)\n8 (0.7%)\n\n\nHypertension treatment\n\n\n\n\n\n\nNo\n1823 (79.5%)\n360 (83.5%)\n659 (83.8%)\n804 (74.7%)\n\n\nYes\n428 (18.7%)\n71 (16.5%)\n127 (16.2%)\n230 (21.4%)\n\n\nMissing\n43 (1.9%)\n0 (0%)\n0 (0%)\n43 (4.0%)\n\n\nTotal cholesterol\n\n\n\n\n\n\nMean ± SD\n219 ± 45.2\n225 ± 43.1\n224 ± 44.4\n213 ± 45.9\n\n\nMissing\n101 (4.4%)\n28 (6.5%)\n71 (9.0%)\n2 (0.2%)\n\n\nHDL cholesterol\n\n\n\n\n\n\nMean ± SD\n52.7 ± 14.7\n51.9 ± 14.5\n52.3 ± 15.6\n53.2 ± 14.2\n\n\nMissing\n69 (3.0%)\n30 (7.0%)\n38 (4.8%)\n1 (0.1%)\n\n\nTriglycerides\n\n\n\n\n\n\nMean ± SD\n116 ± 73.9\n114 ± 74.4\n114 ± 70.7\n117 ± 76.0\n\n\nMissing\n63 (2.7%)\n28 (6.5%)\n34 (4.3%)\n1 (0.1%)\n\n\nLDL cholesterol\n\n\n\n\n\n\nMean ± SD\n143 ± 39.7\n152 ± 38.4\n149 ± 38.6\n136 ± 39.7\n\n\nMissing\n168 (7.3%)\n43 (10.0%)\n98 (12.5%)\n27 (2.5%)\n\n\n\n\n\n\n\n是不是很好看？基本符合要求了！\n但是如果有的数值型变量不符合正态分布怎么办？\n可以通过自定义函数解决。\n\n# 自定义你的变量要如何显示\nrndr &lt;- function(x, name, ...) {\n  # 分类变量就用默认方法\n  if (!is.numeric(x)) return(render.categorical.default(x))\n  \n  # 连续型变量自定义一下\n  what &lt;- switch(name,\n                 age = \"Mean (SD)\",\n                 sbp = \"Mean (SD)\",\n                 dbp = \"Mean ± SD\",\n                 chol = \"Median [IQR]\", \n                 hdl  = \"Median [IQR]\",\n                 triglyc = \"Median [Min, Max]\",\n                 ldl = \"Median [IQR]\"\n                 )\n  parse.abbrev.render.code(c(\"\", what))(x)\n}\n\n这样就能愉快的显示了，不过上面那段代码还是太复杂了~\n\ntable1(strata, labels, groupspan = c(1,1,2),\n       render=rndr\n       )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000年以后\n\n\n\n\nTotal\n(N=2294)\n1995\n(N=431)\n2000\n(N=786)\n2005\n(N=1077)\n\n\n\n\nSex\n\n\n\n\n\n\nMale\n1101 (48.0%)\n206 (47.8%)\n390 (49.6%)\n505 (46.9%)\n\n\nFemale\n1193 (52.0%)\n225 (52.2%)\n396 (50.4%)\n572 (53.1%)\n\n\nAge(year)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n54.7 (11.0)\n54.1 (11.7)\n54.3 (11.2)\n55.3 (10.6)\n\n\nSmoking status\n\n\n\n\n\n\nNever smoker\n1201 (52.4%)\n234 (54.3%)\n414 (52.7%)\n553 (51.3%)\n\n\nCurrent or former &lt; 1y\n593 (25.9%)\n109 (25.3%)\n267 (34.0%)\n217 (20.1%)\n\n\nFormer &gt;= 1y\n439 (19.1%)\n72 (16.7%)\n77 (9.8%)\n290 (26.9%)\n\n\nSystolic blood pressure\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n131 (20.3)\n133 (19.2)\n133 (21.3)\n129 (19.8)\n\n\nDiastolic blood pressure\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean ± SD\n79.7 ± 10.5\n77.0 ± 10.5\n80.8 ± 10.3\n79.9 ± 10.6\n\n\nHistory of hypertension\n\n\n\n\n\n\nYes\n723 (31.5%)\n111 (25.8%)\n233 (29.6%)\n379 (35.2%)\n\n\nNo\n1563 (68.1%)\n320 (74.2%)\n553 (70.4%)\n690 (64.1%)\n\n\nHypertension treatment\n\n\n\n\n\n\nNo\n1823 (79.5%)\n360 (83.5%)\n659 (83.8%)\n804 (74.7%)\n\n\nYes\n428 (18.7%)\n71 (16.5%)\n127 (16.2%)\n230 (21.4%)\n\n\nTotal cholesterol\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedian [IQR]\n215 [56.0]\n225 [58.0]\n222 [57.0]\n209 [54.0]\n\n\nHDL cholesterol\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedian [IQR]\n51.0 [19.4]\n50.6 [20.0]\n49.7 [21.3]\n52.0 [18.0]\n\n\nTriglycerides\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedian [Min, Max]\n97.0 [25.0, 960]\n94.0 [41.0, 882]\n98.0 [25.0, 960]\n98.0 [33.0, 744]\n\n\nLDL cholesterol\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedian [IQR]\n141 [52.4]\n151 [48.8]\n147 [53.4]\n133 [48.8]\n\n\n\n\n\n\n\n还有很多自带的样式可以使用：\n\ntable1(strata, labels, groupspan = c(1,1,2),\n       render=rndr, topclass=\"Rtable1-zebra\"\n       )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000年以后\n\n\n\n\nTotal\n(N=2294)\n1995\n(N=431)\n2000\n(N=786)\n2005\n(N=1077)\n\n\n\n\nSex\n\n\n\n\n\n\nMale\n1101 (48.0%)\n206 (47.8%)\n390 (49.6%)\n505 (46.9%)\n\n\nFemale\n1193 (52.0%)\n225 (52.2%)\n396 (50.4%)\n572 (53.1%)\n\n\nAge(year)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n54.7 (11.0)\n54.1 (11.7)\n54.3 (11.2)\n55.3 (10.6)\n\n\nSmoking status\n\n\n\n\n\n\nNever smoker\n1201 (52.4%)\n234 (54.3%)\n414 (52.7%)\n553 (51.3%)\n\n\nCurrent or former &lt; 1y\n593 (25.9%)\n109 (25.3%)\n267 (34.0%)\n217 (20.1%)\n\n\nFormer &gt;= 1y\n439 (19.1%)\n72 (16.7%)\n77 (9.8%)\n290 (26.9%)\n\n\nSystolic blood pressure\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n131 (20.3)\n133 (19.2)\n133 (21.3)\n129 (19.8)\n\n\nDiastolic blood pressure\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean ± SD\n79.7 ± 10.5\n77.0 ± 10.5\n80.8 ± 10.3\n79.9 ± 10.6\n\n\nHistory of hypertension\n\n\n\n\n\n\nYes\n723 (31.5%)\n111 (25.8%)\n233 (29.6%)\n379 (35.2%)\n\n\nNo\n1563 (68.1%)\n320 (74.2%)\n553 (70.4%)\n690 (64.1%)\n\n\nHypertension treatment\n\n\n\n\n\n\nNo\n1823 (79.5%)\n360 (83.5%)\n659 (83.8%)\n804 (74.7%)\n\n\nYes\n428 (18.7%)\n71 (16.5%)\n127 (16.2%)\n230 (21.4%)\n\n\nTotal cholesterol\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedian [IQR]\n215 [56.0]\n225 [58.0]\n222 [57.0]\n209 [54.0]\n\n\nHDL cholesterol\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedian [IQR]\n51.0 [19.4]\n50.6 [20.0]\n49.7 [21.3]\n52.0 [18.0]\n\n\nTriglycerides\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedian [Min, Max]\n97.0 [25.0, 960]\n94.0 [41.0, 882]\n98.0 [25.0, 960]\n98.0 [33.0, 744]\n\n\nLDL cholesterol\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedian [IQR]\n141 [52.4]\n151 [48.8]\n147 [53.4]\n133 [48.8]"
  },
  {
    "objectID": "table1.html#添加p值",
    "href": "table1.html#添加p值",
    "title": "47  table1绘制三线表",
    "section": "47.4 添加P值",
    "text": "47.4 添加P值\n添加P值可以通过自定义函数解决，这样做的好处是可以自定义使用的方法，不好的一点是太复杂了，小白不友好，不符合简单快捷的原则。\n\n# 构建函数，对不同类型自定义统计方法\npvalue &lt;- function(x, ...) {\n    # Construct vectors of data y, and groups (strata) g\n    y &lt;- unlist(x)\n    g &lt;- factor(rep(1:length(x), times=sapply(x, length)))\n    if (is.numeric(y)) {\n        # 连续型变量用方差分析\n        p &lt;- oneway.test(y ~ g)$p.value\n    } else {\n        # 分类变量采用卡方检验\n        p &lt;- chisq.test(table(y, g))$p.value\n    }\n    # 设置p值的表示形式\n    # The initial empty string places the output on the line below the variable label.\n    c(\"\", sub(\"&lt;\", \"&lt;\", format.pval(p, digits=3, eps=0.001)))\n}\n\n但是添加了P值就不能使用自定义的表头了，果然是鱼与熊掌不可兼得啊\n\ntable1(~ . | year, data = regicor[,-1], overall = F,\n       extra.col=list(`P-value`=pvalue),\n       render.continuous=c(.=\"Mean ± SD\")\n       )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1995\n(N=431)\n2000\n(N=786)\n2005\n(N=1077)\nP-value\n\n\n\n\nAge\n\n\n\n\n\n\nMean ± SD\n54.1 ± 11.7\n54.3 ± 11.2\n55.3 ± 10.6\n0.0786\n\n\nSex\n\n\n\n\n\n\nMale\n206 (47.8%)\n390 (49.6%)\n505 (46.9%)\n0.506\n\n\nFemale\n225 (52.2%)\n396 (50.4%)\n572 (53.1%)\n\n\n\nSmoking status\n\n\n\n\n\n\nNever smoker\n234 (54.3%)\n414 (52.7%)\n553 (51.3%)\n&lt;0.001\n\n\nCurrent or former &lt; 1y\n109 (25.3%)\n267 (34.0%)\n217 (20.1%)\n\n\n\nFormer &gt;= 1y\n72 (16.7%)\n77 (9.8%)\n290 (26.9%)\n\n\n\nMissing\n16 (3.7%)\n28 (3.6%)\n17 (1.6%)\n\n\n\nSystolic blood pressure\n\n\n\n\n\n\nMean ± SD\n133 ± 19.2\n133 ± 21.3\n129 ± 19.8\n&lt;0.001\n\n\nMissing\n3 (0.7%)\n11 (1.4%)\n0 (0%)\n\n\n\nDiastolic blood pressure\n\n\n\n\n\n\nMean ± SD\n77.0 ± 10.5\n80.8 ± 10.3\n79.9 ± 10.6\n&lt;0.001\n\n\nMissing\n3 (0.7%)\n11 (1.4%)\n0 (0%)\n\n\n\nHistory of hypertension\n\n\n\n\n\n\nYes\n111 (25.8%)\n233 (29.6%)\n379 (35.2%)\n&lt;0.001\n\n\nNo\n320 (74.2%)\n553 (70.4%)\n690 (64.1%)\n\n\n\nMissing\n0 (0%)\n0 (0%)\n8 (0.7%)\n\n\n\nHypertension treatment\n\n\n\n\n\n\nNo\n360 (83.5%)\n659 (83.8%)\n804 (74.7%)\n0.00152\n\n\nYes\n71 (16.5%)\n127 (16.2%)\n230 (21.4%)\n\n\n\nMissing\n0 (0%)\n0 (0%)\n43 (4.0%)\n\n\n\nTotal cholesterol\n\n\n\n\n\n\nMean ± SD\n225 ± 43.1\n224 ± 44.4\n213 ± 45.9\n&lt;0.001\n\n\nMissing\n28 (6.5%)\n71 (9.0%)\n2 (0.2%)\n\n\n\nHDL cholesterol\n\n\n\n\n\n\nMean ± SD\n51.9 ± 14.5\n52.3 ± 15.6\n53.2 ± 14.2\n0.198\n\n\nMissing\n30 (7.0%)\n38 (4.8%)\n1 (0.1%)\n\n\n\nTriglycerides\n\n\n\n\n\n\nMean ± SD\n114 ± 74.4\n114 ± 70.7\n117 ± 76.0\n0.583\n\n\nMissing\n28 (6.5%)\n34 (4.3%)\n1 (0.1%)\n\n\n\nLDL cholesterol\n\n\n\n\n\n\nMean ± SD\n152 ± 38.4\n149 ± 38.6\n136 ± 39.7\n&lt;0.001\n\n\nMissing\n43 (10.0%)\n98 (12.5%)\n27 (2.5%)\n\n\n\nHistory of hyperchol.\n\n\n\n\n\n\nYes\n97 (22.5%)\n256 (32.6%)\n356 (33.1%)\n&lt;0.001\n\n\nNo\n334 (77.5%)\n515 (65.5%)\n715 (66.4%)\n\n\n\nMissing\n0 (0%)\n15 (1.9%)\n6 (0.6%)\n\n\n\nCholesterol treatment\n\n\n\n\n\n\nNo\n403 (93.5%)\n705 (89.7%)\n903 (83.8%)\n&lt;0.001\n\n\nYes\n28 (6.5%)\n68 (8.7%)\n132 (12.3%)\n\n\n\nMissing\n0 (0%)\n13 (1.7%)\n42 (3.9%)\n\n\n\nHeight (cm)\n\n\n\n\n\n\nMean ± SD\n163 ± 9.21\n162 ± 9.39\n163 ± 9.05\n0.00368\n\n\nMissing\n8 (1.9%)\n15 (1.9%)\n12 (1.1%)\n\n\n\nWeight (Kg)\n\n\n\n\n\n\nMean ± SD\n72.3 ± 12.6\n73.8 ± 14.0\n73.6 ± 13.9\n0.12\n\n\nMissing\n8 (1.9%)\n15 (1.9%)\n12 (1.1%)\n\n\n\nBody mass index\n\n\n\n\n\n\nMean ± SD\n27.0 ± 4.15\n28.1 ± 4.62\n27.6 ± 4.63\n&lt;0.001\n\n\nMissing\n8 (1.9%)\n15 (1.9%)\n12 (1.1%)\n\n\n\nPhysical activity (Kcal/week)\n\n\n\n\n\n\nMean ± SD\n491 ± 419\n422 ± 377\n351 ± 378\n&lt;0.001\n\n\nMissing\n64 (14.8%)\n22 (2.8%)\n2 (0.2%)\n\n\n\nPhysical component\n\n\n\n\n\n\nMean ± SD\n49.3 ± 8.08\n49.0 ± 9.63\n50.1 ± 8.91\n0.0366\n\n\nMissing\n34 (7.9%)\n123 (15.6%)\n83 (7.7%)\n\n\n\nMental component\n\n\n\n\n\n\nMean ± SD\n49.2 ± 11.3\n48.9 ± 11.0\n46.9 ± 10.8\n&lt;0.001\n\n\nMissing\n34 (7.9%)\n123 (15.6%)\n83 (7.7%)\n\n\n\nCardiovascular event\n\n\n\n\n\n\nNo\n388 (90.0%)\n706 (89.8%)\n977 (90.7%)\n0.161\n\n\nYes\n10 (2.3%)\n35 (4.5%)\n47 (4.4%)\n\n\n\nMissing\n33 (7.7%)\n45 (5.7%)\n53 (4.9%)\n\n\n\nDays to cardiovascular event or end of follow-up\n\n\n\n\n\n\nMean ± SD\n1780 ± 1100\n1690 ± 1080\n1790 ± 1070\n0.0989\n\n\nMissing\n33 (7.7%)\n45 (5.7%)\n53 (4.9%)\n\n\n\nOverall death\n\n\n\n\n\n\nNo\n369 (85.6%)\n657 (83.6%)\n949 (88.1%)\n&lt;0.001\n\n\nYes\n18 (4.2%)\n81 (10.3%)\n74 (6.9%)\n\n\n\nMissing\n44 (10.2%)\n48 (6.1%)\n54 (5.0%)\n\n\n\nDays to overall death or end of follow-up\n\n\n\n\n\n\nMean ± SD\n1710 ± 1040\n1670 ± 1050\n1760 ± 1050\n0.254\n\n\nMissing\n44 (10.2%)\n48 (6.1%)\n54 (5.0%)"
  },
  {
    "objectID": "table1.html#和comparegroups比较",
    "href": "table1.html#和comparegroups比较",
    "title": "47  table1绘制三线表",
    "section": "47.5 和CompareGroups比较",
    "text": "47.5 和CompareGroups比较\nCompareGroups不能自定义复杂表头。但是胜在语法简单，自带P值！详情请参考前面的文章。"
  },
  {
    "objectID": "gt.html#安装",
    "href": "gt.html#安装",
    "title": "48  gt绘制表格",
    "section": "48.1 安装",
    "text": "48.1 安装\n\n# 2种方法选择1种\ninstall.packages(\"gt\")\n\ndevtools::install_github(\"rstudio/gt\")"
  },
  {
    "objectID": "gt.html#使用",
    "href": "gt.html#使用",
    "title": "48  gt绘制表格",
    "section": "48.2 使用",
    "text": "48.2 使用\ngt包绘制表格的理念非常先进，和ggplot2绘制图形的理念有点像，都是一点点添加细节。一个完整的表格在gt包的设计理念中可以分为以下几个部分：\n\n\n48.2.1 基础使用\n\nlibrary(gt)\nlibrary(dplyr)\n\n# 使用islands_tbl数据集演示，数据集是关于陆地的大小\nislands_tbl &lt;- \n  tibble(\n    name = names(islands),\n    size = islands\n  ) %&gt;%\n  arrange(desc(size)) %&gt;% \n  slice(1:10)\n\nislands_tbl\n## # A tibble: 10 × 2\n##    name           size\n##    &lt;chr&gt;         &lt;dbl&gt;\n##  1 Asia          16988\n##  2 Africa        11506\n##  3 North America  9390\n##  4 South America  6795\n##  5 Antarctica     5500\n##  6 Europe         3745\n##  7 Australia      2968\n##  8 Greenland       840\n##  9 New Guinea      306\n## 10 Borneo          280\n\n接下来制作一个简单的表格：\n\ngt_tbl &lt;- gt(islands_tbl)\ngt_tbl\n\n\n\n\n\n  \n    \n    \n      name\n      size\n    \n  \n  \n    Asia\n16988\n    Africa\n11506\n    North America\n9390\n    South America\n6795\n    Antarctica\n5500\n    Europe\n3745\n    Australia\n2968\n    Greenland\n840\n    New Guinea\n306\n    Borneo\n280\n  \n  \n  \n\n\n\n\n这就是一个简单表格。接下来我们就按照gt包分解表格的理念一步步添加各种细节。\n\n\n48.2.2 添加标题\n\ngt_tbl &lt;- gt_tbl %&gt;% \n  tab_header(\n    title = \"Large Landmasses of the World\",\n    subtitle = \"The top ten largest are presented\"\n  )\ngt_tbl\n\n\n\n\n\n  \n    \n      Large Landmasses of the World\n    \n    \n      The top ten largest are presented\n    \n    \n      name\n      size\n    \n  \n  \n    Asia\n16988\n    Africa\n11506\n    North America\n9390\n    South America\n6795\n    Antarctica\n5500\n    Europe\n3745\n    Australia\n2968\n    Greenland\n840\n    New Guinea\n306\n    Borneo\n280\n  \n  \n  \n\n\n\n\n更牛逼的是，这个标题支持markdown语法！\n\ngt(islands_tbl[1:2,]) %&gt;%\n  tab_header(\n    title = md(\"**Large Landmasses of the World**\"),\n    subtitle = md(\"The *top two* largest are presented\")\n  )\n\n\n\n\n\n  \n    \n      Large Landmasses of the World\n    \n    \n      The top two largest are presented\n    \n    \n      name\n      size\n    \n  \n  \n    Asia\n16988\n    Africa\n11506\n  \n  \n  \n\n\n\n\n\n\n48.2.3 添加脚注\n使用tab_source_note()函数，同样也是支持markdown语法的。\n\ngt_tbl &lt;- \n  gt_tbl %&gt;%\n  tab_source_note(\n    source_note = \"Source: The World Almanac and Book of Facts, 1975, page 406.\"\n  ) %&gt;%\n  tab_source_note(\n    source_note = md(\"Reference: McNeil, D. R. (1977) *Interactive Data Analysis*. Wiley.\")\n  )\n\ngt_tbl\n\n\n\n\n\n  \n    \n      Large Landmasses of the World\n    \n    \n      The top ten largest are presented\n    \n    \n      name\n      size\n    \n  \n  \n    Asia\n16988\n    Africa\n11506\n    North America\n9390\n    South America\n6795\n    Antarctica\n5500\n    Europe\n3745\n    Australia\n2968\n    Greenland\n840\n    New Guinea\n306\n    Borneo\n280\n  \n  \n    \n      Source: The World Almanac and Book of Facts, 1975, page 406.\n    \n    \n      Reference: McNeil, D. R. (1977) Interactive Data Analysis. Wiley.\n    \n  \n  \n\n\n\n\n添加带交叉引用的脚注：\n使用tab_footnote()函数，使用locations参数指定要添加角标的位置。\n\ngt_tbl &lt;- \n  gt_tbl %&gt;%\n  tab_footnote(\n    footnote = \"The Americas.\",\n    locations = cells_body(columns = name, rows = 3:4) # 在第3/4行，name这一列添加角标\n  )\n\ngt_tbl\n\n\n\n\n\nLarge Landmasses of the World\nThe top ten largest are presented\nname\nsize\nAsia\n16988\nAfrica\n11506\nNorth America1\n9390\nSouth America1\n6795\nAntarctica\n5500\nEurope\n3745\nAustralia\n2968\nGreenland\n840\nNew Guinea\n306\nBorneo\n280\nSource: The World Almanac and Book of Facts, 1975, page 406.\nReference: McNeil, D. R. (1977) Interactive Data Analysis. Wiley.\n1 The Americas.\n\n\n\n\n\n\n48.2.4 添加左侧边栏\n如果还不清楚左侧边栏包含哪些信息，请翻看上面那张图。\n\ngt_tbl &lt;- islands_tbl %&gt;% \n  gt(rowname_col = \"name\") %&gt;% # 使用name这一列作为左侧边栏\n  tab_stubhead(label = \"landmass\") # 添加左侧边栏的标题\n\ngt_tbl\n\n\n\n\n\n  \n    \n    \n      landmass\n      size\n    \n  \n  \n    Asia\n16988\n    Africa\n11506\n    North America\n9390\n    South America\n6795\n    Antarctica\n5500\n    Europe\n3745\n    Australia\n2968\n    Greenland\n840\n    New Guinea\n306\n    Borneo\n280\n  \n  \n  \n\n\n\n\n在上面展示的这几个陆地中，有一些是国家，有些是大洲，还有的是地区，下面我们把它分一下组：\n\ngt_tbl &lt;- gt_tbl %&gt;% \n  tab_row_group( \n    label = \"continent\",\n    rows = 1:6 # 1-6行是大洲\n  ) %&gt;% \n  tab_row_group(\n    label = \"country\",\n    rows = c(\"Australia\", \"Greenland\")\n  ) %&gt;% \n  tab_row_group(\n    label = \"subregion\",\n    rows = c(\"New Guinea\", \"Borneo\")\n  )\n\ngt_tbl\n\n\n\n\n\n  \n    \n    \n      landmass\n      size\n    \n  \n  \n    \n      subregion\n    \n    New Guinea\n306\n    Borneo\n280\n    \n      country\n    \n    Australia\n2968\n    Greenland\n840\n    \n      continent\n    \n    Asia\n16988\n    Africa\n11506\n    North America\n9390\n    South America\n6795\n    Antarctica\n5500\n    Europe\n3745\n  \n  \n  \n\n\n\n\n我们把上面展示的元素全都添加在一起：\n\ngt_tbl &lt;- islands_tbl %&gt;% \n  gt(rowname_col = \"name\") %&gt;% \n  tab_stubhead(label = \"landmass\") %&gt;% \n  tab_row_group(\n    label = \"continent\",\n    rows = 1:6\n  ) %&gt;%\n  tab_row_group(\n    label = \"country\",\n    rows = c(\"Australia\", \"Greenland\")\n  ) %&gt;%\n  tab_row_group(\n    label = \"subregion\",\n    rows = c(\"New Guinea\", \"Borneo\")\n  ) %&gt;% \n  tab_header(\n    title = \"Large Landmasses of the World\",\n    subtitle = \"The top ten largest are presented\"\n  ) %&gt;%\n  tab_source_note(\n    source_note = \"Source: The World Almanac and Book of Facts, 1975, page 406.\"\n  ) %&gt;%\n  tab_source_note(\n    source_note = md(\"Reference: McNeil, D. R. (1977) *Interactive Data Analysis*. Wiley.\")\n  ) %&gt;%\n  tab_footnote(\n    footnote = md(\"The **largest** by area.\"),\n    locations = cells_body(\n      columns = size, rows = 1\n    )\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"The lowest by population.\",\n    locations = cells_body(\n      columns = size, rows = contains(\"arc\")\n    )\n  )\n  \ngt_tbl\n\n\n\n\n\nLarge Landmasses of the World\nThe top ten largest are presented\nlandmass\nsize\nsubregion\nNew Guinea\n306\nBorneo\n280\ncountry\nAustralia\n2968\nGreenland\n840\ncontinent\nAsia\n1 16988\nAfrica\n11506\nNorth America\n9390\nSouth America\n6795\nAntarctica\n2 5500\nEurope\n3745\nSource: The World Almanac and Book of Facts, 1975, page 406.\nReference: McNeil, D. R. (1977) Interactive Data Analysis. Wiley.\n1 The largest by area.\n2 The lowest by population.\n\n\n\n\n\n\n48.2.5 增加列组别\n对不同的列进行分组是非常常见的操作，gt包提供了tab_spanner()函数实现此功能：\n\ngt_tbl &lt;- \n  gt(airquality) %&gt;%\n  tab_header(\n    title = \"New York Air Quality Measurements\",\n    subtitle = \"Daily measurements in New York City (May 1-10, 1973)\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Time\",\n    columns = c(Month, Day)\n  ) %&gt;%\n  tab_spanner(\n    label = \"Measurement\",\n    columns = c(Ozone, Solar.R, Wind, Temp)\n  )\n\ngt_tbl\n\n\n\n\n\n  \n    \n      New York Air Quality Measurements\n    \n    \n      Daily measurements in New York City (May 1-10, 1973)\n    \n    \n      \n        Measurement\n      \n      \n        Time\n      \n    \n    \n      Ozone\n      Solar.R\n      Wind\n      Temp\n      Month\n      Day\n    \n  \n  \n    41\n190\n7.4\n67\n5\n1\n    36\n118\n8.0\n72\n5\n2\n    12\n149\n12.6\n74\n5\n3\n    18\n313\n11.5\n62\n5\n4\n    NA\nNA\n14.3\n56\n5\n5\n    28\nNA\n14.9\n66\n5\n6\n    23\n299\n8.6\n65\n5\n7\n    19\n99\n13.8\n59\n5\n8\n    8\n19\n20.1\n61\n5\n9\n    NA\n194\n8.6\n69\n5\n10\n    7\nNA\n6.9\n74\n5\n11\n    16\n256\n9.7\n69\n5\n12\n    11\n290\n9.2\n66\n5\n13\n    14\n274\n10.9\n68\n5\n14\n    18\n65\n13.2\n58\n5\n15\n    14\n334\n11.5\n64\n5\n16\n    34\n307\n12.0\n66\n5\n17\n    6\n78\n18.4\n57\n5\n18\n    30\n322\n11.5\n68\n5\n19\n    11\n44\n9.7\n62\n5\n20\n    1\n8\n9.7\n59\n5\n21\n    11\n320\n16.6\n73\n5\n22\n    4\n25\n9.7\n61\n5\n23\n    32\n92\n12.0\n61\n5\n24\n    NA\n66\n16.6\n57\n5\n25\n    NA\n266\n14.9\n58\n5\n26\n    NA\nNA\n8.0\n57\n5\n27\n    23\n13\n12.0\n67\n5\n28\n    45\n252\n14.9\n81\n5\n29\n    115\n223\n5.7\n79\n5\n30\n    37\n279\n7.4\n76\n5\n31\n    NA\n286\n8.6\n78\n6\n1\n    NA\n287\n9.7\n74\n6\n2\n    NA\n242\n16.1\n67\n6\n3\n    NA\n186\n9.2\n84\n6\n4\n    NA\n220\n8.6\n85\n6\n5\n    NA\n264\n14.3\n79\n6\n6\n    29\n127\n9.7\n82\n6\n7\n    NA\n273\n6.9\n87\n6\n8\n    71\n291\n13.8\n90\n6\n9\n    39\n323\n11.5\n87\n6\n10\n    NA\n259\n10.9\n93\n6\n11\n    NA\n250\n9.2\n92\n6\n12\n    23\n148\n8.0\n82\n6\n13\n    NA\n332\n13.8\n80\n6\n14\n    NA\n322\n11.5\n79\n6\n15\n    21\n191\n14.9\n77\n6\n16\n    37\n284\n20.7\n72\n6\n17\n    20\n37\n9.2\n65\n6\n18\n    12\n120\n11.5\n73\n6\n19\n    13\n137\n10.3\n76\n6\n20\n    NA\n150\n6.3\n77\n6\n21\n    NA\n59\n1.7\n76\n6\n22\n    NA\n91\n4.6\n76\n6\n23\n    NA\n250\n6.3\n76\n6\n24\n    NA\n135\n8.0\n75\n6\n25\n    NA\n127\n8.0\n78\n6\n26\n    NA\n47\n10.3\n73\n6\n27\n    NA\n98\n11.5\n80\n6\n28\n    NA\n31\n14.9\n77\n6\n29\n    NA\n138\n8.0\n83\n6\n30\n    135\n269\n4.1\n84\n7\n1\n    49\n248\n9.2\n85\n7\n2\n    32\n236\n9.2\n81\n7\n3\n    NA\n101\n10.9\n84\n7\n4\n    64\n175\n4.6\n83\n7\n5\n    40\n314\n10.9\n83\n7\n6\n    77\n276\n5.1\n88\n7\n7\n    97\n267\n6.3\n92\n7\n8\n    97\n272\n5.7\n92\n7\n9\n    85\n175\n7.4\n89\n7\n10\n    NA\n139\n8.6\n82\n7\n11\n    10\n264\n14.3\n73\n7\n12\n    27\n175\n14.9\n81\n7\n13\n    NA\n291\n14.9\n91\n7\n14\n    7\n48\n14.3\n80\n7\n15\n    48\n260\n6.9\n81\n7\n16\n    35\n274\n10.3\n82\n7\n17\n    61\n285\n6.3\n84\n7\n18\n    79\n187\n5.1\n87\n7\n19\n    63\n220\n11.5\n85\n7\n20\n    16\n7\n6.9\n74\n7\n21\n    NA\n258\n9.7\n81\n7\n22\n    NA\n295\n11.5\n82\n7\n23\n    80\n294\n8.6\n86\n7\n24\n    108\n223\n8.0\n85\n7\n25\n    20\n81\n8.6\n82\n7\n26\n    52\n82\n12.0\n86\n7\n27\n    82\n213\n7.4\n88\n7\n28\n    50\n275\n7.4\n86\n7\n29\n    64\n253\n7.4\n83\n7\n30\n    59\n254\n9.2\n81\n7\n31\n    39\n83\n6.9\n81\n8\n1\n    9\n24\n13.8\n81\n8\n2\n    16\n77\n7.4\n82\n8\n3\n    78\nNA\n6.9\n86\n8\n4\n    35\nNA\n7.4\n85\n8\n5\n    66\nNA\n4.6\n87\n8\n6\n    122\n255\n4.0\n89\n8\n7\n    89\n229\n10.3\n90\n8\n8\n    110\n207\n8.0\n90\n8\n9\n    NA\n222\n8.6\n92\n8\n10\n    NA\n137\n11.5\n86\n8\n11\n    44\n192\n11.5\n86\n8\n12\n    28\n273\n11.5\n82\n8\n13\n    65\n157\n9.7\n80\n8\n14\n    NA\n64\n11.5\n79\n8\n15\n    22\n71\n10.3\n77\n8\n16\n    59\n51\n6.3\n79\n8\n17\n    23\n115\n7.4\n76\n8\n18\n    31\n244\n10.9\n78\n8\n19\n    44\n190\n10.3\n78\n8\n20\n    21\n259\n15.5\n77\n8\n21\n    9\n36\n14.3\n72\n8\n22\n    NA\n255\n12.6\n75\n8\n23\n    45\n212\n9.7\n79\n8\n24\n    168\n238\n3.4\n81\n8\n25\n    73\n215\n8.0\n86\n8\n26\n    NA\n153\n5.7\n88\n8\n27\n    76\n203\n9.7\n97\n8\n28\n    118\n225\n2.3\n94\n8\n29\n    84\n237\n6.3\n96\n8\n30\n    85\n188\n6.3\n94\n8\n31\n    96\n167\n6.9\n91\n9\n1\n    78\n197\n5.1\n92\n9\n2\n    73\n183\n2.8\n93\n9\n3\n    91\n189\n4.6\n93\n9\n4\n    47\n95\n7.4\n87\n9\n5\n    32\n92\n15.5\n84\n9\n6\n    20\n252\n10.9\n80\n9\n7\n    23\n220\n10.3\n78\n9\n8\n    21\n230\n10.9\n75\n9\n9\n    24\n259\n9.7\n73\n9\n10\n    44\n236\n14.9\n81\n9\n11\n    21\n259\n15.5\n76\n9\n12\n    28\n238\n6.3\n77\n9\n13\n    9\n24\n10.9\n71\n9\n14\n    13\n112\n11.5\n71\n9\n15\n    46\n237\n6.9\n78\n9\n16\n    18\n224\n13.8\n67\n9\n17\n    13\n27\n10.3\n76\n9\n18\n    24\n238\n10.3\n68\n9\n19\n    16\n201\n8.0\n82\n9\n20\n    13\n238\n12.6\n64\n9\n21\n    23\n14\n9.2\n71\n9\n22\n    36\n139\n10.3\n81\n9\n23\n    7\n49\n10.3\n69\n9\n24\n    14\n20\n16.6\n63\n9\n25\n    30\n193\n6.9\n70\n9\n26\n    NA\n145\n13.2\n77\n9\n27\n    14\n191\n14.3\n75\n9\n28\n    18\n131\n8.0\n76\n9\n29\n    20\n223\n11.5\n68\n9\n30\n  \n  \n  \n\n\n\n\n支持随意更改列的位置以及HTML语法：\n\ngt_tbl &lt;- \n  gt_tbl %&gt;%\n  cols_move_to_start( # 移到前面去\n    columns = c(Month, Day)\n  ) %&gt;%\n  cols_label(\n    Ozone = html(\"Ozone,&lt;br&gt;ppbV\"),\n    Solar.R = html(\"Solar R.,&lt;br&gt;cal/m&lt;sup&gt;2&lt;/sup&gt;\"),\n    Wind = html(\"Wind,&lt;br&gt;mph\"),\n    Temp = html(\"Temp,&lt;br&gt;&deg;F\")\n  )\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n  \n    \n      New York Air Quality Measurements\n    \n    \n      Daily measurements in New York City (May 1-10, 1973)\n    \n    \n      \n        Time\n      \n      \n        Measurement\n      \n    \n    \n      Month\n      Day\n      Ozone,ppbV\n      Solar R.,cal/m2\n      Wind,mph\n      Temp,°F\n    \n  \n  \n    5\n1\n41\n190\n7.4\n67\n    5\n2\n36\n118\n8.0\n72\n    5\n3\n12\n149\n12.6\n74\n    5\n4\n18\n313\n11.5\n62\n    5\n5\nNA\nNA\n14.3\n56\n    5\n6\n28\nNA\n14.9\n66\n    5\n7\n23\n299\n8.6\n65\n    5\n8\n19\n99\n13.8\n59\n    5\n9\n8\n19\n20.1\n61\n    5\n10\nNA\n194\n8.6\n69\n    5\n11\n7\nNA\n6.9\n74\n    5\n12\n16\n256\n9.7\n69\n    5\n13\n11\n290\n9.2\n66\n    5\n14\n14\n274\n10.9\n68\n    5\n15\n18\n65\n13.2\n58\n    5\n16\n14\n334\n11.5\n64\n    5\n17\n34\n307\n12.0\n66\n    5\n18\n6\n78\n18.4\n57\n    5\n19\n30\n322\n11.5\n68\n    5\n20\n11\n44\n9.7\n62\n    5\n21\n1\n8\n9.7\n59\n    5\n22\n11\n320\n16.6\n73\n    5\n23\n4\n25\n9.7\n61\n    5\n24\n32\n92\n12.0\n61\n    5\n25\nNA\n66\n16.6\n57\n    5\n26\nNA\n266\n14.9\n58\n    5\n27\nNA\nNA\n8.0\n57\n    5\n28\n23\n13\n12.0\n67\n    5\n29\n45\n252\n14.9\n81\n    5\n30\n115\n223\n5.7\n79\n    5\n31\n37\n279\n7.4\n76\n    6\n1\nNA\n286\n8.6\n78\n    6\n2\nNA\n287\n9.7\n74\n    6\n3\nNA\n242\n16.1\n67\n    6\n4\nNA\n186\n9.2\n84\n    6\n5\nNA\n220\n8.6\n85\n    6\n6\nNA\n264\n14.3\n79\n    6\n7\n29\n127\n9.7\n82\n    6\n8\nNA\n273\n6.9\n87\n    6\n9\n71\n291\n13.8\n90\n    6\n10\n39\n323\n11.5\n87\n    6\n11\nNA\n259\n10.9\n93\n    6\n12\nNA\n250\n9.2\n92\n    6\n13\n23\n148\n8.0\n82\n    6\n14\nNA\n332\n13.8\n80\n    6\n15\nNA\n322\n11.5\n79\n    6\n16\n21\n191\n14.9\n77\n    6\n17\n37\n284\n20.7\n72\n    6\n18\n20\n37\n9.2\n65\n    6\n19\n12\n120\n11.5\n73\n    6\n20\n13\n137\n10.3\n76\n    6\n21\nNA\n150\n6.3\n77\n    6\n22\nNA\n59\n1.7\n76\n    6\n23\nNA\n91\n4.6\n76\n    6\n24\nNA\n250\n6.3\n76\n    6\n25\nNA\n135\n8.0\n75\n    6\n26\nNA\n127\n8.0\n78\n    6\n27\nNA\n47\n10.3\n73\n    6\n28\nNA\n98\n11.5\n80\n    6\n29\nNA\n31\n14.9\n77\n    6\n30\nNA\n138\n8.0\n83\n    7\n1\n135\n269\n4.1\n84\n    7\n2\n49\n248\n9.2\n85\n    7\n3\n32\n236\n9.2\n81\n    7\n4\nNA\n101\n10.9\n84\n    7\n5\n64\n175\n4.6\n83\n    7\n6\n40\n314\n10.9\n83\n    7\n7\n77\n276\n5.1\n88\n    7\n8\n97\n267\n6.3\n92\n    7\n9\n97\n272\n5.7\n92\n    7\n10\n85\n175\n7.4\n89\n    7\n11\nNA\n139\n8.6\n82\n    7\n12\n10\n264\n14.3\n73\n    7\n13\n27\n175\n14.9\n81\n    7\n14\nNA\n291\n14.9\n91\n    7\n15\n7\n48\n14.3\n80\n    7\n16\n48\n260\n6.9\n81\n    7\n17\n35\n274\n10.3\n82\n    7\n18\n61\n285\n6.3\n84\n    7\n19\n79\n187\n5.1\n87\n    7\n20\n63\n220\n11.5\n85\n    7\n21\n16\n7\n6.9\n74\n    7\n22\nNA\n258\n9.7\n81\n    7\n23\nNA\n295\n11.5\n82\n    7\n24\n80\n294\n8.6\n86\n    7\n25\n108\n223\n8.0\n85\n    7\n26\n20\n81\n8.6\n82\n    7\n27\n52\n82\n12.0\n86\n    7\n28\n82\n213\n7.4\n88\n    7\n29\n50\n275\n7.4\n86\n    7\n30\n64\n253\n7.4\n83\n    7\n31\n59\n254\n9.2\n81\n    8\n1\n39\n83\n6.9\n81\n    8\n2\n9\n24\n13.8\n81\n    8\n3\n16\n77\n7.4\n82\n    8\n4\n78\nNA\n6.9\n86\n    8\n5\n35\nNA\n7.4\n85\n    8\n6\n66\nNA\n4.6\n87\n    8\n7\n122\n255\n4.0\n89\n    8\n8\n89\n229\n10.3\n90\n    8\n9\n110\n207\n8.0\n90\n    8\n10\nNA\n222\n8.6\n92\n    8\n11\nNA\n137\n11.5\n86\n    8\n12\n44\n192\n11.5\n86\n    8\n13\n28\n273\n11.5\n82\n    8\n14\n65\n157\n9.7\n80\n    8\n15\nNA\n64\n11.5\n79\n    8\n16\n22\n71\n10.3\n77\n    8\n17\n59\n51\n6.3\n79\n    8\n18\n23\n115\n7.4\n76\n    8\n19\n31\n244\n10.9\n78\n    8\n20\n44\n190\n10.3\n78\n    8\n21\n21\n259\n15.5\n77\n    8\n22\n9\n36\n14.3\n72\n    8\n23\nNA\n255\n12.6\n75\n    8\n24\n45\n212\n9.7\n79\n    8\n25\n168\n238\n3.4\n81\n    8\n26\n73\n215\n8.0\n86\n    8\n27\nNA\n153\n5.7\n88\n    8\n28\n76\n203\n9.7\n97\n    8\n29\n118\n225\n2.3\n94\n    8\n30\n84\n237\n6.3\n96\n    8\n31\n85\n188\n6.3\n94\n    9\n1\n96\n167\n6.9\n91\n    9\n2\n78\n197\n5.1\n92\n    9\n3\n73\n183\n2.8\n93\n    9\n4\n91\n189\n4.6\n93\n    9\n5\n47\n95\n7.4\n87\n    9\n6\n32\n92\n15.5\n84\n    9\n7\n20\n252\n10.9\n80\n    9\n8\n23\n220\n10.3\n78\n    9\n9\n21\n230\n10.9\n75\n    9\n10\n24\n259\n9.7\n73\n    9\n11\n44\n236\n14.9\n81\n    9\n12\n21\n259\n15.5\n76\n    9\n13\n28\n238\n6.3\n77\n    9\n14\n9\n24\n10.9\n71\n    9\n15\n13\n112\n11.5\n71\n    9\n16\n46\n237\n6.9\n78\n    9\n17\n18\n224\n13.8\n67\n    9\n18\n13\n27\n10.3\n76\n    9\n19\n24\n238\n10.3\n68\n    9\n20\n16\n201\n8.0\n82\n    9\n21\n13\n238\n12.6\n64\n    9\n22\n23\n14\n9.2\n71\n    9\n23\n36\n139\n10.3\n81\n    9\n24\n7\n49\n10.3\n69\n    9\n25\n14\n20\n16.6\n63\n    9\n26\n30\n193\n6.9\n70\n    9\n27\nNA\n145\n13.2\n77\n    9\n28\n14\n191\n14.3\n75\n    9\n29\n18\n131\n8.0\n76\n    9\n30\n20\n223\n11.5\n68\n  \n  \n  \n\n\n\n\n怎么样，绘制表格是不是非常方便呢？在进行数据展示的时候又多了一大利器！\n还可以添加各种格式，比如更改颜色背景、数字增加标点符号、格式化日期等。\n使用内置的gtcars数据集进行演示，这个数据集是根据mtcars数据改编而来。\n\nlibrary(gt)\nlibrary(dplyr)\n\nglimpse(gtcars)\n## Rows: 47\n## Columns: 15\n## $ mfr         &lt;chr&gt; \"Ford\", \"Ferrari\", \"Ferrari\", \"Ferrari\", \"Ferrari\", \"Ferra…\n## $ model       &lt;chr&gt; \"GT\", \"458 Speciale\", \"458 Spider\", \"458 Italia\", \"488 GTB…\n## $ year        &lt;dbl&gt; 2017, 2015, 2015, 2014, 2016, 2015, 2017, 2015, 2015, 2015…\n## $ trim        &lt;chr&gt; \"Base Coupe\", \"Base Coupe\", \"Base\", \"Base Coupe\", \"Base Co…\n## $ bdy_style   &lt;chr&gt; \"coupe\", \"coupe\", \"convertible\", \"coupe\", \"coupe\", \"conver…\n## $ hp          &lt;dbl&gt; 647, 597, 562, 562, 661, 553, 680, 652, 731, 949, 573, 545…\n## $ hp_rpm      &lt;dbl&gt; 6250, 9000, 9000, 9000, 8000, 7500, 8250, 8000, 8250, 9000…\n## $ trq         &lt;dbl&gt; 550, 398, 398, 398, 561, 557, 514, 504, 509, 664, 476, 436…\n## $ trq_rpm     &lt;dbl&gt; 5900, 6000, 6000, 6000, 3000, 4750, 5750, 6000, 6000, 6750…\n## $ mpg_c       &lt;dbl&gt; 11, 13, 13, 13, 15, 16, 12, 11, 11, 12, 21, 16, 11, 16, 12…\n## $ mpg_h       &lt;dbl&gt; 18, 17, 17, 17, 22, 23, 17, 16, 16, 16, 22, 22, 18, 20, 20…\n## $ drivetrain  &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"rwd\", \"rwd\", \"rwd\", \"awd\", \"awd\", \"r…\n## $ trsmn       &lt;chr&gt; \"7a\", \"7a\", \"7a\", \"7a\", \"7a\", \"7a\", \"7a\", \"7a\", \"7a\", \"7a\"…\n## $ ctry_origin &lt;chr&gt; \"United States\", \"Italy\", \"Italy\", \"Italy\", \"Italy\", \"Ital…\n## $ msrp        &lt;dbl&gt; 447000, 291744, 263553, 233509, 245400, 198973, 298000, 29…\n\n为了方便演示，我们截取部分数据：\n\ngtcars_8 &lt;-\n  gtcars %&gt;%\n  group_by(ctry_origin) %&gt;%\n  slice_head(n = 2) %&gt;%\n  ungroup() %&gt;%\n  filter(ctry_origin != \"United Kingdom\")\n\nglimpse(gtcars_8)\n## Rows: 8\n## Columns: 15\n## $ mfr         &lt;chr&gt; \"BMW\", \"BMW\", \"Ferrari\", \"Ferrari\", \"Acura\", \"Nissan\", \"Fo…\n## $ model       &lt;chr&gt; \"6-Series\", \"i8\", \"458 Speciale\", \"458 Spider\", \"NSX\", \"GT…\n## $ year        &lt;dbl&gt; 2016, 2016, 2015, 2015, 2017, 2016, 2017, 2016\n## $ trim        &lt;chr&gt; \"640 I Coupe\", \"Mega World Coupe\", \"Base Coupe\", \"Base\", \"…\n## $ bdy_style   &lt;chr&gt; \"coupe\", \"coupe\", \"coupe\", \"convertible\", \"coupe\", \"coupe\"…\n## $ hp          &lt;dbl&gt; 315, 357, 597, 562, 573, 545, 647, 650\n## $ hp_rpm      &lt;dbl&gt; 5800, 5800, 9000, 9000, 6500, 6400, 6250, 6400\n## $ trq         &lt;dbl&gt; 330, 420, 398, 398, 476, 436, 550, 650\n## $ trq_rpm     &lt;dbl&gt; 1400, 3700, 6000, 6000, 2000, 3200, 5900, 3600\n## $ mpg_c       &lt;dbl&gt; 20, 28, 13, 13, 21, 16, 11, 15\n## $ mpg_h       &lt;dbl&gt; 30, 29, 17, 17, 22, 22, 18, 22\n## $ drivetrain  &lt;chr&gt; \"rwd\", \"awd\", \"rwd\", \"rwd\", \"awd\", \"awd\", \"rwd\", \"rwd\"\n## $ trsmn       &lt;chr&gt; \"8am\", \"6am\", \"7a\", \"7a\", \"9a\", \"6a\", \"7a\", \"7m\"\n## $ ctry_origin &lt;chr&gt; \"Germany\", \"Germany\", \"Italy\", \"Italy\", \"Japan\", \"Japan\", …\n## $ msrp        &lt;dbl&gt; 77300, 140700, 291744, 263553, 156000, 101770, 447000, 883…"
  },
  {
    "objectID": "gt.html#分组操作",
    "href": "gt.html#分组操作",
    "title": "48  gt绘制表格",
    "section": "48.3 分组操作",
    "text": "48.3 分组操作\n支持和tidyverse系列，比如使用group_by()函数：\n\ntab &lt;- gtcars_8 %&gt;% \n  group_by(ctry_origin) %&gt;% \n  arrange(mfr, desc(msrp)) %&gt;% \n  gt()\n\ntab\n\n\n\n\n\n  \n    \n    \n      mfr\n      model\n      year\n      trim\n      bdy_style\n      hp\n      hp_rpm\n      trq\n      trq_rpm\n      mpg_c\n      mpg_h\n      drivetrain\n      trsmn\n      msrp\n    \n  \n  \n    \n      Japan\n    \n    Acura\nNSX\n2017\nBase Coupe\ncoupe\n573\n6500\n476\n2000\n21\n22\nawd\n9a\n156000\n    Nissan\nGT-R\n2016\nPremium Coupe\ncoupe\n545\n6400\n436\n3200\n16\n22\nawd\n6a\n101770\n    \n      Germany\n    \n    BMW\ni8\n2016\nMega World Coupe\ncoupe\n357\n5800\n420\n3700\n28\n29\nawd\n6am\n140700\n    BMW\n6-Series\n2016\n640 I Coupe\ncoupe\n315\n5800\n330\n1400\n20\n30\nrwd\n8am\n77300\n    \n      United States\n    \n    Chevrolet\nCorvette\n2016\nZ06 Coupe\ncoupe\n650\n6400\n650\n3600\n15\n22\nrwd\n7m\n88345\n    Ford\nGT\n2017\nBase Coupe\ncoupe\n647\n6250\n550\n5900\n11\n18\nrwd\n7a\n447000\n    \n      Italy\n    \n    Ferrari\n458 Speciale\n2015\nBase Coupe\ncoupe\n597\n9000\n398\n6000\n13\n17\nrwd\n7a\n291744\n    Ferrari\n458 Spider\n2015\nBase\nconvertible\n562\n9000\n398\n6000\n13\n17\nrwd\n7a\n263553"
  },
  {
    "objectID": "gt.html#隐藏移动某些列",
    "href": "gt.html#隐藏移动某些列",
    "title": "48  gt绘制表格",
    "section": "48.4 隐藏、移动某些列",
    "text": "48.4 隐藏、移动某些列\n\ntab &lt;- \n  tab %&gt;%\n  cols_hide(columns = c(drivetrain, bdy_style)) %&gt;% # 隐藏列\n  cols_move( # 移动列\n    columns = c(trsmn, mpg_c, mpg_h),\n    after = trim\n  )\n\ntab\n\n\n\n\n\n  \n    \n    \n      mfr\n      model\n      year\n      trim\n      trsmn\n      mpg_c\n      mpg_h\n      hp\n      hp_rpm\n      trq\n      trq_rpm\n      msrp\n    \n  \n  \n    \n      Japan\n    \n    Acura\nNSX\n2017\nBase Coupe\n9a\n21\n22\n573\n6500\n476\n2000\n156000\n    Nissan\nGT-R\n2016\nPremium Coupe\n6a\n16\n22\n545\n6400\n436\n3200\n101770\n    \n      Germany\n    \n    BMW\ni8\n2016\nMega World Coupe\n6am\n28\n29\n357\n5800\n420\n3700\n140700\n    BMW\n6-Series\n2016\n640 I Coupe\n8am\n20\n30\n315\n5800\n330\n1400\n77300\n    \n      United States\n    \n    Chevrolet\nCorvette\n2016\nZ06 Coupe\n7m\n15\n22\n650\n6400\n650\n3600\n88345\n    Ford\nGT\n2017\nBase Coupe\n7a\n11\n18\n647\n6250\n550\n5900\n447000\n    \n      Italy\n    \n    Ferrari\n458 Speciale\n2015\nBase Coupe\n7a\n13\n17\n597\n9000\n398\n6000\n291744\n    Ferrari\n458 Spider\n2015\nBase\n7a\n13\n17\n562\n9000\n398\n6000\n263553"
  },
  {
    "objectID": "gt.html#列分组操作",
    "href": "gt.html#列分组操作",
    "title": "48  gt绘制表格",
    "section": "48.5 列分组操作",
    "text": "48.5 列分组操作\n列分组操作非常常见，有时我们需要一个小表头，把不同的列聚在一起。\n可以通过tab_spanner()函数实现：\n\ntab &lt;- tab %&gt;% \n  tab_spanner(label = \"Performance\",\n              columns = c(mpg_c,mpg_h,hp,hp_rpm,trq,trq_rpm)\n              )\n\ntab\n\n\n\n\n\n  \n    \n    \n      mfr\n      model\n      year\n      trim\n      trsmn\n      \n        Performance\n      \n      msrp\n    \n    \n      mpg_c\n      mpg_h\n      hp\n      hp_rpm\n      trq\n      trq_rpm\n    \n  \n  \n    \n      Japan\n    \n    Acura\nNSX\n2017\nBase Coupe\n9a\n21\n22\n573\n6500\n476\n2000\n156000\n    Nissan\nGT-R\n2016\nPremium Coupe\n6a\n16\n22\n545\n6400\n436\n3200\n101770\n    \n      Germany\n    \n    BMW\ni8\n2016\nMega World Coupe\n6am\n28\n29\n357\n5800\n420\n3700\n140700\n    BMW\n6-Series\n2016\n640 I Coupe\n8am\n20\n30\n315\n5800\n330\n1400\n77300\n    \n      United States\n    \n    Chevrolet\nCorvette\n2016\nZ06 Coupe\n7m\n15\n22\n650\n6400\n650\n3600\n88345\n    Ford\nGT\n2017\nBase Coupe\n7a\n11\n18\n647\n6250\n550\n5900\n447000\n    \n      Italy\n    \n    Ferrari\n458 Speciale\n2015\nBase Coupe\n7a\n13\n17\n597\n9000\n398\n6000\n291744\n    Ferrari\n458 Spider\n2015\nBase\n7a\n13\n17\n562\n9000\n398\n6000\n263553"
  },
  {
    "objectID": "gt.html#合并列-添加标签",
    "href": "gt.html#合并列-添加标签",
    "title": "48  gt绘制表格",
    "section": "48.6 合并列 & 添加标签",
    "text": "48.6 合并列 & 添加标签\n和dplyr包中的union()函数功能差不多。\n一次合并2列，第一列的列名会被保留，第2列的列名会被丢弃，默认使用{1} & {2}代替第一列、第二列，支持HTML语法\n\ntab &lt;- tab %&gt;% \n  cols_merge(columns = c(mpg_c,mpg_h),\n             pattern = \"{1}c&lt;br&gt;{2}h\" # html语法添加空格\n             ) %&gt;% \n  cols_merge(\n    columns = c(hp, hp_rpm),\n    pattern = \"{1}&lt;br&gt;@{2}rpm\" # html语法添加空格和文字\n  ) %&gt;%\n  cols_merge(\n    columns = c(trq, trq_rpm),\n    pattern = \"{1}&lt;br&gt;@{2}rpm\"\n  ) %&gt;%\n  cols_label(\n    mpg_c = \"MPG\",\n    hp = \"HP\",\n    trq = \"Torque\",\n    year = \"Year\",\n    trim = \"Trim\",\n    trsmn = \"Transmission\",\n    msrp = \"MSRP\"\n  )\n\ntab\n\n\n\n\n\n  \n    \n    \n      mfr\n      model\n      Year\n      Trim\n      Transmission\n      \n        Performance\n      \n      MSRP\n    \n    \n      MPG\n      HP\n      Torque\n    \n  \n  \n    \n      Japan\n    \n    Acura\nNSX\n2017\nBase Coupe\n9a\n21c22h\n573@6500rpm\n476@2000rpm\n156000\n    Nissan\nGT-R\n2016\nPremium Coupe\n6a\n16c22h\n545@6400rpm\n436@3200rpm\n101770\n    \n      Germany\n    \n    BMW\ni8\n2016\nMega World Coupe\n6am\n28c29h\n357@5800rpm\n420@3700rpm\n140700\n    BMW\n6-Series\n2016\n640 I Coupe\n8am\n20c30h\n315@5800rpm\n330@1400rpm\n77300\n    \n      United States\n    \n    Chevrolet\nCorvette\n2016\nZ06 Coupe\n7m\n15c22h\n650@6400rpm\n650@3600rpm\n88345\n    Ford\nGT\n2017\nBase Coupe\n7a\n11c18h\n647@6250rpm\n550@5900rpm\n447000\n    \n      Italy\n    \n    Ferrari\n458 Speciale\n2015\nBase Coupe\n7a\n13c17h\n597@9000rpm\n398@6000rpm\n291744\n    Ferrari\n458 Spider\n2015\nBase\n7a\n13c17h\n562@9000rpm\n398@6000rpm\n263553"
  },
  {
    "objectID": "gt.html#使用格式化功能",
    "href": "gt.html#使用格式化功能",
    "title": "48  gt绘制表格",
    "section": "48.7 使用格式化功能",
    "text": "48.7 使用格式化功能\n支持对数字、货币、日期时间等格式进行各种方便的格式化操作，使呈现方式更加专业、美观。\n\ntab &lt;- \n  tab %&gt;%\n  fmt_currency(\n    columns = msrp,\n    currency = \"USD\",\n    decimals = 0\n  )\n\ntab\n\n\n\n\n\n  \n    \n    \n      mfr\n      model\n      Year\n      Trim\n      Transmission\n      \n        Performance\n      \n      MSRP\n    \n    \n      MPG\n      HP\n      Torque\n    \n  \n  \n    \n      Japan\n    \n    Acura\nNSX\n2017\nBase Coupe\n9a\n21c22h\n573@6500rpm\n476@2000rpm\n$156,000\n    Nissan\nGT-R\n2016\nPremium Coupe\n6a\n16c22h\n545@6400rpm\n436@3200rpm\n$101,770\n    \n      Germany\n    \n    BMW\ni8\n2016\nMega World Coupe\n6am\n28c29h\n357@5800rpm\n420@3700rpm\n$140,700\n    BMW\n6-Series\n2016\n640 I Coupe\n8am\n20c30h\n315@5800rpm\n330@1400rpm\n$77,300\n    \n      United States\n    \n    Chevrolet\nCorvette\n2016\nZ06 Coupe\n7m\n15c22h\n650@6400rpm\n650@3600rpm\n$88,345\n    Ford\nGT\n2017\nBase Coupe\n7a\n11c18h\n647@6250rpm\n550@5900rpm\n$447,000\n    \n      Italy\n    \n    Ferrari\n458 Speciale\n2015\nBase Coupe\n7a\n13c17h\n597@9000rpm\n398@6000rpm\n$291,744\n    Ferrari\n458 Spider\n2015\nBase\n7a\n13c17h\n562@9000rpm\n398@6000rpm\n$263,553"
  },
  {
    "objectID": "gt.html#对齐方式及风格",
    "href": "gt.html#对齐方式及风格",
    "title": "48  gt绘制表格",
    "section": "48.8 对齐方式及风格",
    "text": "48.8 对齐方式及风格\n使用cols_align()函数更改对齐方式； 使用tab_style()函数更改主题风格、颜色背景等\n\ntab &lt;- \n  tab %&gt;%\n  cols_align( # 某些列使用居中对齐\n    align = \"center\",\n    columns = c(mpg_c, hp, trq)\n  ) %&gt;%\n  tab_style( # 更改字体外观\n    style = cell_text(size = px(12),color=\"black\"),\n    locations = cells_body(\n      columns = c(trim, trsmn, mpg_c, hp, trq)\n    )\n  )\n\ntab\n\n\n\n\n\n  \n    \n    \n      mfr\n      model\n      Year\n      Trim\n      Transmission\n      \n        Performance\n      \n      MSRP\n    \n    \n      MPG\n      HP\n      Torque\n    \n  \n  \n    \n      Japan\n    \n    Acura\nNSX\n2017\nBase Coupe\n9a\n21c22h\n573@6500rpm\n476@2000rpm\n$156,000\n    Nissan\nGT-R\n2016\nPremium Coupe\n6a\n16c22h\n545@6400rpm\n436@3200rpm\n$101,770\n    \n      Germany\n    \n    BMW\ni8\n2016\nMega World Coupe\n6am\n28c29h\n357@5800rpm\n420@3700rpm\n$140,700\n    BMW\n6-Series\n2016\n640 I Coupe\n8am\n20c30h\n315@5800rpm\n330@1400rpm\n$77,300\n    \n      United States\n    \n    Chevrolet\nCorvette\n2016\nZ06 Coupe\n7m\n15c22h\n650@6400rpm\n650@3600rpm\n$88,345\n    Ford\nGT\n2017\nBase Coupe\n7a\n11c18h\n647@6250rpm\n550@5900rpm\n$447,000\n    \n      Italy\n    \n    Ferrari\n458 Speciale\n2015\nBase Coupe\n7a\n13c17h\n597@9000rpm\n398@6000rpm\n$291,744\n    Ferrari\n458 Spider\n2015\nBase\n7a\n13c17h\n562@9000rpm\n398@6000rpm\n$263,553"
  },
  {
    "objectID": "gt.html#主体的字体美化",
    "href": "gt.html#主体的字体美化",
    "title": "48  gt绘制表格",
    "section": "48.9 主体的字体美化",
    "text": "48.9 主体的字体美化\n可以使用text_transform()函数继续美化cell_body部分的字体。\n\ntab &lt;- \n  tab %&gt;%\n  text_transform(\n    locations = cells_body(columns = trsmn), # 定位需要美化的位置transmission列\n    fn = function(x) {\n      \n      # transmission这一列中每行的第一个字符表示speed\n      speed &lt;- substr(x, 1, 1)\n      \n      # 第2-3个字符表示type,共分成4中type\n      type &lt;-\n        dplyr::case_when(\n          substr(x, 2, 3) == \"am\" ~ \"Automatic/Manual\",\n          substr(x, 2, 2) == \"m\" ~ \"Manual\",\n          substr(x, 2, 2) == \"a\" ~ \"Automatic\",\n          substr(x, 2, 3) == \"dd\" ~ \"Direct Drive\"\n        )\n      \n      # 把speed和type拼在一起\n      paste(speed, \" Speed&lt;br&gt;&lt;em&gt;\", type, \"&lt;/em&gt;\")\n    }\n  )\n\ntab\n\n\n\n\n\n  \n    \n    \n      mfr\n      model\n      Year\n      Trim\n      Transmission\n      \n        Performance\n      \n      MSRP\n    \n    \n      MPG\n      HP\n      Torque\n    \n  \n  \n    \n      Japan\n    \n    Acura\nNSX\n2017\nBase Coupe\n9  Speed Automatic \n21c22h\n573@6500rpm\n476@2000rpm\n$156,000\n    Nissan\nGT-R\n2016\nPremium Coupe\n6  Speed Automatic \n16c22h\n545@6400rpm\n436@3200rpm\n$101,770\n    \n      Germany\n    \n    BMW\ni8\n2016\nMega World Coupe\n6  Speed Automatic/Manual \n28c29h\n357@5800rpm\n420@3700rpm\n$140,700\n    BMW\n6-Series\n2016\n640 I Coupe\n8  Speed Automatic/Manual \n20c30h\n315@5800rpm\n330@1400rpm\n$77,300\n    \n      United States\n    \n    Chevrolet\nCorvette\n2016\nZ06 Coupe\n7  Speed Manual \n15c22h\n650@6400rpm\n650@3600rpm\n$88,345\n    Ford\nGT\n2017\nBase Coupe\n7  Speed Automatic \n11c18h\n647@6250rpm\n550@5900rpm\n$447,000\n    \n      Italy\n    \n    Ferrari\n458 Speciale\n2015\nBase Coupe\n7  Speed Automatic \n13c17h\n597@9000rpm\n398@6000rpm\n$291,744\n    Ferrari\n458 Spider\n2015\nBase\n7  Speed Automatic \n13c17h\n562@9000rpm\n398@6000rpm\n$263,553"
  },
  {
    "objectID": "gt.html#标题和副标题",
    "href": "gt.html#标题和副标题",
    "title": "48  gt绘制表格",
    "section": "48.10 标题和副标题",
    "text": "48.10 标题和副标题\n\ntab &lt;- \n  tab %&gt;%\n  tab_header(\n    title = md(\"The Cars of **gtcars**\"),\n    subtitle = \"These are some fine automobiles\"\n  )\n\ntab\n\n\n\n\n\n  \n    \n      The Cars of gtcars\n    \n    \n      These are some fine automobiles\n    \n    \n      mfr\n      model\n      Year\n      Trim\n      Transmission\n      \n        Performance\n      \n      MSRP\n    \n    \n      MPG\n      HP\n      Torque\n    \n  \n  \n    \n      Japan\n    \n    Acura\nNSX\n2017\nBase Coupe\n9  Speed Automatic \n21c22h\n573@6500rpm\n476@2000rpm\n$156,000\n    Nissan\nGT-R\n2016\nPremium Coupe\n6  Speed Automatic \n16c22h\n545@6400rpm\n436@3200rpm\n$101,770\n    \n      Germany\n    \n    BMW\ni8\n2016\nMega World Coupe\n6  Speed Automatic/Manual \n28c29h\n357@5800rpm\n420@3700rpm\n$140,700\n    BMW\n6-Series\n2016\n640 I Coupe\n8  Speed Automatic/Manual \n20c30h\n315@5800rpm\n330@1400rpm\n$77,300\n    \n      United States\n    \n    Chevrolet\nCorvette\n2016\nZ06 Coupe\n7  Speed Manual \n15c22h\n650@6400rpm\n650@3600rpm\n$88,345\n    Ford\nGT\n2017\nBase Coupe\n7  Speed Automatic \n11c18h\n647@6250rpm\n550@5900rpm\n$447,000\n    \n      Italy\n    \n    Ferrari\n458 Speciale\n2015\nBase Coupe\n7  Speed Automatic \n13c17h\n597@9000rpm\n398@6000rpm\n$291,744\n    Ferrari\n458 Spider\n2015\nBase\n7  Speed Automatic \n13c17h\n562@9000rpm\n398@6000rpm\n$263,553"
  },
  {
    "objectID": "gt.html#添加脚注-1",
    "href": "gt.html#添加脚注-1",
    "title": "48  gt绘制表格",
    "section": "48.11 添加脚注",
    "text": "48.11 添加脚注\n\ntab &lt;- \n  tab %&gt;%\n  tab_source_note(\n    source_note = md(\n      \"Source: Various pages within the Edmonds website.\"\n    )\n  )\n\ntab\n\n\n\n\n\n  \n    \n      The Cars of gtcars\n    \n    \n      These are some fine automobiles\n    \n    \n      mfr\n      model\n      Year\n      Trim\n      Transmission\n      \n        Performance\n      \n      MSRP\n    \n    \n      MPG\n      HP\n      Torque\n    \n  \n  \n    \n      Japan\n    \n    Acura\nNSX\n2017\nBase Coupe\n9  Speed Automatic \n21c22h\n573@6500rpm\n476@2000rpm\n$156,000\n    Nissan\nGT-R\n2016\nPremium Coupe\n6  Speed Automatic \n16c22h\n545@6400rpm\n436@3200rpm\n$101,770\n    \n      Germany\n    \n    BMW\ni8\n2016\nMega World Coupe\n6  Speed Automatic/Manual \n28c29h\n357@5800rpm\n420@3700rpm\n$140,700\n    BMW\n6-Series\n2016\n640 I Coupe\n8  Speed Automatic/Manual \n20c30h\n315@5800rpm\n330@1400rpm\n$77,300\n    \n      United States\n    \n    Chevrolet\nCorvette\n2016\nZ06 Coupe\n7  Speed Manual \n15c22h\n650@6400rpm\n650@3600rpm\n$88,345\n    Ford\nGT\n2017\nBase Coupe\n7  Speed Automatic \n11c18h\n647@6250rpm\n550@5900rpm\n$447,000\n    \n      Italy\n    \n    Ferrari\n458 Speciale\n2015\nBase Coupe\n7  Speed Automatic \n13c17h\n597@9000rpm\n398@6000rpm\n$291,744\n    Ferrari\n458 Spider\n2015\nBase\n7  Speed Automatic \n13c17h\n562@9000rpm\n398@6000rpm\n$263,553\n  \n  \n    \n      Source: Various pages within the Edmonds website.\n    \n  \n  \n\n\n\n\nOK，手工，这就是gt包的常见功能了。"
  },
  {
    "objectID": "gtExtra.html#安装",
    "href": "gtExtra.html#安装",
    "title": "49  gtExtras美化表格",
    "section": "49.1 安装",
    "text": "49.1 安装\n2选1：\n\ninstall.packages(\"gtExtras\")\n# if needed install.packages(\"remotes\")\nremotes::install_github(\"jthomasmock/gtExtras\")"
  },
  {
    "objectID": "gtExtra.html#快速上手",
    "href": "gtExtra.html#快速上手",
    "title": "49  gtExtras美化表格",
    "section": "49.2 快速上手",
    "text": "49.2 快速上手\n\n49.2.1 fmt_symbol_first\ngt中提供了非常好用的格式化功能，而这个函数可以只格式化一列的第一行，包括添加各种符号等，然后在其余行的最后添加空格，达到对齐的效果。\n\nlibrary(gtExtras)\n## Loading required package: gt\nlibrary(gt)\n\ngtcars %&gt;%\n  head() %&gt;%\n  dplyr::select(mfr, year, bdy_style, mpg_h, hp) %&gt;%\n  dplyr::mutate(mpg_h = rnorm(n = dplyr::n(), mean = 22, sd = 1)) %&gt;%\n  gt::gt() %&gt;%\n  gt::opt_table_lines() %&gt;%\n  fmt_symbol_first(column = mfr, symbol = \"&#x24;\", last_row_n = 6) %&gt;%\n  fmt_symbol_first(column = year, suffix = \"%\") %&gt;%\n  fmt_symbol_first(column = mpg_h, symbol = \"&#37;\", decimals = 1) %&gt;%\n  fmt_symbol_first(hp, symbol = \"&#176;\", suffix = \"F\", symbol_first = TRUE)\n\n\n\n\n\n  \n    \n    \n      mfr\n      year\n      bdy_style\n      mpg_h\n      hp\n    \n  \n  \n    Ford$\n2017%\ncoupe\n21.5%\n647°F\n    Ferrari$\n2015%\ncoupe\n22.4%\n597°F\n    Ferrari$\n2015%\nconvertible\n22.2%\n562°F\n    Ferrari$\n2014%\ncoupe\n21.5%\n562°F\n    Ferrari$\n2016%\ncoupe\n22.4%\n661°F\n    Ferrari$\n2015%\nconvertible\n20.9%\n553°F\n  \n  \n  \n\n\n\n\n\n\n49.2.2 pad_fn\n可以用于快速对齐有小数点的数字。\n\ndata.frame(x = c(1.2345, 12.345, 123.45, 1234.5, 12345)) %&gt;%\n  gt() %&gt;%\n  # 4位小数，不够的用0填充\n  fmt(fns = function(x){pad_fn(x, nsmall = 4, pad0 = T)}) %&gt;%\n  tab_style(\n    # 设置字体\n    style = cell_text(font = google_font(\"Fira Mono\")),\n    locations = cells_body(columns = x)\n    )\n\n\n\n\n\n  \n    \n    \n      x\n    \n  \n  \n        1.2345\n       12.3450\n      123.4500\n     1234.5000\n    12345.0000\n  \n  \n  \n\n\n\n\n\n\n49.2.3 主题\n提供了多套主题\n\nhead(mtcars) %&gt;%\n  gt() %&gt;% \n  gt_theme_538()\n\n\n\n\n\n  \n    \n    \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n    21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n    22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n    21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n    18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n    18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n  \n  \n  \n\n\n\n\n\nhead(mtcars) %&gt;%\n  gt() %&gt;% \n  gt_theme_espn()\n\n\n\n\n\n  \n    \n    \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n    21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n    22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n    21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n    18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n    18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n  \n  \n  \n\n\n\n\n\nhead(mtcars) %&gt;% \n  gt() %&gt;% \n  gt_theme_nytimes() %&gt;% \n  tab_header(title = \"Table styled like the NY Times\")\n\n\n\n\n\n  \n    \n      Table styled like the NY Times\n    \n    \n    \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n    21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n    22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n    21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n    18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n    18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n  \n  \n  \n\n\n\n\n\n\n49.2.4 给特定行或列上色\ngt_hulk_col_numerical()，数值从小到大，颜色渐变为从紫色到绿色。\n\nhead(mtcars) %&gt;%\n  gt::gt() %&gt;%\n  gt_hulk_col_numeric(mpg) # 只给mpg这一列上色\n\n\n\n\n\n  \n    \n    \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n    21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n    22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n    21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n    18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n    18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n  \n  \n  \n\n\n\n\n可以反转颜色：\n\nhead(mtcars) %&gt;%\n  gt::gt() %&gt;%\n  # 多列上色，并反转颜色\n  gt_hulk_col_numeric(mpg:disp, reverse = FALSE) \n\n\n\n\n\n  \n    \n    \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n    21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n    22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n    21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n    18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n    18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n  \n  \n  \n\n\n\n\ngt_color_rows()也是给列上色的，不知为啥要叫row。。。默认是红色渐变，支持其他主题的扩展！\n\nmtcars %&gt;%\n  head() %&gt;%\n  gt() %&gt;%\n  gt_color_rows(mpg:disp, palette = \"ggsci::red_material\")\n## Warning: Domain not specified, defaulting to observed range within each\n## specified column.\n\n\n\n\n\n  \n    \n    \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n    21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n    22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n    21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n    18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n    18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n  \n  \n  \n\n\n\n\n还支持自定义颜色：\n\nmtcars %&gt;%\n  head() %&gt;%\n  gt() %&gt;%\n  gt_color_rows(\n    mpg:disp, \n    palette = c(\"white\", \"green\") # 也可以用16进制颜色\n    )\n## Warning: Domain not specified, defaulting to observed range within each\n## specified column.\n\n\n\n\n\n  \n    \n    \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n    21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n    22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n    21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n    18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n    18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n  \n  \n  \n\n\n\n\n离散型变量也支持使用颜色：\n\nmtcars %&gt;%\n  head() %&gt;%\n  gt() %&gt;%\n  gt_color_rows(\n    cyl, \n    pal_type = \"discrete\",\n    palette = \"ggthemes::colorblind\", \n    # 支持 c(4,6,8) 这种格式\n    domain = range(mtcars$cyl)\n   )\n\n\n\n\n\n  \n    \n    \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n    21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n    22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n    21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n    18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n    18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n  \n  \n  \n\n\n\n\n\n\n49.2.5 高亮某些行\n\nhead(mtcars[,1:5]) %&gt;% \n  tibble::rownames_to_column(\"car\") %&gt;% \n  gt() %&gt;% \n  gt_highlight_rows(\n    rows = 5, # 哪一行\n    fill = \"lightgrey\", # 背景色\n    font_weight = \"bold\"\n    )\n\n\n\n\n\n  \n    \n    \n      car\n      mpg\n      cyl\n      disp\n      hp\n      drat\n    \n  \n  \n    Mazda RX4\n21.0\n6\n160\n110\n3.90\n    Mazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n    Datsun 710\n22.8\n4\n108\n93\n3.85\n    Hornet 4 Drive\n21.4\n6\n258\n110\n3.08\n    Hornet Sportabout\n18.7\n8\n360\n175\n3.15\n    Valiant\n18.1\n6\n225\n105\n2.76"
  },
  {
    "objectID": "gtExtra.html#支持各种行内图形",
    "href": "gtExtra.html#支持各种行内图形",
    "title": "49  gtExtras美化表格",
    "section": "49.3 支持各种行内图形",
    "text": "49.3 支持各种行内图形\n\n49.3.1 gt_sparkline\n可以是折线图/面积图/直方图等。\n\nmtcars %&gt;%\n   dplyr::group_by(cyl) %&gt;%\n   dplyr::summarize(mpg_data = list(mpg), .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline(mpg_data)\n\n\n\n\n\n  \n    \n    \n      cyl\n      mpg_data\n    \n  \n  \n    4\n          21.4\n    6\n          19.7\n    8\n          15.0\n  \n  \n  \n\n\n\n\n通过更改参数，可以变成面积图或者直方图：\n\nmtcars %&gt;%\n   dplyr::group_by(cyl) %&gt;%\n   dplyr::summarize(mpg_data = list(mpg), .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline(mpg_data,type = \"shaded\")\n\n\n\n\n\n  \n    \n    \n      cyl\n      mpg_data\n    \n  \n  \n    4\n          21.4\n    6\n          19.7\n    8\n          15.0\n  \n  \n  \n\n\n\n\n\nmtcars %&gt;% \n  dplyr::group_by(cyl) %&gt;% \n  dplyr::summarise(mpg_data=list(mpg),.groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  gt_plt_sparkline(mpg_data,type = \"ref_mean\")\n\n\n\n\n\n  \n    \n    \n      cyl\n      mpg_data\n    \n  \n  \n    4\n          21.4\n    6\n          19.7\n    8\n          15.0\n  \n  \n  \n\n\n\n\n\n\n49.3.2 密度图\n\nmtcars %&gt;% \n  dplyr::group_by(cyl) %&gt;% \n  dplyr::summarise(mpg_data=list(mpg),.groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  gt_plt_dist(mpg_data,type = \"density\", line_color = \"blue\", \n                         fill_color = \"red\")\n\n\n\n\n\n  \n    \n    \n      cyl\n      mpg_data\n    \n  \n  \n    4\n          \n    6\n          \n    8\n          \n  \n  \n  \n\n\n\n\n\nmtcars %&gt;% \n  dplyr::group_by(cyl) %&gt;% \n  dplyr::summarise(mpg_data=list(mpg),.groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  gt_plt_dist(mpg_data,type = \"histogram\", line_color = \"blue\", \n                         fill_color = \"red\")\n\n\n\n\n\n  \n    \n    \n      cyl\n      mpg_data\n    \n  \n  \n    4\n          \n    6\n          \n    8\n          \n  \n  \n  \n\n\n\n\n\n\n49.3.3 条形图\n\nmtcars %&gt;% \n  dplyr::select(cyl:wt,mpg) %&gt;% \n  head() %&gt;% \n  gt() %&gt;% \n  gt_plt_bar(column = mpg,\n             keep_column = T,\n             width = 35, # 条形宽度\n             color = \"firebrick\", # 条形颜色\n             scale_type = \"number\", # 添加标签\n             text_color = \"white\" # 标签颜色\n             )\n\n\n\n\n\n  \n    \n    \n      cyl\n      disp\n      hp\n      drat\n      wt\n      mpg\n      mpg\n    \n  \n  \n    6\n160\n110\n3.90\n2.620\n21.0\n          21\n    6\n160\n110\n3.90\n2.875\n21.0\n          21\n    4\n108\n93\n3.85\n2.320\n22.8\n          23\n    6\n258\n110\n3.08\n3.215\n21.4\n          21\n    8\n360\n175\n3.15\n3.440\n18.7\n          19\n    6\n225\n105\n2.76\n3.460\n18.1\n          18\n  \n  \n  \n\n\n\n\n\n\n49.3.4 百分比条形图\n先计算好比例再通过gt_plt_bar_pct()函数画图：\n\nmtcars %&gt;%\n  head() %&gt;%\n  dplyr::select(cyl, mpg) %&gt;%\n  dplyr::mutate(mpg_pct_max = round(mpg/max(mpg) * 100, digits = 2),\n                mpg_scaled = mpg/max(mpg) * 100) %&gt;%\n  dplyr::mutate(mpg_unscaled = mpg) %&gt;%\n  gt() %&gt;%\n  gt_plt_bar_pct(column = mpg_scaled, scaled = TRUE) %&gt;%\n  gt_plt_bar_pct(column = mpg_unscaled, scaled = FALSE, fill = \"blue\", \n                 background = \"lightblue\") %&gt;%\n  cols_align(\"center\", contains(\"scale\")) %&gt;%\n  cols_width(4 ~ px(125),\n             5 ~ px(125))\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n    \n      cyl\n      mpg\n      mpg_pct_max\n      mpg_scaled\n      mpg_unscaled\n    \n  \n  \n    6\n21.0\n92.11\n\n\n    6\n21.0\n92.11\n\n\n    4\n22.8\n100.00\n\n\n    6\n21.4\n93.86\n\n\n    8\n18.7\n82.02\n\n\n    6\n18.1\n79.39\n\n\n  \n  \n  \n\n\n\n\n\n\n49.3.5 堆积条形图\n首先要自己把比例算好，这个百分比需要由多列组成。然后使用gt_plt_bar_stack()函数画出百分比堆积条形图。\n\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n\nex_df &lt;- dplyr::tibble(\n  x = c(\"Example 1\",\"Example 1\",\n        \"Example 1\",\"Example 2\",\"Example 2\",\"Example 2\",\n        \"Example 3\",\"Example 3\",\"Example 3\",\"Example 4\",\"Example 4\",\n        \"Example 4\"),\n  measure = c(\"Measure 1\",\"Measure 2\",\n              \"Measure 3\",\"Measure 1\",\"Measure 2\",\"Measure 3\",\n              \"Measure 1\",\"Measure 2\",\"Measure 3\",\"Measure 1\",\"Measure 2\",\n              \"Measure 3\"),\n  data = c(30, 20, 50, 30, 30, 40, 30, 40, 30, 30, 50, 20)\n)\n\n\ntab_df &lt;- ex_df %&gt;%\n  group_by(x) %&gt;%\n  summarise(list_data = list(data))\n\ntab_df\n## # A tibble: 4 × 2\n##   x         list_data\n##   &lt;chr&gt;     &lt;list&gt;   \n## 1 Example 1 &lt;dbl [3]&gt;\n## 2 Example 2 &lt;dbl [3]&gt;\n## 3 Example 3 &lt;dbl [3]&gt;\n## 4 Example 4 &lt;dbl [3]&gt;\n\ntab_df %&gt;%\n  gt() %&gt;%\n  gt_plt_bar_stack(column = list_data)\n\n\n\n\n\n  \n    \n    \n      x\n      Group 1||Group 2||Group 3\n    \n  \n  \n    Example 1\n          302050\n    Example 2\n          303040\n    Example 3\n          304030\n    Example 4\n          305020\n  \n  \n  \n\n\n\n\n\n\n49.3.6 win/loss plot\n这个图形在体育领域用的比较多，暂时没想到在医学领域有什么用。。。\n\ncreate_input_df &lt;- function(repeats = 3){\n  \n  input_df &lt;- dplyr::tibble(\n    team = c(\"A1\", \"B2\", \"C3\", \"C4\"),\n    Wins = c(3, 2, 1, 1),\n    Losses = c(2, 3, 2, 4),\n    Ties = c(0, 0, 2, 0),\n    outcomes = list(\n      c(1, .5, 0) %&gt;% rep(each = repeats),\n      c(0, 1, 0.5) %&gt;% rep(each = repeats),\n      c(0, 0.5, 1) %&gt;% rep(each = repeats),\n      c(0.5, 1, 0) %&gt;% rep(each = repeats)\n    )\n  )\n  \n  input_df\n  \n}\n\ncreate_input_df(5) %&gt;% \n  dplyr::glimpse()\n## Rows: 4\n## Columns: 5\n## $ team     &lt;chr&gt; \"A1\", \"B2\", \"C3\", \"C4\"\n## $ Wins     &lt;dbl&gt; 3, 2, 1, 1\n## $ Losses   &lt;dbl&gt; 2, 3, 2, 4\n## $ Ties     &lt;dbl&gt; 0, 0, 2, 0\n## $ outcomes &lt;list&gt; &lt;1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.0, 0.0, …\n\n\ncreate_input_df(1) %&gt;% \n  gt() %&gt;% \n  gt_plt_winloss(outcomes, max_wins = 15) %&gt;% \n  tab_options(data_row.padding = px(2))\n\n\n\n\n\n  \n    \n    \n      team\n      Wins\n      Losses\n      Ties\n      outcomes\n    \n  \n  \n    A1\n3\n2\n0\n          \n    B2\n2\n3\n0\n          \n    C3\n1\n2\n2\n          \n    C4\n1\n4\n0\n          \n  \n  \n  \n\n\n\n\n\n\n49.3.7 bullet chart\n可以在条形图的基础上添加均值标记、中位数标记等。\n\nlibrary(dplyr)\n\nmtcars %&gt;% \n  select(mpg:drat) %&gt;%\n  group_by(cyl) %&gt;% \n  mutate(target_col = mean(mpg)) %&gt;% \n  ungroup() %&gt;% \n  gt() %&gt;% \n  gt_plt_bullet(column = mpg, target = target_col,\n                width = 50\n                )\n\n\n\n\n\n  \n    \n    \n      mpg\n      cyl\n      disp\n      hp\n      drat\n    \n  \n  \n              \n6\n160.0\n110\n3.90\n              \n6\n160.0\n110\n3.90\n              \n4\n108.0\n93\n3.85\n              \n6\n258.0\n110\n3.08\n              \n8\n360.0\n175\n3.15\n              \n6\n225.0\n105\n2.76\n              \n8\n360.0\n245\n3.21\n              \n4\n146.7\n62\n3.69\n              \n4\n140.8\n95\n3.92\n              \n6\n167.6\n123\n3.92\n              \n6\n167.6\n123\n3.92\n              \n8\n275.8\n180\n3.07\n              \n8\n275.8\n180\n3.07\n              \n8\n275.8\n180\n3.07\n              \n8\n472.0\n205\n2.93\n              \n8\n460.0\n215\n3.00\n              \n8\n440.0\n230\n3.23\n              \n4\n78.7\n66\n4.08\n              \n4\n75.7\n52\n4.93\n              \n4\n71.1\n65\n4.22\n              \n4\n120.1\n97\n3.70\n              \n8\n318.0\n150\n2.76\n              \n8\n304.0\n150\n3.15\n              \n8\n350.0\n245\n3.73\n              \n8\n400.0\n175\n3.08\n              \n4\n79.0\n66\n4.08\n              \n4\n120.3\n91\n4.43\n              \n4\n95.1\n113\n3.77\n              \n8\n351.0\n264\n4.22\n              \n6\n145.0\n175\n3.62\n              \n8\n301.0\n335\n3.54\n              \n4\n121.0\n109\n4.11"
  },
  {
    "objectID": "gtsummary.html#安装",
    "href": "gtsummary.html#安装",
    "title": "50  gtsummary绘制表格",
    "section": "50.1 安装",
    "text": "50.1 安装\n\n# 2选1\ninstall.packages(\"gtsummary\")\n\nremotes::install_github(\"ddsjoberg/gtsummary\")"
  },
  {
    "objectID": "gtsummary.html#tbl_summary",
    "href": "gtsummary.html#tbl_summary",
    "title": "50  gtsummary绘制表格",
    "section": "50.2 tbl_summary",
    "text": "50.2 tbl_summary\n自动计算描述性统计指标，支持连续型变量、分类变量，生成的表格支持自定义细节。\n可用于绘制我们临床中常见的表1（基线资料表/三线表）！\n\nlibrary(gtsummary)\n## #BlackLivesMatter\nsuppressPackageStartupMessages(library(tidyverse))\n\n使用自带的trial数据集进行演示，这个数据集也是临床中常见的数据类型。包含200个病人的基本信息，比如年龄、性别、治疗方式、肿瘤分级等，分为2组，一组用A药，另一组用B药。\n\n# 查看一下数据结构\nstr(trial)\n## tibble [200 × 8] (S3: tbl_df/tbl/data.frame)\n##  $ trt     : chr [1:200] \"Drug A\" \"Drug B\" \"Drug A\" \"Drug A\" ...\n##   ..- attr(*, \"label\")= chr \"Chemotherapy Treatment\"\n##  $ age     : num [1:200] 23 9 31 NA 51 39 37 32 31 34 ...\n##   ..- attr(*, \"label\")= chr \"Age\"\n##  $ marker  : num [1:200] 0.16 1.107 0.277 2.067 2.767 ...\n##   ..- attr(*, \"label\")= chr \"Marker Level (ng/mL)\"\n##  $ stage   : Factor w/ 4 levels \"T1\",\"T2\",\"T3\",..: 1 2 1 3 4 4 1 1 1 3 ...\n##   ..- attr(*, \"label\")= chr \"T Stage\"\n##  $ grade   : Factor w/ 3 levels \"I\",\"II\",\"III\": 2 1 2 3 3 1 2 1 2 1 ...\n##   ..- attr(*, \"label\")= chr \"Grade\"\n##  $ response: int [1:200] 0 1 0 1 1 0 0 0 0 0 ...\n##   ..- attr(*, \"label\")= chr \"Tumor Response\"\n##  $ death   : int [1:200] 0 0 0 1 1 1 0 1 0 1 ...\n##   ..- attr(*, \"label\")= chr \"Patient Died\"\n##  $ ttdeath : num [1:200] 24 24 24 17.6 16.4 ...\n##   ..- attr(*, \"label\")= chr \"Months to Death/Censor\"\n\n\n50.2.1 基本使用\n\n数据类型自动检测（连续型变量或者分类变量）\n如果列有属性值（label attributes），自动添加\n自动添加脚注\n\n\n# 选取部分数据，方便演示\ntrial2 &lt;- trial %&gt;% select(trt,age,grade)\n\ntrial2 %&gt;% tbl_summary()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 2001\n    \n  \n  \n    Chemotherapy Treatment\n\n        Drug A\n98 (49%)\n        Drug B\n102 (51%)\n    Age\n47 (38, 57)\n        Unknown\n11\n    Grade\n\n        I\n68 (34%)\n        II\n68 (34%)\n        III\n64 (32%)\n  \n  \n  \n    \n      1 n (%); Median (IQR)\n    \n  \n\n\n\n\n当然是支持分组比较的，添加P值不在话下！\n\ntrial2 %&gt;% tbl_summary(by = trt) %&gt;% add_p()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    \n  \n  \n    Age\n46 (37, 59)\n48 (39, 56)\n0.7\n        Unknown\n7\n4\n\n    Grade\n\n\n0.9\n        I\n35 (36%)\n33 (32%)\n\n        II\n32 (33%)\n36 (35%)\n\n        III\n31 (32%)\n33 (32%)\n\n  \n  \n  \n    \n      1 Median (IQR); n (%)\n    \n    \n      2 Wilcoxon rank sum test; Pearson’s Chi-squared test\n    \n  \n\n\n\n\n\n\n50.2.2 自定义输出\n超多自定义选项：\n\n自定义输出表格外观：\n\ntrial2 %&gt;%\n  tbl_summary(\n    by = trt, # 分组\n    \n    # 根据变量类型选择显示方式，和case_when()的使用非常像哦\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\",\n                     all_categorical() ~ \"{n} / {N} ({p}%)\"),\n    \n    # 控制小数点\n    digits = all_continuous() ~ 2,\n    \n    # 列名\n    label = grade ~ \"Tumor Grade\",\n    \n    # 缺失值\n    missing_text = \"(Missing)\"\n  ) %&gt;% \n  add_p()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    \n  \n  \n    Age\n47.01 (14.71)\n47.45 (14.01)\n0.7\n        (Missing)\n7\n4\n\n    Tumor Grade\n\n\n0.9\n        I\n35 / 98 (36%)\n33 / 102 (32%)\n\n        II\n32 / 98 (33%)\n36 / 102 (35%)\n\n        III\n31 / 98 (32%)\n33 / 102 (32%)\n\n  \n  \n  \n    \n      1 Mean (SD); n / N (%)\n    \n    \n      2 Wilcoxon rank sum test; Pearson’s Chi-squared test\n    \n  \n\n\n\n\n根据变量类型选择显示方式，有多种实现方法，下面列出了支持的3种方式：\n\n\n\n\n\n\n\n\n方法1\n方法2\n方法3\n\n\n\n\nall_continuous() ~ “{mean}”\nc(“age”, “marker”) ~ “{mean}”\nlist(age = “{mean}”, marker = “{mean}”)\n\n\nlist(all_continuous() ~ “{mean}”)\nc(age, marker) ~ “{mean}”\n-\n\n\n-\nlist(c(age, marker) ~ “{mean}”)\n-\n\n\n\n官方贴心的给出了使用方法：\n\n修改变量显示的名称也可以用同样的方法。\n\n\n50.2.3 修改统计方法\n可以为不同的列自定义不同的统计方法。\n\ntrial2 %&gt;%\n  tbl_summary(\n    by = trt, # 分组\n    \n    # 根据变量类型选择显示方式，和case_when()的使用非常像哦\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\",\n                     all_categorical() ~ \"{n} / {N} ({p}%)\"),\n    \n    # 控制小数点\n    digits = all_continuous() ~ 2,\n    \n    # 列名\n    label = grade ~ \"Tumor Grade\",\n    \n    # 缺失值\n    missing_text = \"(Missing)\"\n  ) %&gt;% \n  add_p(test = list(age ~ \"t.test\", # 为不同的列选择不同的统计方法\n                    grade ~ \"kruskal.test\"\n                    ),\n        pvalue_fun = ~style_pvalue(.x, digits = 2)\n        )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    \n  \n  \n    Age\n47.01 (14.71)\n47.45 (14.01)\n0.83\n        (Missing)\n7\n4\n\n    Tumor Grade\n\n\n0.72\n        I\n35 / 98 (36%)\n33 / 102 (32%)\n\n        II\n32 / 98 (33%)\n36 / 102 (35%)\n\n        III\n31 / 98 (32%)\n33 / 102 (32%)\n\n  \n  \n  \n    \n      1 Mean (SD); n / N (%)\n    \n    \n      2 Welch Two Sample t-test; Kruskal-Wallis rank sum test\n    \n  \n\n\n\n\n除了添加P值外，还可以添加超多东西：\n\n修改表格细节的选项：\n\n一个简单的小例子：\n\ntrial2 %&gt;%\n  tbl_summary(by = trt) %&gt;%\n  add_p(pvalue_fun = ~style_pvalue(.x, digits = 2)) %&gt;%\n  add_overall() %&gt;%\n  add_n() %&gt;%\n  modify_header(label ~ \"**Variable**\") %&gt;%\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Treatment Received**\") %&gt;%\n  modify_footnote(\n    all_stat_cols() ~ \"Median (IQR) or Frequency (%)\"\n  ) %&gt;%\n  modify_caption(\"**Table 1. Patient Characteristics**\") %&gt;%\n  bold_labels()\n\n\n\n\n\n  Table 1. Patient Characteristics\n  \n    \n    \n      Variable\n      N\n      Overall, N = 2001\n      \n        Treatment Received\n      \n      p-value2\n    \n    \n      Drug A, N = 981\n      Drug B, N = 1021\n    \n  \n  \n    Age\n189\n47 (38, 57)\n46 (37, 59)\n48 (39, 56)\n0.72\n        Unknown\n\n11\n7\n4\n\n    Grade\n200\n\n\n\n0.87\n        I\n\n68 (34%)\n35 (36%)\n33 (32%)\n\n        II\n\n68 (34%)\n32 (33%)\n36 (35%)\n\n        III\n\n64 (32%)\n31 (32%)\n33 (32%)\n\n  \n  \n  \n    \n      1 Median (IQR) or Frequency (%)\n    \n    \n      2 Wilcoxon rank sum test; Pearson’s Chi-squared test\n    \n  \n\n\n\n\n还可以和gt包连用。使用as_gt()函数转换为gt对象后们就可以使用gt包的函数了。\n\ntrial2 %&gt;%\n  tbl_summary(by = trt, missing = \"no\") %&gt;%\n  add_n() %&gt;%\n  as_gt() %&gt;% # 转换为gt对象\n  gt::tab_source_note(gt::md(\"*This data is simulated*\"))\n\n\n\n\n\nCharacteristic\nN\nDrug A, N = 981\nDrug B, N = 1021\nAge\n189\n46 (37, 59)\n48 (39, 56)\nGrade\n200\n\n\n\n\n    I\n\n\n35 (36%)\n33 (32%)\n    II\n\n\n32 (33%)\n36 (35%)\n    III\n\n\n31 (32%)\n33 (32%)\nThis data is simulated\n1 Median (IQR); n (%)\n\n\n\n\n\n\n50.2.4 同一个变量展示多个统计量\n对于连续型变量，可以在多行显示多个统计值，只要设置type = all_continuous() ~ \"continuous2\"即可。\n\ntrial2 %&gt;%\n  select(age, trt) %&gt;%\n  tbl_summary(\n    by = trt,\n    type = all_continuous() ~ \"continuous2\",\n    statistic = all_continuous() ~ c(\"{N_nonmiss}\",\n                                     \"{median} ({p25}, {p75})\", \n                                     \"{min}, {max}\"),\n    missing = \"no\"\n  ) %&gt;%\n  add_p(pvalue_fun = ~style_pvalue(.x, digits = 2)) #修改P值小数点\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Drug A, N = 98\n      Drug B, N = 102\n      p-value1\n    \n  \n  \n    Age\n\n\n0.72\n        N\n91\n98\n\n        Median (IQR)\n46 (37, 59)\n48 (39, 56)\n\n        Range\n6, 78\n9, 83\n\n  \n  \n  \n    \n      1 Wilcoxon rank sum test\n    \n  \n\n\n\n\n\n\n50.2.5 交叉表\n可以非常方便的绘制交叉表，临床上我们喜欢叫列联表~\n\ntrial %&gt;%\n  tbl_cross(\n    row = stage, # 指定行\n    col = trt,   # 指定列\n    percent = \"cell\"\n  ) %&gt;%\n  add_p()\n\n\n\n\n\n  \n    \n    \n      \n      \n        Chemotherapy Treatment\n      \n      Total\n      p-value1\n    \n    \n      Drug A\n      Drug B\n    \n  \n  \n    T Stage\n\n\n\n0.9\n        T1\n28 (14%)\n25 (13%)\n53 (27%)\n\n        T2\n25 (13%)\n29 (15%)\n54 (27%)\n\n        T3\n22 (11%)\n21 (11%)\n43 (22%)\n\n        T4\n23 (12%)\n27 (14%)\n50 (25%)\n\n    Total\n98 (49%)\n102 (51%)\n200 (100%)\n\n  \n  \n  \n    \n      1 Pearson’s Chi-squared test"
  },
  {
    "objectID": "gtsummary.html#和comparegroups包进行比较",
    "href": "gtsummary.html#和comparegroups包进行比较",
    "title": "50  gtsummary绘制表格",
    "section": "50.3 和compareGroups包进行比较",
    "text": "50.3 和compareGroups包进行比较\n这么多画表格的包，这个很强，比gt强不少！但我还是喜欢用compareGroups包，因为简单，一句代码即可搞定，所以，必须比较下，哪个好用，尤其是在画基线资料表方面！\n\nlibrary(compareGroups)\ndata(\"regicor\")\n\nglimpse(regicor)\n## Rows: 2,294\n## Columns: 25\n## $ id       &lt;dbl&gt; 2265, 1882, 3000105616, 3000103485, 3000103963, 3000100883, 3…\n## $ year     &lt;fct&gt; 2005, 2005, 2000, 2000, 2000, 2000, 2000, 1995, 2005, 1995, 2…\n## $ age      &lt;int&gt; 70, 56, 37, 69, 70, 40, 66, 53, 43, 70, 54, 42, 54, 48, 68, 4…\n## $ sex      &lt;fct&gt; Female, Female, Male, Female, Female, Female, Male, Female, F…\n## $ smoker   &lt;fct&gt; Never smoker, Never smoker, Current or former &lt; 1y, Never smo…\n## $ sbp      &lt;int&gt; 138, 139, 132, 168, NA, 108, 120, 132, 95, 142, 130, 99, 117,…\n## $ dbp      &lt;int&gt; 75, 89, 82, 97, NA, 70, 72, 78, 65, 78, 66, 60, 70, 73, 71, 8…\n## $ histhtn  &lt;fct&gt; No, No, No, No, No, No, Yes, No, No, No, No, Yes, No, No, Yes…\n## $ txhtn    &lt;fct&gt; No, No, No, No, No, No, Yes, No, No, No, No, No, No, No, Yes,…\n## $ chol     &lt;dbl&gt; 294, 220, 245, 168, NA, NA, 298, 254, 194, 188, 268, 116, 211…\n## $ hdl      &lt;dbl&gt; 57.00000, 50.00000, 59.80429, 53.17571, NA, 68.90000, 78.8900…\n## $ triglyc  &lt;dbl&gt; 93, 160, 89, 116, NA, 94, 71, NA, 68, 137, 128, 39, 144, 45, …\n## $ ldl      &lt;dbl&gt; 218.40000, 138.00000, 167.39571, 91.62429, NA, NA, 204.91000,…\n## $ histchol &lt;fct&gt; No, No, No, No, NA, No, Yes, No, No, No, No, No, Yes, No, No,…\n## $ txchol   &lt;fct&gt; No, No, No, No, NA, No, No, No, No, No, No, No, No, No, No, N…\n## $ height   &lt;dbl&gt; 160.0, 163.0, 170.0, 147.0, NA, 158.0, 164.0, 154.5, 160.0, 1…\n## $ weight   &lt;dbl&gt; 64.0, 67.0, 70.0, 68.0, NA, 43.5, 79.2, 45.8, 53.0, 62.0, 79.…\n## $ bmi      &lt;dbl&gt; 25.00000, 25.21736, 24.22145, 31.46837, NA, 17.42509, 29.4467…\n## $ phyact   &lt;dbl&gt; 304.20000, 160.30000, 552.79121, 522.00000, NA, 386.95055, 47…\n## $ pcs      &lt;dbl&gt; 54.45500, 58.16500, 43.42900, 54.32500, NA, 57.31500, 59.6020…\n## $ mcs      &lt;dbl&gt; 58.91800, 47.99500, 62.58500, 57.90000, NA, 47.86900, 40.8420…\n## $ cv       &lt;fct&gt; No, No, No, No, NA, No, No, No, No, No, No, No, No, Yes, No, …\n## $ tocv     &lt;dbl&gt; 1024.88243, 2756.84851, 1905.96879, 1055.37962, NA, 3239.2407…\n## $ death    &lt;fct&gt; Yes, No, No, No, NA, No, Yes, No, No, No, No, No, No, No, No,…\n## $ todeath  &lt;dbl&gt; 1299.16343, 39.32629, 858.42203, 1833.07619, NA, 877.61155, 2…\n\n首先是compareGroups:\n\n# 一样代码，太简单！不会写复杂代码的可以直接输出到Word里面修改\nrestab &lt;- descrTable(year ~ ., data = regicor[,-1]) \nrestab\n## \n## --------Summary descriptives table by 'Recruitment year'---------\n## \n## ______________________________________________________________________________________________ \n##                                                     1995        2000        2005     p.overall \n##                                                     N=431       N=786      N=1077              \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ \n## Age                                              54.1 (11.7) 54.3 (11.2) 55.3 (10.6)   0.078   \n## Sex:                                                                                   0.506   \n##     Male                                         206 (47.8%) 390 (49.6%) 505 (46.9%)           \n##     Female                                       225 (52.2%) 396 (50.4%) 572 (53.1%)           \n## Smoking status:                                                                       &lt;0.001   \n##     Never smoker                                 234 (56.4%) 414 (54.6%) 553 (52.2%)           \n##     Current or former &lt; 1y                       109 (26.3%) 267 (35.2%) 217 (20.5%)           \n##     Former &gt;= 1y                                 72 (17.3%)  77 (10.2%)  290 (27.4%)           \n## Systolic blood pressure                          133 (19.2)  133 (21.3)  129 (19.8)   &lt;0.001   \n## Diastolic blood pressure                         77.0 (10.5) 80.8 (10.3) 79.9 (10.6)  &lt;0.001   \n## History of hypertension:                                                              &lt;0.001   \n##     Yes                                          111 (25.8%) 233 (29.6%) 379 (35.5%)           \n##     No                                           320 (74.2%) 553 (70.4%) 690 (64.5%)           \n## Hypertension treatment:                                                                0.002   \n##     No                                           360 (83.5%) 659 (83.8%) 804 (77.8%)           \n##     Yes                                          71 (16.5%)  127 (16.2%) 230 (22.2%)           \n## Total cholesterol                                225 (43.1)  224 (44.4)  213 (45.9)   &lt;0.001   \n## HDL cholesterol                                  51.9 (14.5) 52.3 (15.6) 53.2 (14.2)   0.208   \n## Triglycerides                                    114 (74.4)  114 (70.7)  117 (76.0)    0.582   \n## LDL cholesterol                                  152 (38.4)  149 (38.6)  136 (39.7)   &lt;0.001   \n## History of hyperchol.:                                                                &lt;0.001   \n##     Yes                                          97 (22.5%)  256 (33.2%) 356 (33.2%)           \n##     No                                           334 (77.5%) 515 (66.8%) 715 (66.8%)           \n## Cholesterol treatment:                                                                &lt;0.001   \n##     No                                           403 (93.5%) 705 (91.2%) 903 (87.2%)           \n##     Yes                                          28 (6.50%)  68 (8.80%)  132 (12.8%)           \n## Height (cm)                                      163 (9.21)  162 (9.39)  163 (9.05)    0.003   \n## Weight (Kg)                                      72.3 (12.6) 73.8 (14.0) 73.6 (13.9)   0.150   \n## Body mass index                                  27.0 (4.15) 28.1 (4.62) 27.6 (4.63)  &lt;0.001   \n## Physical activity (Kcal/week)                     491 (419)   422 (377)   351 (378)   &lt;0.001   \n## Physical component                               49.3 (8.08) 49.0 (9.63) 50.1 (8.91)   0.032   \n## Mental component                                 49.2 (11.3) 48.9 (11.0) 46.9 (10.8)  &lt;0.001   \n## Cardiovascular event:                                                                  0.161   \n##     No                                           388 (97.5%) 706 (95.3%) 977 (95.4%)           \n##     Yes                                          10 (2.51%)  35 (4.72%)  47 (4.59%)            \n## Days to cardiovascular event or end of follow-up 1784 (1101) 1686 (1080) 1793 (1072)   0.099   \n## Overall death:                                                                        &lt;0.001   \n##     No                                           369 (95.3%) 657 (89.0%) 949 (92.8%)           \n##     Yes                                          18 (4.65%)  81 (11.0%)  74 (7.23%)            \n## Days to overall death or end of follow-up        1713 (1042) 1674 (1050) 1758 (1055)   0.252   \n## ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n\n然后是gtSummary:\n\nregicor[,-1] %&gt;% \n  tbl_summary(by = year,\n              type = all_dichotomous() ~ \"categorical\"\n              ) %&gt;% \n  add_overall() %&gt;% \n  \n  # 可以自定义每一列的统计方法，这里就不演示了\n  add_p(pvalue_fun = ~style_pvalue(.x, digits = 3)) %&gt;% \n  \n  # 添加spanner\n  modify_spanning_header(c(\"stat_1\",\"stat_2\",\"stat_3\") ~ \"**我随便起名**\")\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Overall, N = 2,2941\n      \n        我随便起名\n      \n      p-value2\n    \n    \n      1995, N = 4311\n      2000, N = 7861\n      2005, N = 1,0771\n    \n  \n  \n    Age\n55 (46, 64)\n54 (44, 64)\n54 (45, 64)\n56 (47, 64)\n0.082\n    Sex\n\n\n\n\n0.506\n        Male\n1,101 (48%)\n206 (48%)\n390 (50%)\n505 (47%)\n\n        Female\n1,193 (52%)\n225 (52%)\n396 (50%)\n572 (53%)\n\n    Smoking status\n\n\n\n\n&lt;0.001\n        Never smoker\n1,201 (54%)\n234 (56%)\n414 (55%)\n553 (52%)\n\n        Current or former &lt; 1y\n593 (27%)\n109 (26%)\n267 (35%)\n217 (20%)\n\n        Former &gt;= 1y\n439 (20%)\n72 (17%)\n77 (10%)\n290 (27%)\n\n        Unknown\n61\n16\n28\n17\n\n    Systolic blood pressure\n129 (116, 144)\n132 (118, 145)\n132 (118, 148)\n125 (115, 142)\n&lt;0.001\n        Unknown\n14\n3\n11\n0\n\n    Diastolic blood pressure\n80 (72, 86)\n78 (72, 84)\n80 (74, 88)\n79 (73, 87)\n&lt;0.001\n        Unknown\n14\n3\n11\n0\n\n    History of hypertension\n\n\n\n\n&lt;0.001\n        Yes\n723 (32%)\n111 (26%)\n233 (30%)\n379 (35%)\n\n        No\n1,563 (68%)\n320 (74%)\n553 (70%)\n690 (65%)\n\n        Unknown\n8\n0\n0\n8\n\n    Hypertension treatment\n\n\n\n\n0.002\n        No\n1,823 (81%)\n360 (84%)\n659 (84%)\n804 (78%)\n\n        Yes\n428 (19%)\n71 (16%)\n127 (16%)\n230 (22%)\n\n        Unknown\n43\n0\n0\n43\n\n    Total cholesterol\n215 (189, 245)\n225 (196, 254)\n222 (193, 250)\n209 (184, 238)\n&lt;0.001\n        Unknown\n101\n28\n71\n2\n\n    HDL cholesterol\n51 (42, 61)\n51 (41, 61)\n50 (41, 62)\n52 (43, 61)\n0.077\n        Unknown\n69\n30\n38\n1\n\n    Triglycerides\n97 (72, 136)\n94 (71, 136)\n98 (72, 133)\n98 (72, 139)\n0.762\n        Unknown\n63\n28\n34\n1\n\n    LDL cholesterol\n141 (116, 168)\n151 (127, 175)\n147 (120, 174)\n133 (110, 159)\n&lt;0.001\n        Unknown\n168\n43\n98\n27\n\n    History of hyperchol.\n\n\n\n\n&lt;0.001\n        Yes\n709 (31%)\n97 (23%)\n256 (33%)\n356 (33%)\n\n        No\n1,564 (69%)\n334 (77%)\n515 (67%)\n715 (67%)\n\n        Unknown\n21\n0\n15\n6\n\n    Cholesterol treatment\n\n\n\n\n&lt;0.001\n        No\n2,011 (90%)\n403 (94%)\n705 (91%)\n903 (87%)\n\n        Yes\n228 (10%)\n28 (6.5%)\n68 (8.8%)\n132 (13%)\n\n        Unknown\n55\n0\n13\n42\n\n    Height (cm)\n163 (156, 169)\n163 (157, 170)\n162 (155, 169)\n163 (156, 170)\n0.010\n        Unknown\n35\n8\n15\n12\n\n    Weight (Kg)\n73 (64, 82)\n72 (64, 80)\n73 (64, 83)\n73 (63, 83)\n0.300\n        Unknown\n35\n8\n15\n12\n\n    Body mass index\n27.2 (24.4, 30.4)\n26.5 (24.2, 29.5)\n27.6 (24.7, 30.7)\n27.2 (24.3, 30.4)\n&lt;0.001\n        Unknown\n35\n8\n15\n12\n\n    Physical activity (Kcal/week)\n304 (159, 522)\n390 (226, 617)\n347 (185, 574)\n262 (127, 443)\n&lt;0.001\n        Unknown\n88\n64\n22\n2\n\n    Physical component\n52 (45, 56)\n52 (45, 55)\n52 (44, 56)\n53 (45, 56)\n0.001\n        Unknown\n240\n34\n123\n83\n\n    Mental component\n51 (42, 56)\n52 (45, 57)\n53 (43, 57)\n50 (41, 55)\n&lt;0.001\n        Unknown\n240\n34\n123\n83\n\n    Cardiovascular event\n\n\n\n\n0.161\n        No\n2,071 (96%)\n388 (97%)\n706 (95%)\n977 (95%)\n\n        Yes\n92 (4.3%)\n10 (2.5%)\n35 (4.7%)\n47 (4.6%)\n\n        Unknown\n131\n33\n45\n53\n\n    Days to cardiovascular event or end of follow-up\n1,718 (783, 2,691)\n1,728 (746, 2,767)\n1,617 (723, 2,596)\n1,775 (835, 2,723)\n0.096\n        Unknown\n131\n33\n45\n53\n\n    Overall death\n\n\n\n\n&lt;0.001\n        No\n1,975 (92%)\n369 (95%)\n657 (89%)\n949 (93%)\n\n        Yes\n173 (8.1%)\n18 (4.7%)\n81 (11%)\n74 (7.2%)\n\n        Unknown\n146\n44\n48\n54\n\n    Days to overall death or end of follow-up\n1,668 (788, 2,663)\n1,557 (812, 2,689)\n1,609 (734, 2,549)\n1,734 (817, 2,713)\n0.249\n        Unknown\n146\n44\n48\n54\n\n  \n  \n  \n    \n      1 Median (IQR); n (%)\n    \n    \n      2 Kruskal-Wallis rank sum test; Pearson’s Chi-squared test\n    \n  \n\n\n\n\n貌似结果比compareGroups更好看…有些结果不太一样，因为默认方法不太一样。\n一个是一行代码出表，另一个只需要多加几行代码就可以绘制发表级别的表，选哪个呢？"
  },
  {
    "objectID": "9999-appendix.html#r语言rtoolsrstudio的安装",
    "href": "9999-appendix.html#r语言rtoolsrstudio的安装",
    "title": "附录 A — 其他合集",
    "section": "A.1 R语言、Rtools、Rstudio的安装",
    "text": "A.1 R语言、Rtools、Rstudio的安装\n\n公众号推文：可能是最适合小白的R语言和R包安装教程\nb站播放量超7w的视频教程：适合小白的R语言和Rstudio安装教程"
  },
  {
    "objectID": "9999-appendix.html#r包安装",
    "href": "9999-appendix.html#r包安装",
    "title": "附录 A — 其他合集",
    "section": "A.2 R包安装",
    "text": "A.2 R包安装\n\n公众号推文：可能是最好的R包安装教程\nb站播放量超7w的视频教程：可能是最好用的R包安装教程"
  },
  {
    "objectID": "9999-appendix.html#pass软件链接",
    "href": "9999-appendix.html#pass软件链接",
    "title": "附录 A — 其他合集",
    "section": "A.3 PASS软件链接",
    "text": "A.3 PASS软件链接\n关注公众号：医学和生信笔记，后台回复PASS即可获得软件链接。"
  },
  {
    "objectID": "9999-appendix.html#临床预测模型",
    "href": "9999-appendix.html#临床预测模型",
    "title": "附录 A — 其他合集",
    "section": "A.4 临床预测模型",
    "text": "A.4 临床预测模型\n临床预测模型合集：临床预测模型"
  },
  {
    "objectID": "9999-appendix.html#机器学习",
    "href": "9999-appendix.html#机器学习",
    "title": "附录 A — 其他合集",
    "section": "A.5 机器学习",
    "text": "A.5 机器学习\n医学和生信笔记后台回复caret即可获取caret包的合集教程；回复tidymodels即可获取tidymodels的合集教程；回复mlr3即可获取mlr3合集教程，回复机器学习即可获取机器学习推文合集。\nR语言机器学习合集：R语言机器学习\n在线版电子书敬请期待，即将上线！\n本号很少涉及理论知识，主要还是R语言实战，所以理论部分大家需要自己多多学习。"
  },
  {
    "objectID": "9999-appendix.html#生信数据挖掘",
    "href": "9999-appendix.html#生信数据挖掘",
    "title": "附录 A — 其他合集",
    "section": "A.6 生信数据挖掘",
    "text": "A.6 生信数据挖掘\n生信数据挖掘合集：生信数据挖掘\n医学和生信笔记公众号所有关于生信数据挖掘的推文都可以免费下载使用，请看：“灌水”生信类文章会用到哪些生信下游分析？（附下载地址）\ngithub地址：R语言生信数据挖掘"
  },
  {
    "objectID": "9999-appendix.html#扫码关注",
    "href": "9999-appendix.html#扫码关注",
    "title": "附录 A — 其他合集",
    "section": "A.7 扫码关注",
    "text": "A.7 扫码关注\n欢迎扫码关注：医学和生信笔记"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R语言实战医学统计",
    "section": "",
    "text": "前言"
  },
  {
    "objectID": "index.html#本书缘起",
    "href": "index.html#本书缘起",
    "title": "R语言实战医学统计",
    "section": "本书缘起",
    "text": "本书缘起\nR语言是一门编程语言，但同时也是一个统计软件，R语言是由统计学家开发的，所以天生就适合做统计。\n很多刚接触R语言的朋友不知道如何入手，只知道目前R语言在临床医学领域很火爆，做统计分析、画图、做生信分析、孟德尔随机化、数据库挖掘等都离不开R语言。\n万事开头难，我非常理解新手面对R语言的痛苦，因为我也是从0开始的，作为从未接触过编程的医学生/医生来说，初学R语言简直就是读天书！我最开始接触R语言是因为偶然间听师兄师姐说R语言可以做统计学，当时的我对SPSS的使用不熟练，觉得SPSS的使用步骤太多，难以记住，于是入了R语言的坑…没想到从此一发不可收拾，打开了新世界的大门。\n这个系列也是我最开始学习R语言时的笔记，在我的公众号：医学和生信笔记，都可以找到，现在对原内容进行重新整理，并把数据一起打包，方便有需要的同学学习。\n\n\n\n\n\n\n提醒\n\n\n\n本书不适合R语言零基础的人。\n如果你是刚入门的小白，我首先推荐你了解下R语言的基础知识，比如R语言和R包安装（初学者可参考附录）、Rstudio的界面、R语言中数据类型（向量、矩阵、数据框、列表等）、R语言中的数据导入导出、R语言的基础数据操作等。\n对于医学生/医生，我比较推荐先看《R语言实战》，再看《R数据科学》，无需全部了解，只需要熟悉其中的基础知识即可。\n\n\n然后你就可以跟着本系列一起学习R语言在医学统计学中的使用。这个系列非常适合初学者，因为是按照课本来的，使用R语言复现课本中的例题，得到结果后可以与课本对照！我使用的课本是孙振球主编的《医学统计学》第4版（第5版和第4版内容变化不大），封面如下：\n\n\n\n\n\n由于R和SPSS在进行统计分析时的一些数学计算方面并不是完全一致，所以导致有些结果和课本中的结果有些出入，但是并不影响结果的正确性。\n\n\n\n\n\n\n注意\n\n\n\n本书实际上是我公众号历史推文的整理和汇总（部分内容有改动），书中涉及的所有数据都可以在相关历史推文中免费获取！历史推文合集链接：医学统计学\n我也准备了一个PDF版合集，内容和网页版一致，只是打包了所有的数据，付费获取（10元），介意勿扰！PDF版合集获取链接：R语言实战医学统计\n\n\n限于本人水平等问题，难免会有一些错误，欢迎大家以各种方式批评指正，比如公众号留言、粉丝QQ群、github、个人微信等。\n本书会不定期更新，内容和格式都会不断完善。\n更新日志：\n\n20231230：全部内容从Rmarkdown改为quarto；增加三线表内容\n20230905：\n\n格式升级，改为3列式；\nRCS增加推荐阅读；\nt检验增加正态性检验和方差齐性检验；\ntidy风格医学统计增加秩和检验和计数资料统计分析；\n增加亚组分析和森林图绘制；\n\n20230612：改正样条回归中的一个笔误（age的Nonlinear的P&lt;0.05……）\n20230407：首次上传"
  },
  {
    "objectID": "index.html#作者简介",
    "href": "index.html#作者简介",
    "title": "R语言实战医学统计",
    "section": "作者简介",
    "text": "作者简介\n\n阿越，外科医生，R语言爱好者，长期分享R语言和医学统计学、临床预测模型、生信数据挖掘、R语言机器学习等知识。\n公众号：医学和生信笔记\n知乎：医学和生信笔记\nCSDN：医学和生信笔记\n哔哩哔哩：阿越就是我\nGithub：ayueme"
  },
  {
    "objectID": "1016-partialcorrelation.html#偏相关partial-correlation",
    "href": "1016-partialcorrelation.html#偏相关partial-correlation",
    "title": "16  偏相关和典型相关",
    "section": "16.1 偏相关（partial correlation）",
    "text": "16.1 偏相关（partial correlation）\n使用R包ppcor实现。\n首先是加载数据和R包。\n\nlibrary(ppcor)\n## Loading required package: MASS\n\ndf &lt;- haven::read_sav(\"datasets/data01.sav\")\ndf1 &lt;- df[,2:4]\nnames(df1) &lt;- c(\"height\",\"weight\",\"vc\")\nhead(df1)\n## # A tibble: 6 × 3\n##   height weight    vc\n##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1   139.   30.4  2   \n## 2   164.   46.2  2.75\n## 3   156.   37.1  2.75\n## 4   156.   35.5  2   \n## 5   150.   31    1.5 \n## 6   145    33    2.5\n\n这个数据有3列，现在我们要探索身高（height）和体重（weight）的关系，其中vc是需要控制的因素。\n首先进行pearson偏相关分析：\n\np1 &lt;- pcor(df1,method = \"pearson\")\np1\n## $estimate\n##            height    weight         vc\n## height  1.0000000 0.7941292 -0.2022408\n## weight  0.7941292 1.0000000  0.6166786\n## vc     -0.2022408 0.6166786  1.0000000\n## \n## $p.value\n##              height       weight          vc\n## height 0.0000000000 0.0000491115 0.406351395\n## weight 0.0000491115 0.0000000000 0.004920346\n## vc     0.4063513954 0.0049203462 0.000000000\n## \n## $statistic\n##            height   weight         vc\n## height  0.0000000 5.387551 -0.8514549\n## weight  5.3875507 0.000000  3.2299064\n## vc     -0.8514549 3.229906  0.0000000\n## \n## $n\n## [1] 20\n## \n## $gp\n## [1] 1\n## \n## $method\n## [1] \"pearson\"\n\n结果中$estimate给出了偏相关系数，可以看到在控制了vc后，height和weight的偏相关系数是0.7941292；$p.value给出了相应的P值，$statistic给出了检验统计量。\n上面演示的是pearson偏相关分析，下面展示一个spearman偏相关分析。\n\n# 加载数据\ndf2 &lt;- haven::read_sav(\"datasets/data02.sav\")\nnames(df2) &lt;- c(\"id\",\"x\",\"y\",\"z\")\n\nhead(df2)\n## # A tibble: 6 × 4\n##      id x         y             z\n##   &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt;\n## 1     7 1 [矮]    1 [轻]     1.25\n## 2    17 1 [矮]    1 [轻]     1.25\n## 3     1 1 [矮]    1 [轻]     2   \n## 4    11 1 [矮]    1 [轻]     2   \n## 5     5 2 [中]    1 [轻]     1.5 \n## 6    15 2 [中]    1 [轻]     1.5\n\n现在我们要计算x和y的相关性，z是要控制的因素，由于这两个变量是分类变量，所以要用spearman偏相关分析。\n其实用法是一样的，就是改个参数而已：\n\npcor(df2[,-1],method = \"spearman\")\n## $estimate\n##            x         y          z\n## x  1.0000000 0.6985577 -0.4212568\n## y  0.6985577 1.0000000  0.8486095\n## z -0.4212568 0.8486095  1.0000000\n## \n## $p.value\n##              x            y            z\n## x 0.0000000000 8.779998e-04 7.245901e-02\n## y 0.0008779998 0.000000e+00 4.386687e-06\n## z 0.0724590110 4.386687e-06 0.000000e+00\n## \n## $statistic\n##           x        y         z\n## x  0.000000 4.025172 -1.915103\n## y  4.025172 0.000000  6.613943\n## z -1.915103 6.613943  0.000000\n## \n## $n\n## [1] 20\n## \n## $gp\n## [1] 1\n## \n## $method\n## [1] \"spearman\"\n\n结果解读同上。\n\n16.1.1 偏相关散点图\n还是用df1的数据作为演示，现在是研究weight对height的影响，vc是需要控制的变量。\n所以我们可以分别计算残差，用残差的散点图代表偏相关的散点图。\n\n# 首先计算height为因变量，vc是自变量的残差\nresidX &lt;- resid(lm(height~vc,data = df1))\n\n# 再计算weight为因变量，vc是自变量的残差\nresidY &lt;- resid(lm(weight~vc, data = df1))\n\n# 两个残差的相关系数就是weight和height的偏相关系数！\ncor(residX, residY, method = \"pearson\")\n## [1] 0.7941292\n\n# 画图即可\nplot(residX, residY)\n\n\n\n\n但是这个图的横纵坐标取值范围对实际来说是不能解释的，所以我们可以分别加上它们各自的平均值，然后再画散点图，方法借鉴了这篇文章：\n\nresidX1 &lt;- residX + mean(df1$height)\nresidY1 &lt;- residY + mean(df1$weight)\n\nplot(residX1, residY1,xlab = \"身高\",ylab = \"体重\")\n\n\n\n\n这个就是偏相关散点图了！"
  },
  {
    "objectID": "1016-partialcorrelation.html#典型相关canonical-correlation",
    "href": "1016-partialcorrelation.html#典型相关canonical-correlation",
    "title": "16  偏相关和典型相关",
    "section": "16.2 典型相关（Canonical Correlation）",
    "text": "16.2 典型相关（Canonical Correlation）\n这个数据来自孙振球《医学统计学》第四版的例23-1，探讨小学生的生长发育指标（肺活量、身高、体重、胸围）和身体素质（短跑、跳高、跳远、实心球）的相互关系。\n\ndf &lt;- read.csv(\"datasets/例23-1.csv\",header = T)\npsych::headtail(df)\n## Warning: headtail is deprecated.  Please use the headTail function\n##     肺活量  身高 体重 胸围 短跑 跳高 跳远 实心球\n## 1     1210 120.1 23.8   61 10.2 66.3 2.01   2.73\n## 2     1210 120.7 23.4 59.8 11.3 67.6 1.92   2.71\n## 3     1040 121.2 22.9   59 10.1 66.5 1.92    2.6\n## 4     1620 121.5 24.6 59.5  9.5 67.8 1.95   2.64\n## ...    ...   ...  ...  ...  ...  ...  ...    ...\n## 81    1310 129.7 24.7 61.7 10.1 69.4 2.03    2.8\n## 82    2280 143.6 37.6   70  9.7 88.8 2.17   4.18\n## 83    1580 136.6 32.3 67.2 10.3 87.1 2.66   4.04\n## 84    2370 147.4 38.8   73 10.8 90.7 2.82   4.38\n\n典型相关分析R语言自带了cancor()函数，无需借助第三方R包：\n\n# 前4个变量和后4个变量做相关性，直接提供2个数据框也可以\ncc1 &lt;- cancor(df[,1:4],df[,5:8])\n\ncc1\n## $cor\n## [1] 0.8858445 0.2791523 0.1940486 0.0379654\n## \n## $xcoef\n##                 [,1]          [,2]         [,3]          [,4]\n## 肺活量 -5.267493e-05 -0.0001955795 -0.000407694  0.0002971469\n## 身高   -7.754975e-03 -0.0086910713  0.021599065  0.0079782016\n## 体重   -3.471120e-03 -0.0180620718 -0.015626841 -0.0522321990\n## 胸围   -1.552353e-02  0.0464952778  0.004886088  0.0178728641\n## \n## $ycoef\n##               [,1]        [,2]        [,3]        [,4]\n## 短跑    0.02340474 -0.08458262  0.07017709 -0.13566387\n## 跳高   -0.01068107 -0.02440377  0.01443519  0.01626168\n## 跳远   -0.02867642  0.92500098  0.23862503 -0.29882238\n## 实心球 -0.06884355 -0.07825414 -0.29442851 -0.19118769\n## \n## $xcenter\n##     肺活量       身高       体重       胸围 \n## 1490.47619  131.52024   26.44405   61.51190 \n## \n## $ycenter\n##      短跑      跳高      跳远    实心球 \n## 10.271429 72.805952  2.109048  2.978929\n\n$cor给出了两组数据之间的典型相关系数，$xcoef是第一组的典型相关系数，可以看到计算出了4个虚拟变量，$ycoef是第二组的典型相关系数。\n下面进行典型相关的显著性检验，使用R包CCP实现。\n\nlibrary(CCP)\n\nrho &lt;- cc1$cor\nn &lt;- dim(df[,1:4])[1]\np &lt;- length(df[,1:4])\nq &lt;- length(df[,5:8])\n\np.asym()函数实现典型相关的显著性检验。需要典型相关系数、观测个数、第一组的变量个数、第二组的变量个数。\n\n# 4种典型相关的结果\np.asym(rho,n,p,q, tstat = \"Wilks\")\n## Wilks' Lambda, using F-approximation (Rao's F):\n##               stat     approx df1      df2   p.value\n## 1 to 4:  0.1907537 10.4765088  16 232.8215 0.0000000\n## 2 to 4:  0.8860745  1.0618303   9 187.5484 0.3930330\n## 3 to 4:  0.9609581  0.7843615   4 156.0000 0.5369444\n## 4 to 4:  0.9985586  0.1140327   1  79.0000 0.7364945\np.asym(rho,n,p,q, tstat = \"Hotelling\")\n##  Hotelling-Lawley Trace, using F-approximation:\n##                 stat     approx df1 df2   p.value\n## 1 to 4:  3.770206950 17.5550261  16 298 0.0000000\n## 2 to 4:  0.125083307  1.0632081   9 306 0.3898996\n## 3 to 4:  0.040571670  0.7962190   4 314 0.5283457\n## 4 to 4:  0.001443452  0.1161979   1 322 0.7334177\np.asym(rho,n,p,q, tstat = \"Pillai\")\n##  Pillai-Bartlett Trace, using F-approximation:\n##                 stat    approx df1 df2      p.value\n## 1 to 4:  0.901742684 5.7482049  16 316 5.963363e-11\n## 2 to 4:  0.117022206 1.0849404   9 324 3.733220e-01\n## 3 to 4:  0.039096223 0.8192541   4 332 5.135803e-01\n## 4 to 4:  0.001441371 0.1225607   1 340 7.264904e-01\np.asym(rho,n,p,q, tstat = \"Roy\")\n##  Roy's Largest Root, using F-approximation:\n##               stat   approx df1 df2 p.value\n## 1 to 1:  0.7847205 71.99119   4  79       0\n## \n##  F statistic for Roy's Greatest Root is an upper bound.\n\n我们就看下Wilks结果，可以看到只有第一个典型相关系数是有意义的，后面3个都没有显著性。"
  }
]